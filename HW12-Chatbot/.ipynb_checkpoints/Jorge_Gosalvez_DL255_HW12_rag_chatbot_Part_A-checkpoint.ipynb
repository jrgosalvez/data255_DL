{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jrgosalvez/data255_DL/blob/main/HW12-Chatbot/Jorge_Gosalvez_DL255_HW12_rag_chatbot_Part_A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SJSU MSDS 255 DL, Spring 2024 - Building RAG Chatbots with LangChain\n",
        "Homework 12 - Part A: Code Chatbot"
      ],
      "metadata": {
        "id": "Pld0aGdIBVhZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4WsAryrw4wU"
      },
      "source": [
        "Git: https://github.com/jrgosalvez/data255_DL"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sources:\n",
        "* SJSU DL 255 RAG Chatbot with LangChain demo\n",
        "* [OpenAI API key](https://platform.openai.com/account/api-keys) and [Pinecone API key](https://app.pinecone.io)\n",
        "* [OpenAI Embeddings](https://platform.openai.com/docs/guides/embeddings/use-cases)\n",
        "* [RAGs with OpenAI](https://cookbook.openai.com/examples/parse_pdf_docs_for_rag)"
      ],
      "metadata": {
        "id": "9p6sE9-KBxdL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xF28qk8w4wT"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/generation/langchain/rag-chatbot.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/learn/generation/langchain/rag-chatbot.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3xE-W5ew4wU"
      },
      "source": [
        "### Part A Goal\n",
        "\n",
        "Build a code understanding model using LangChain, OpenAI, and Pinecone vector DB, to build a chatbot capable of learning from the external world using **R**etrieval **A**ugmented **G**eneration (RAG).\n",
        "\n",
        "Uploading my previous code files to the model I will be able to ask questions based on the code file as context.\n",
        "\n",
        "This example will have a functioning chatbot and RAG pipeline that can hold a conversation and provide informative responses based on a knowledge base."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rg8_mMrYw4wV"
      },
      "source": [
        "### Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnRFn2gdw4wV"
      },
      "source": [
        "Install the following Python libraries:\n",
        "\n",
        "- **langchain**: This is a library for GenAI. We'll use it to chain together different language models and components for our chatbot.\n",
        "- **openai**: This is the official OpenAI Python client. We'll use it to interact with the OpenAI API and generate responses for our chatbot.\n",
        "- **datasets**: This library provides a vast array of datasets for machine learning. We'll use it to load our knowledge base for the chatbot.\n",
        "- **pinecone-client**: This is the official Pinecone Python client. We'll use it to interact with the Pinecone API and store our chatbot's knowledge base in a vector database.\n",
        "\n",
        "**NOTE**: *OpenAI dataloaders will not load locally for on-prem devices easily. To simplify the use of these loaders, it is recommended to use an online notebook such as CoLab.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CflZ3e82w4wV"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "    langchain==0.0.354 \\\n",
        "    openai==1.6.1 \\\n",
        "    datasets==2.10.1 \\\n",
        "    pinecone-client==3.1.0 \\\n",
        "    tiktoken==0.5.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUH6ontXw4wV"
      },
      "source": [
        "### BACKGROUND: Building a Chatbot (no RAG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAps57bQw4wW"
      },
      "source": [
        "We will be relying heavily on the LangChain library to bring together the different components needed for our chatbot. To begin, we'll create a simple chatbot without any retrieval augmentation. We do this by initializing a `ChatOpenAI` object. For this we do need an [OpenAI API key](https://platform.openai.com/account/api-keys)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-hF4HeBjw4wW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64da795f-467c-4728-95a3-d71b984867e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OpenAI')\n",
        "\n",
        "chat = ChatOpenAI(\n",
        "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "    model='gpt-3.5-turbo'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ax2S29rZw4wW"
      },
      "source": [
        "Chats with OpenAI's `gpt-3.5-turbo` and `gpt-4` chat models are typically structured (in plain text) like this:\n",
        "\n",
        "```\n",
        "System: You are a helpful assistant.\n",
        "\n",
        "User: Hi AI, how are you today?\n",
        "\n",
        "Assistant: I'm great thank you. How can I help you?\n",
        "\n",
        "User: I'd like to understand string theory.\n",
        "\n",
        "Assistant:\n",
        "```\n",
        "\n",
        "The final `\"Assistant:\"` without a response is what would prompt the model to continue the conversation. In the official OpenAI `ChatCompletion` endpoint these would be passed to the model in a format like:\n",
        "\n",
        "```python\n",
        "[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Hi AI, how are you today?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"I'm great thank you. How can I help you?\"}\n",
        "    {\"role\": \"user\", \"content\": \"I'd like to understand string theory.\"}\n",
        "]\n",
        "```\n",
        "\n",
        "In LangChain there is a slightly different format. We use three _message_ objects like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kSbM7Phiw4wW"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import (\n",
        "    SystemMessage,\n",
        "    HumanMessage,\n",
        "    AIMessage\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful data scientist python coder.\"),\n",
        "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
        "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
        "    HumanMessage(content=\"I'd like to understand pytorch.\")\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6TnpBRAw4wW"
      },
      "source": [
        "The format is very similar, we're just swapped the role of `\"user\"` for `HumanMessage`, and the role of `\"assistant\"` for `AIMessage`.\n",
        "\n",
        "We generate the next response from the AI by passing these messages to the `ChatOpenAI` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yHFShuVMw4wW",
        "outputId": "06038913-1d00-452c-e0be-04bbd995a179",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Sure! PyTorch is an open-source machine learning library for Python that provides a flexible and dynamic computational graph. It is widely used for deep learning tasks such as neural networks. PyTorch offers a wide range of tools and utilities for building, training, and deploying machine learning models. \\n\\nIf you have any specific questions about PyTorch or need help with a particular task, feel free to ask!')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "res = chat(messages)\n",
        "res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7LoZQClw4wW"
      },
      "source": [
        "In response we get another AI message object. We can print it more clearly like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DzT8TK29w4wX",
        "outputId": "cd87c6f5-b6be-4950-b530-5f63df71f2bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure! PyTorch is an open-source machine learning library for Python that provides a flexible and dynamic computational graph. It is widely used for deep learning tasks such as neural networks. PyTorch offers a wide range of tools and utilities for building, training, and deploying machine learning models. \n",
            "\n",
            "If you have any specific questions about PyTorch or need help with a particular task, feel free to ask!\n"
          ]
        }
      ],
      "source": [
        "print(res.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXb4dFHXw4wX"
      },
      "source": [
        "### Stringing Messages for a Conversation\n",
        "Because `res` is just another `AIMessage` object, we can append it to `messages`, add another `HumanMessage`, and generate the next response in the conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2UZVj7P5w4wX",
        "outputId": "6a201108-c695-4e74-e0d6-abcd9ba50c3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch, like other deep learning frameworks, is a powerful tool for building complex neural networks and training them on large datasets. While PyTorch is widely used in the field of artificial intelligence, it is important to note that the development of general artificial intelligence (AGI) goes beyond just the choice of framework.\n",
            "\n",
            "Data scientists believe that PyTorch can contribute to the development of AGI because of its flexibility, scalability, and ease of use in building sophisticated neural network architectures. By leveraging PyTorch's capabilities, researchers can experiment with advanced deep learning models and algorithms that may potentially lead to breakthroughs in AGI.\n",
            "\n",
            "However, achieving AGI requires more than just a powerful framework - it involves interdisciplinary research in fields such as cognitive science, neuroscience, and philosophy, in addition to advancements in machine learning and computational technologies. While PyTorch is a valuable tool in the pursuit of AGI, it is just one piece of the puzzle in the larger quest for creating truly intelligent machines.\n"
          ]
        }
      ],
      "source": [
        "# add latest AI response to messages\n",
        "messages.append(res)\n",
        "\n",
        "# now create a new user prompt\n",
        "prompt = HumanMessage(\n",
        "    content=\"Why do data scientists believe it can produce general artificial intelligence?\"\n",
        ")\n",
        "# add to messages\n",
        "messages.append(prompt)\n",
        "\n",
        "# send to chat-gpt\n",
        "res = chat(messages)\n",
        "\n",
        "print(res.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtSC6WmBw4wX"
      },
      "source": [
        "### Dealing with Hallucinations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAhStCG9w4wX"
      },
      "source": [
        "We have our chatbot, but as mentioned — the knowledge of LLMs can be limited. The reason for this is that LLMs learn all they know during training. An LLM essentially compresses the \"world\" as seen in the training data into the internal parameters of the model. We call this knowledge the _parametric knowledge_ of the model.\n",
        "\n",
        "By default, LLMs have no access to the external world.\n",
        "\n",
        "The result of this is very clear when we ask LLMs about more recent information, like about the new (and very popular) Llama 2 LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vdtjTQebw4wX"
      },
      "outputs": [],
      "source": [
        "# add latest AI response to messages\n",
        "messages.append(res)\n",
        "\n",
        "# now create a new user prompt\n",
        "prompt = HumanMessage(\n",
        "    content=\"What is so special about Llama 2?\"\n",
        ")\n",
        "# add to messages\n",
        "messages.append(prompt)\n",
        "\n",
        "# send to OpenAI\n",
        "res = chat(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zMi7B6LZw4wX",
        "outputId": "1def4ed1-62b0-428a-d19d-d9106423ab7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm not familiar with a specific technology or framework called \"Llama 2\" in the context of data science or artificial intelligence. It's possible that it may be a new or specialized tool that I'm not aware of.\n",
            "\n",
            "If you can provide more context or details about Llama 2, I'd be happy to look into it further and provide more information or insights. Alternatively, if you meant something else or have a different question, feel free to clarify and I'll do my best to assist you.\n"
          ]
        }
      ],
      "source": [
        "print(res.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YORp9R_Pw4wX"
      },
      "source": [
        "Our chatbot can no longer help us, it doesn't contain the information we need to answer the question. It was very clear from this answer that the LLM doesn't know the informaiton, but sometimes an LLM may respond like it _does_ know the answer — and this can be very hard to detect.\n",
        "\n",
        "OpenAI have since adjusted the behavior for this particular example as we can see below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "30od2wnkw4wX"
      },
      "outputs": [],
      "source": [
        "# add latest AI response to messages\n",
        "messages.append(res)\n",
        "\n",
        "# now create a new user prompt\n",
        "prompt = HumanMessage(\n",
        "    content=\"Can you tell me about the LLMChain in LangChain?\"\n",
        ")\n",
        "# add to messages\n",
        "messages.append(prompt)\n",
        "\n",
        "# send to OpenAI\n",
        "res = chat(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QZOSGe-Kw4wX",
        "outputId": "02478bee-e3ac-4529-d0ee-ee308ba65313",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm not aware of a specific technology called \"LLMChain in LangChain.\" It's possible that these terms refer to specialized tools or concepts within a specific domain that I may not be familiar with.\n",
            "\n",
            "If you can provide more context or details about LLMChain and LangChain, I'd be happy to try and help you understand them better. Alternatively, if you have a different question or topic in mind, feel free to let me know so I can assist you accordingly.\n"
          ]
        }
      ],
      "source": [
        "print(res.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma4gl6-rw4wX"
      },
      "source": [
        "### Feed the LLM More Data Manually [Not scalable]\n",
        "There is another way of feeding knowledge into LLMs. It is called _source knowledge_ and it refers to any information fed into the LLM via the prompt. We can try that with the LLMChain question. We can take a description of this object from the LangChain documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HBpF2S0fw4wX"
      },
      "outputs": [],
      "source": [
        "llmchain_information = [\n",
        "    \"A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\",\n",
        "    \"Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\",\n",
        "    \"LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\"\n",
        "]\n",
        "\n",
        "source_knowledge = \"\\n\".join(llmchain_information)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Th7n7OZaw4wX"
      },
      "source": [
        "We can feed this additional knowledge into our prompt with some instructions telling the LLM how we'd like it to use this information alongside our original query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Tq3IIKciw4wX"
      },
      "outputs": [],
      "source": [
        "query = \"Can you tell me about the LLMChain in LangChain?\"\n",
        "\n",
        "augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
        "\n",
        "Contexts:\n",
        "{source_knowledge}\n",
        "\n",
        "Query: {query}\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAwwu54Ow4wX"
      },
      "source": [
        "Now we feed this into our chatbot as we were before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FnUzBUH0w4wX"
      },
      "outputs": [],
      "source": [
        "# create a new user prompt\n",
        "prompt = HumanMessage(\n",
        "    content=augmented_prompt\n",
        ")\n",
        "# add to messages\n",
        "messages.append(prompt)\n",
        "\n",
        "# send to OpenAI\n",
        "res = chat(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "VvYXrStmw4wX",
        "outputId": "669a5f05-8c25-4fe9-f636-f77a1583b6ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The LLMChain in LangChain is a common type of chain that is part of the LangChain framework for developing applications powered by language models. The LLMChain consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. \n",
            "\n",
            "In the context of LangChain, a chain is a generic concept that refers to a sequence of modular components (or other chains) combined in a specific way to achieve a common use case. The LLMChain takes multiple input variables, formats them into a prompt using the PromptTemplate, passes that prompt to the model (LLM or ChatModel), and then uses the OutputParser (if provided) to parse the output of the language model into a final format.\n",
            "\n",
            "LangChain aims to enable the development of applications that are data-aware and agentic, meaning they can connect a language model to other sources of data and allow the language model to interact with its environment. By leveraging the capabilities of the LLMChain within the LangChain framework, developers can create powerful and differentiated applications that go beyond simple language model API calls.\n"
          ]
        }
      ],
      "source": [
        "print(res.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNdgE8hZw4wY"
      },
      "source": [
        "The quality of this answer is phenomenal. This is made possible thanks to the idea of augmented our query with external knowledge (source knowledge). There's just one problem — how do we get this information in the first place?\n",
        "\n",
        "We learned in the previous chapters about Pinecone and vector databases. Well, they can help us here too. But first, we'll need a dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-JA-ut8w4wY"
      },
      "source": [
        "### Importing the Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dL7AT2Wvd1wK",
        "outputId": "a9dfc543-c5d7-49f0-e056-9ed0902a5233"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oPwiUmbhBx8",
        "outputId": "c9c4e0a9-6bc2-4b74-ebf4-509905c688d0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.2.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.11.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "\n",
        "#load pdf files\n",
        "loader = PyPDFDirectoryLoader('/content/drive/MyDrive/MSDA/DATA255/codePDF')\n",
        "data = loader.load()\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxc1ORMRf3qv",
        "outputId": "dea99557-fcb7-464a-e5e7-6c3e59e72ce3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content=\"Homework 10: BER TSJSU MSDS 255 DL, Spring 2024 - Transformers \\ue313\\nGit: https:/ /github.com/jr gosalv ez/data255_DL\\nSour ce:\\nhttps:/ /rajpurkar .github.io/SQuAD-explor er\\nhttps:/ /pytorch.or g/text/stable/datasets.html#t orchtext.datasets.SQuAD2\\nimport\\xa0torch\\nimport\\xa0json\\nimport\\xa0pandas\\xa0 as\\xa0pd\\nfrom\\xa0transformers\\xa0 import\\xa0BertTokenizer\\n/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: User\\nThe secret `HF_TOKEN` does not exist in your Colab secrets.\\nTo authenticate with the Hugging Face Hub, create a token in your settings tab (\\nYou will be able to reuse this secret in all of your notebooks.\\nPlease note that authentication is recommended but still optional to access publ\\n  warnings.warn(\\ntokenizer_conﬁg.json:\\u2007100% \\u200748.0/48.0\\u2007[00:00<00:00,\\u20073.84kB/s]\\nvocab.txt:\\u2007100% \\u2007232k/232k\\u2007[00:00<00:00,\\u20073.93MB/s]\\ntokenizer .json:\\u2007100% \\u2007466k/466k\\u2007[00:00<00:00,\\u20073.70MB/s]\\nconﬁg.json:\\u2007100% \\u2007570/570\\u2007[00:00<00:00,\\u200750.9kB/s]#\\xa0Load\\xa0pre-trained\\xa0model\\xa0tokenizer\\ntokenizer\\xa0=\\xa0BertTokenizer.from_pretrained ('bert-base-uncased' )\\nDownload the tr aining data \\ue313\\n#\\xa0Define\\xa0the\\xa0path\\xa0to\\xa0your\\xa0SQuAD2\\xa0data\\xa0file\\nsquad_data_path\\xa0=\\xa0 '/content/train-v2.0.json'\\n/content/train-v2.0.json\\n'\\n'squad_data_path\\ndf\\xa0=\\xa0pd.read_json (squad_data_path )\\ndf5/5/24, 12:53 PM Jorge_Gosalvez_HW10_BERT.ipynb - Colab\\nhttps://colab.research.google.com/drive/1K2vYT2LBTWdhmSNVh7uEFu0xars8PcSJ#printMode=true 1/7\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW10_BERT.pdf', 'page': 0}), Document(page_content='version data\\n0 v2.0 {\\'title\\': \\'Beyoncé\\', \\'paragraphs\\': [{\\'qas\\': [{...\\n1 v2.0 {\\'title\\': \\'Frédéric_Chopin\\', \\'paragraphs\\': [{\\'...\\n2 v2.0 {\\'title\\': \\'Sino-T ibetan_relations_during_the_M...\\n3 v2.0 {\\'title\\': \\'IPod\\', \\'paragraphs\\': [{\\'qas\\': [{\\'qu...\\n4 v2.0 {\\'title\\': \\'The_Legend_of_Zelda:_T wilight_Princ...\\n... ... ...\\n437 v2.0 {\\'title\\': \\'Infection\\', \\'paragraphs\\': [{\\'qas\\': ...\\n438 v2.0 {\\'title\\': \\'Hunting\\', \\'paragraphs\\': [{\\'qas\\': [{...\\n439 v2.0 {\\'title\\': \\'Kathmandu\\', \\'paragraphs\\': [{\\'qas\\': ...\\n440 v2.0 {\\'title\\': \\'Myocardial_infarction\\', \\'paragraphs...\\n441 v2.0 {\\'title\\': \\'Matter\\', \\'paragraphs\\': [{\\'qas\\': [{\\'...\\n442 rows × 2 columns\\nOpen and pr eprocess (add special t okens) dataset per BER T format\\nLoad pr eprocess (add special t okens) the SQU AD 2.0  dataset per BER T format. Get a minimum 20 QnA pairs. \\ue313\\n#\\xa0Function\\xa0to\\xa0load\\xa0SQuAD2\\xa0data\\xa0and\\xa0add\\xa0special\\xa0tok ens\\xa0[CLS]\\xa0and\\xa0[SEP]\\ndef\\xa0load_squad_data (file_path ,\\xa0num_samples =20):\\n\\xa0\\xa0\\xa0\\xa0with\\xa0open(file_path ,\\xa0\\'r\\',\\xa0encoding= \\'utf-8\\')\\xa0as\\xa0f:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0squad_data\\xa0=\\xa0json.load (f)\\n\\xa0\\xa0\\xa0\\xa0data\\xa0=\\xa0 []\\n\\xa0\\xa0\\xa0\\xa0for\\xa0i\\xa0in\\xa0range(min(len(squad_data [\\'data\\']),\\xa0num_samples )):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0paragraphs\\xa0=\\xa0squad_data [\\'data\\'][i][\\'paragraphs\\' ]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for\\xa0paragraph\\xa0 in\\xa0paragraphs :\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0context\\xa0=\\xa0paragraph [\\'context\\' ]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0qas\\xa0=\\xa0paragraph [\\'qas\\']\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for\\xa0qa\\xa0in\\xa0qas:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0question\\xa0=\\xa0qa [\\'question\\' ]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0answers\\xa0\\xa0=\\xa0qa [\\'answers\\' ]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\xa0answers :\\xa0\\xa0#\\xa0Check\\xa0if\\xa0answers\\xa0are\\xa0available\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0answer_text\\xa0=\\xa0answers [0][\\'text\\']\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Tokenize\\xa0question\\xa0and\\xa0answer\\xa0text\\xa0.encode\\xa0automa tically\\xa0adds\\xa0[CLS]\\xa0and\\xa0[SEP]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0tokenized_question\\xa0=\\xa0tokenizer .encode(question ,\\xa0add_special_tokens= True)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0tokenized_answer\\xa0=\\xa0tokenizer.e ncode(answer_text ,\\xa0add_special_tokens= True)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0data.append ((tokenized_question ,\\xa0tokenized_answer ))\\n\\xa0\\xa0\\xa0\\xa0return\\xa0data\\nsquad_data\\xa0=\\xa0load_squad_data (squad_data_path ,\\xa0num_samples= 20)\\nDispla y the \\x00rst f ew question-answer pairs \\ue313\\nfor\\xa0i\\xa0in\\xa0range(8):\\n\\xa0\\xa0\\xa0\\xa0print(\"Question:\" ,\\xa0tokenizer.decode (squad_data [i][0]))\\n\\xa0\\xa0\\xa0\\xa0print(\"Answer:\" ,\\xa0tokenizer.decode (squad_data [i][1]))\\n\\xa0\\xa0\\xa0\\xa0print()\\nQuestion: [CLS] when did beyonce start becoming popular? [SEP]\\nAnswer: [CLS] in the late 1990s [SEP]\\nQuestion: [CLS] what areas did beyonce compete in when she was growing up? [SEP]\\nAnswer: [CLS] singing and dancing [SEP]\\nQuestion: [CLS] when did beyonce leave destiny\\'s child and become a solo singer? [SEP]\\nAnswer: [CLS] 2003 [SEP]\\nQuestion: [CLS] in what city and state did beyonce grow up? [SEP]\\nAnswer: [CLS] houston, texas [SEP]\\nQuestion: [CLS] in which decade did beyonce become famous? [SEP]\\nAnswer: [CLS] late 1990s [SEP]5/5/24, 12:53 PM Jorge_Gosalvez_HW10_BERT.ipynb - Colab\\nhttps://colab.research.google.com/drive/1K2vYT2LBTWdhmSNVh7uEFu0xars8PcSJ#printMode=true 2/7', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW10_BERT.pdf', 'page': 1}), Document(page_content='Question: [CLS] in what r & b group was she the lead singer? [SEP]\\nAnswer: [CLS] destiny\\'s child [SEP]\\nQuestion: [CLS] what album made her a worldwide known artist? [SEP]\\nAnswer: [CLS] dangerously in love [SEP]\\nQuestion: [CLS] who managed the destiny\\'s child group? [SEP]\\nAnswer: [CLS] mathew knowles [SEP]\\nDataloader t okenizes text aut omatically \\ue313\\nTrain and e valuate the BER T QnA model \\ue313\\nimport\\xa0torch\\nfrom\\xa0transformers\\xa0 import\\xa0BertTokenizer ,\\xa0BertForQuestionAnswering ,\\xa0AdamW,\\xa0get_linear_schedule_with_warmup\\nfrom\\xa0torch.utils.data\\xa0 import\\xa0DataLoader ,\\xa0RandomSampler ,\\xa0SequentialSampler\\nfrom\\xa0transformers\\xa0 import\\xa0squad_convert_examples_to_features\\nfrom\\xa0transformers.data.processors.squad\\xa0 import\\xa0SquadV2Processor\\nfrom\\xa0tqdm\\xa0import\\xa0tqdm\\nimport\\xa0numpy\\xa0as\\xa0np\\nimport\\xa0random\\n#\\xa0Set\\xa0random\\xa0seeds\\xa0for\\xa0reproducibility\\nseed_val\\xa0=\\xa0 42\\nrandom.seed (seed_val )\\nnp.random.seed (seed_val )\\ntorch.manual_seed (seed_val )\\ntorch.cuda.manual_seed_all (seed_val )\\nSome weights of BertForQuestionAnswering were not initialized from the model che\\nYou should probably TRAIN this model on a down-stream task to be able to use it model.safetensors:\\u2007100% \\u2007440M/440M\\u2007[00:01<00:00,\\u2007329MB/s]#\\xa0Load\\xa0pre-trained\\xa0BERT\\xa0model\\xa0and\\xa0tokenizer\\nmodel_name\\xa0=\\xa0 \\'bert-base-uncased\\'\\ntokenizer\\xa0\\xa0=\\xa0BertTokenizer.from_pretrained (model_name )\\nmodel\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0=\\xa0BertForQuestionAnswering.from_pretrai ned(model_name )\\n#\\xa0Define\\xa0fine-tuning\\xa0parameters\\nbatch_size\\xa0=\\xa0 16\\nepochs\\xa0=\\xa0 3\\nlearning_rate\\xa0=\\xa0 3e-5\\nadam_epsilon\\xa0=\\xa0 1e-8\\n#\\xa0Load\\xa0SQuAD\\xa02.0\\xa0data\\xa0and\\xa0limit\\xa0to\\xa020\\xa0QnA\\xa0pairs\\nprocessor\\xa0=\\xa0SquadV2Processor ()\\ntrain_examples\\xa0=\\xa0processor.get_train_examples (\\'./\\'\\xa0,\\xa0filename= \"train-v2.0.json\" )[:20]\\nval_examples\\xa0\\xa0\\xa0=\\xa0processor.get_train_examples (\\'./\\'\\xa0,\\xa0filename= \"train-v2.0.json\" )[21:29]\\ntest_dataset\\xa0\\xa0\\xa0=\\xa0processor.get_train_examples (\\'./\\'\\xa0,\\xa0filename= \"dev-v2.0.json\" )[:5]\\n100%|██████████| 442/442 [00:42<00:00, 10.32it/s]\\n100%|██████████| 442/442 [00:42<00:00, 10.39it/s]\\n100%|██████████| 35/35 [00:03<00:00,  8.94it/s]5/5/24, 12:53 PM Jorge_Gosalvez_HW10_BERT.ipynb - Colab\\nhttps://colab.research.google.com/drive/1K2vYT2LBTWdhmSNVh7uEFu0xars8PcSJ#printMode=true 3/7', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW10_BERT.pdf', 'page': 2}), Document(page_content='#\\xa0Convert\\xa0examples\\xa0to\\xa0features\\ntrain_features ,\\xa0train_dataset\\xa0=\\xa0squad_convert_examples_to_feature s(\\n\\xa0\\xa0\\xa0\\xa0examples=train_examples ,\\n\\xa0\\xa0\\xa0\\xa0tokenizer=tokenizer ,\\n\\xa0\\xa0\\xa0\\xa0max_seq_length= 384,\\xa0\\xa0#\\xa0BERT\\xa0max\\xa0input\\xa0sequence\\xa0length\\xa0-\\xa02\\xa0for\\xa0[CLS]\\xa0and \\xa0[SEP]\\n\\xa0\\xa0\\xa0\\xa0doc_stride= 128,\\n\\xa0\\xa0\\xa0\\xa0max_query_length= 64,\\n\\xa0\\xa0\\xa0\\xa0is_training= True,\\n\\xa0\\xa0\\xa0\\xa0return_dataset= \"pt\"\\n)\\nval_features ,\\xa0val_dataset\\xa0=\\xa0squad_convert_examples_to_features (\\n\\xa0\\xa0\\xa0\\xa0examples=val_examples ,\\n\\xa0\\xa0\\xa0\\xa0tokenizer=tokenizer ,\\n\\xa0\\xa0\\xa0\\xa0max_seq_length= 384,\\xa0\\xa0#\\xa0BERT\\xa0max\\xa0input\\xa0length\\n\\xa0\\xa0\\xa0\\xa0doc_stride= 128,\\n\\xa0\\xa0\\xa0\\xa0max_query_length= 64,\\n\\xa0\\xa0\\xa0\\xa0is_training= True,\\n\\xa0\\xa0\\xa0\\xa0return_dataset= \"pt\"\\n)\\n/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with m\\n  self.pid = os.fork()\\nconvert squad examples to features: 100%|██████████| 20/20 [00:00<00:00, 123.34it/s]\\nadd example index and unique id: 100%|██████████| 20/20 [00:00<00:00, 150874.24it/s]\\nconvert squad examples to features: 100%|██████████| 8/8 [00:00<00:00, 91.22it/s]\\nadd example index and unique id: 100%|██████████| 8/8 [00:00<00:00, 101989.16it/s]\\n#\\xa0Create\\xa0data\\xa0loader\\xa0[CLS]\\xa0&\\xa0[SEP]\\n#\\xa0NOTE:\\xa0For\\xa0each\\xa0batch\\xa0of\\xa0the\\xa0dataloader;\\xa0input\\xa0ID s,\\xa0attention\\xa0masks,\\xa0and\\xa0token\\xa0type\\xa0IDs\\xa0are\\xa0created ,\\xa0including\\xa0[CLS]\\xa0and\\xa0[SEP]\\n#train_sampler\\xa0\\xa0\\xa0\\xa0=\\xa0RandomSampler(train_dataset)\\n#train_dataloader\\xa0=\\xa0DataLoader(train_dataset,\\xa0samp ler=train_sampler,\\xa0batch_size=batch_size)\\ntrain_dataloader\\xa0=\\xa0DataLoader (train_dataset ,\\xa0sampler=RandomSampler (train_dataset ),\\xa0batch_size=batch_size )\\nval_dataloader\\xa0\\xa0\\xa0=\\xa0DataLoader (val_dataset ,\\xa0sampler=RandomSampler (val_dataset ),\\xa0batch_size=batch_size )\\ntest_dataloader\\xa0\\xa0=\\xa0DataLoader (test_dataset ,\\xa0sampler=RandomSampler (test_dataset ),\\xa0batch_size=batch_size )\\n#\\xa0Prepare\\xa0optimizer\\xa0and\\xa0scheduler\\noptimizer\\xa0\\xa0\\xa0=\\xa0torch.optim.AdamW (model.parameters (),\\xa0lr=learning_rate ,\\xa0eps=adam_epsilon )\\ntotal_steps\\xa0=\\xa0 len(train_dataloader )\\xa0*\\xa0epochs\\nscheduler\\xa0\\xa0\\xa0=\\xa0get_linear_schedule_with_warmup (optimizer ,\\xa0num_warmup_steps= 0,\\xa0num_training_steps=total_steps )5/5/24, 12:53 PM Jorge_Gosalvez_HW10_BERT.ipynb - Colab\\nhttps://colab.research.google.com/drive/1K2vYT2LBTWdhmSNVh7uEFu0xars8PcSJ#printMode=true 4/7', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW10_BERT.pdf', 'page': 3}), Document(page_content='#\\xa0Train\\xa0the\\xa0model\\ndevice\\xa0=\\xa0torch.device (\"cuda\"\\xa0if\\xa0torch.cuda.is_available ()\\xa0else\\xa0\"cpu\")\\nmodel.to (device)\\n\"\"\"print(\"Training...\")\\nfor\\xa0epoch\\xa0in\\xa0range(epochs):\\n\\xa0\\xa0\\xa0\\xa0model.train()\\n\\xa0\\xa0\\xa0\\xa0total_loss\\xa0=\\xa00\\n\\xa0\\xa0\\xa0\\xa0for\\xa0step,\\xa0batch\\xa0in\\xa0enumerate(train_dataloader) :\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0batch\\xa0=\\xa0tuple(t.to(device)\\xa0for\\xa0t\\xa0in\\xa0batch)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0inputs\\xa0=\\xa0{\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\"input_ids\":\\xa0batch[0],\\xa0#\\xa0Input\\xa0token\\xa0I Ds\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\"attention_mask\":\\xa0batch[1],\\xa0#\\xa0Attentio n\\xa0mask\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\"token_type_ids\":\\xa0batch[2],\\xa0#\\xa0Segment\\xa0 token\\xa0IDs\\xa0(for\\xa0sequence\\xa0pairs)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\"start_positions\":\\xa0batch[3],#\\xa0Start\\xa0po sition\\xa0of\\xa0the\\xa0answer\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\"end_positions\":\\xa0batch[4]\\xa0#\\xa0End\\xa0positi on\\xa0of\\xa0the\\xa0answer\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0}\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0optimizer.zero_grad()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0outputs\\xa0=\\xa0model(**inputs)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0loss\\xa0=\\xa0outputs.loss\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0total_loss\\xa0+=\\xa0loss.item()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0loss.backward()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0torch.nn.utils.clip_grad_norm_(model.param eters(),\\xa01.0)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0optimizer.step()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0scheduler.step()\\n\\xa0\\xa0\\xa0\\xa0avg_train_loss\\xa0=\\xa0total_loss\\xa0/\\xa0len(train_datalo ader)\\n\\xa0\\xa0\\xa0\\xa0print(f\\'\\\\nEpoch\\xa0{epoch\\xa0+\\xa01}:\\')\\n\\xa0\\xa0\\xa0\\xa0print(f\\'Average\\xa0training\\xa0loss:\\xa0{avg_train_loss :.4f}\\')\\nprint(\"\\\\nTraining\\xa0complete!\")\"\"\"\\nprint(\"Training...\" )\\nn_epochs\\xa0=\\xa0 0\\ntrain_losses ,\\xa0valid_losses\\xa0=\\xa0 [],\\xa0[]\\nfor\\xa0epoch\\xa0in\\xa0range(epochs):\\n\\xa0\\xa0\\xa0\\xa0model.train ()\\n\\xa0\\xa0\\xa0\\xa0total_train_loss\\xa0=\\xa0 0\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Training\\xa0loop\\n\\xa0\\xa0\\xa0\\xa0for\\xa0step,\\xa0batch\\xa0in\\xa0enumerate (train_dataloader ):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0batch\\xa0=\\xa0 tuple(t.to(device)\\xa0for\\xa0t\\xa0in\\xa0batch)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0inputs\\xa0=\\xa0 {\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \"input_ids\" :\\xa0batch[0],\\xa0#\\xa0Input\\xa0token\\xa0IDs\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \"attention_mask\" :\\xa0batch[1],\\xa0#\\xa0Attention\\xa0mask\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \"token_type_ids\" :\\xa0batch[2],\\xa0#\\xa0Segment\\xa0token\\xa0IDs\\xa0(for\\xa0sequence\\xa0pairs)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \"start_positions\" :\\xa0batch[3],#\\xa0Start\\xa0position\\xa0of\\xa0the\\xa0answer\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \"end_positions\" :\\xa0batch[4]\\xa0#\\xa0End\\xa0position\\xa0of\\xa0the\\xa0answer\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 }\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0optimizer.zero_grad ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0outputs\\xa0=\\xa0model (**inputs )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0loss\\xa0=\\xa0outputs.loss\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0total_train_loss\\xa0+=\\xa0loss.item ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0loss.backward ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0torch.nn.utils.clip_grad_norm_ (model.parameters (),\\xa01.0)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0optimizer.step ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0scheduler.step ()\\n\\xa0\\xa0\\xa0\\xa0avg_train_loss\\xa0=\\xa0total_train_loss\\xa0/\\xa0 len(train_dataloader )\\n\\xa0\\xa0\\xa0\\xa0train_losses.append (avg_train_loss )\\n\\xa0\\xa0\\xa0\\xa0print(f\\'Epoch\\xa0{epoch\\xa0+\\xa0 1}:\\')\\n\\xa0\\xa0\\xa0\\xa0print(f\\'\\xa0\\xa0Average\\xa0training\\xa0loss:\\xa0 {avg_train_loss :.4f}\\')\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Validation\\xa0loop\\n\\xa0\\xa0\\xa0\\xa0model. eval()\\n\\xa0\\xa0\\xa0\\xa0total_val_loss\\xa0=\\xa0 0\\n\\xa0\\xa0\\xa0\\xa0for\\xa0step,\\xa0batch\\xa0in\\xa0enumerate (val_dataloader ):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0batch\\xa0=\\xa0 tuple(t.to(device)\\xa0for\\xa0t\\xa0in\\xa0batch)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0inputs\\xa0=\\xa0 {\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \"input_ids\" :\\xa0batch[0],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \"attention_mask\" :\\xa0batch[1],5/5/24, 12:53 PM Jorge_Gosalvez_HW10_BERT.ipynb - Colab\\nhttps://colab.research.google.com/drive/1K2vYT2LBTWdhmSNVh7uEFu0xars8PcSJ#printMode=true 5/7', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW10_BERT.pdf', 'page': 4}), Document(page_content='\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \"token_type_ids\" :\\xa0batch[2],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \"start_positions\" :\\xa0batch[3],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \"end_positions\" :\\xa0batch[4]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 }\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 with\\xa0torch.no_grad ():\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0outputs\\xa0=\\xa0model (**inputs )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0loss\\xa0=\\xa0outputs.loss\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0total_val_loss\\xa0+=\\xa0loss.item ()\\n\\xa0\\xa0\\xa0\\xa0avg_val_loss\\xa0=\\xa0total_val_loss\\xa0/\\xa0 len(val_dataloader )\\n\\xa0\\xa0\\xa0\\xa0valid_losses.append (avg_val_loss )\\n\\xa0\\xa0\\xa0\\xa0print(f\\'\\xa0\\xa0Average\\xa0validation\\xa0loss:\\xa0 {avg_val_loss :.4f}\\')\\n\\xa0\\xa0\\xa0\\xa0n_epochs\\xa0+=\\xa0 1\\nprint(\"Training\\xa0complete!\" )\\nTraining...\\nEpoch 1:\\n  Average training loss: 5.8237\\n  Average validation loss: 5.5687\\nEpoch 2:\\n  Average training loss: 5.1328\\n  Average validation loss: 5.4107\\nEpoch 3:\\n  Average training loss: 4.9340\\n  Average validation loss: 5.3430\\nTraining complete!\\nimport\\xa0matplotlib.pyplot\\xa0 as\\xa0plt\\nimport\\xa0numpy\\xa0as\\xa0np\\nepoch_ticks\\xa0=\\xa0 range(1,\\xa0n_epochs\\xa0+\\xa0 1)\\xa0\\xa0#\\xa0Assuming\\xa0n_epochs\\xa0is\\xa0defined\\xa0somewhere\\nplt.plot (epoch_ticks ,\\xa0train_losses )\\nplt.plot (epoch_ticks ,\\xa0valid_losses )\\nplt.legend ([\\'Train\\xa0Loss\\' ,\\xa0\\'Valid\\xa0Loss\\' ])\\nplt.xlabel (\\'Epochs\\' )\\nplt.ylabel (\\'Loss\\')\\nplt.title (\\'Training\\xa0and\\xa0Validation\\xa0Losses\\' )\\nplt.show ()\\nContinual tr aining loss implies potential o ver\\x00tting\\nPerform an Inf erence and show the pr edicted vs gr ound truth answers. \\ue3135/5/24, 12:53 PM Jorge_Gosalvez_HW10_BERT.ipynb - Colab\\nhttps://colab.research.google.com/drive/1K2vYT2LBTWdhmSNVh7uEFu0xars8PcSJ#printMode=true 6/7', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW10_BERT.pdf', 'page': 5}), Document(page_content='for\\xa0example\\xa0 in\\xa0test_dataset :\\n\\xa0\\xa0\\xa0\\xa0inputs\\xa0=\\xa0tokenizer.encode_plus (example.question_text ,\\xa0example.context_text ,\\xa0add_special_tokens= True,\\xa0return_tensors= \"pt\")\\n\\xa0\\xa0\\xa0\\xa0input_ids\\xa0=\\xa0inputs [\\'input_ids\\' ].tolist()\\n\\xa0\\xa0\\xa0\\xa0decoded_text\\xa0=\\xa0tokenizer.decode (input_ids [0])\\n\\xa0\\xa0\\xa0\\xa0print(\"DECODED\\xa0TEXT:\" ,\\xa0decoded_text )\\n\\xa0\\xa0\\xa0\\xa0print(\"\\\\nQUESTION:\" ,\\xa0example.question_text )\\n\\xa0\\xa0\\xa0\\xa0print(\"CONTEXT:\" ,\\xa0example.context_text )\\n\\xa0\\xa0\\xa0\\xa0print(\"ANSWER:\" ,\\xa0example.answer_text )\\n\\xa0\\xa0\\xa0\\xa0print()\\n\\xa0\\xa0\\xa0\\xa0print()\\nDECODED TEXT: [CLS] in what country is normandy located? [SEP] the normans ( norman : nourmands ; french : normands ; latin \\nQUESTION: In what country is Normandy located?\\nCONTEXT: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuri\\nANSWER: France\\nDECODED TEXT: [CLS] when were the normans in normandy? [SEP] the normans ( norman : nourmands ; french : normands ; latin : \\nQUESTION: When were the Normans in Normandy?\\nCONTEXT: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuri\\nANSWER: 10th and 11th centuries\\nDECODED TEXT: [CLS] from which countries did the norse originate? [SEP] the normans ( norman : nourmands ; french : normands\\nQUESTION: From which countries did the Norse originate?\\nCONTEXT: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuri\\nANSWER: Denmark, Iceland and Norway\\nDECODED TEXT: [CLS] who was the norse leader? [SEP] the normans ( norman : nourmands ; french : normands ; latin : normanni \\nQUESTION: Who was the Norse leader?\\nCONTEXT: TheNormans (Norman: Nourmands; French: Normands; Latin:Normanni) werethepeoplewhointhe10thand11thcenturi5/5/24, 12:53 PM Jorge_Gosalvez_HW10_BERT.ipynb - Colab\\nhttps://colab.research.google.com/drive/1K2vYT2LBTWdhmSNVh7uEFu0xars8PcSJ#printMode=true 7/7', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW10_BERT.pdf', 'page': 6}), Document(page_content='Homework 05: GANSJSU MSDS 255 DL, Spring 2024\\ue313\\nGit: https:/ /github.com/jr gosalv ez/data255_DL\\nPart 1 - GAN\\ue313\\nStep 1. Load F ashion MNIST  & Replace Dataset in Demo \\ue313\\n#\\xa0prerequisites\\nimport\\xa0torch\\nimport\\xa0torch.nn\\xa0 as\\xa0nn\\nimport\\xa0torch.nn.functional\\xa0 as\\xa0F\\nimport\\xa0torch.optim\\xa0 as\\xa0optim\\nfrom\\xa0torchvision\\xa0 import\\xa0datasets ,\\xa0transforms\\nfrom\\xa0torch.autograd\\xa0 import\\xa0Variable\\nfrom\\xa0torchvision.utils\\xa0 import\\xa0save_image\\nimport\\xa0warnings\\nwarnings.filterwarnings (\"ignore\" )\\n#\\xa0Device\\xa0configuration\\ndevice\\xa0=\\xa0torch.device (\\'cuda\\'\\xa0if\\xa0torch.cuda.is_available ()\\xa0else\\xa0\\'cpu\\')\\n#\\xa0Fashion\\xa0MNIST\\xa0Dataset\\ntransform\\xa0=\\xa0transforms.Compose ([transforms.ToTensor ()])\\n#transform\\xa0=\\xa0transforms.Compose([transforms.ToTens or(),\\n#\\xa0\\xa0transforms.Normalize((0.5,),\\xa0(0.5,))\\n#])\\ntrain_dataset\\xa0=\\xa0datasets.FashionMNIST (root=\\'./mnist_data/\\' ,\\xa0train=True,\\xa0transform=tr\\ntest_dataset\\xa0\\xa0=\\xa0datasets.FashionMNIST (root=\\'./mnist_data/\\' ,\\xa0train=False,\\xa0transform=t\\nimg,\\xa0label\\xa0=\\xa0train_dataset [0]\\nprint(\\'Label:\\xa0\\' ,\\xa0label)\\nprint(img[:,10:15,10:15])\\ntorch.min(img),\\xa0torch.max(img)\\nLabel:  9\\ntensor([[[0.0000, 0.0000, 0.0000, 0.7569, 0.8941],5/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 1/25', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 0}), Document(page_content=\"         [0.0118, 0.0000, 0.0471, 0.8588, 0.8627],\\n         [0.0235, 0.0000, 0.3882, 0.9569, 0.8706],\\n         [0.0000, 0.0000, 0.2157, 0.9255, 0.8941],\\n         [0.0000, 0.0000, 0.9294, 0.8863, 0.8510]]])\\n(tensor(0.), tensor(1.))\\ndef\\xa0denorm(x):\\n\\xa0\\xa0\\xa0\\xa0out\\xa0=\\xa0 (x+1)\\xa0/\\xa02\\n\\xa0\\xa0\\xa0\\xa0return\\xa0out.clamp (0,1)\\nLabel: 9\\nimport\\xa0matplotlib.pyplot\\xa0 as\\xa0plt\\n%matplotlib\\xa0 inline\\nimg_norm\\xa0=\\xa0denorm (img)\\nplt.imshow (img_norm [0],\\xa0cmap='gray')\\nprint('Label:' ,\\xa0label)\\nDownload in batches\\ue313\\nbatch_size\\xa0=\\xa0 100\\n#\\xa0Data\\xa0Loader\\xa0(Input\\xa0Pipeline)\\ntrain_loader\\xa0=\\xa0torch.utils.data.DataLoader (dataset=train_dataset ,\\xa0batch_size=batch_s\\ntest_loader\\xa0\\xa0=\\xa0torch.utils.data.DataLoader (dataset=test_dataset ,\\xa0batch_size=batch_si5/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 2/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 1}), Document(page_content=\"first batch\\ntorch.Size([100, 1, 28, 28])\\ntensor([4, 6, 6, 1, 4, 9, 2, 8, 0, 3, 7, 8, 4, 2, 7, 1, 0, 9, 9, 5, 8, 9, 4, 0,\\n        2, 3, 4, 7, 7, 3, 8, 1, 1, 9, 3, 7, 9, 8, 5, 4, 3, 7, 4, 3, 7, 8, 3, 0,\\n        4, 3, 1, 7, 9, 4, 0, 2, 6, 9, 4, 9, 3, 7, 7, 9, 8, 4, 8, 4, 5, 8, 2, 1,\\n        7, 8, 5, 4, 5, 9, 9, 5, 1, 9, 2, 9, 7, 6, 3, 9, 5, 9, 2, 7, 5, 6, 3, 1,\\n        3, 8, 1, 9])\\nfor\\xa0img_batch ,\\xa0label_batch\\xa0 in\\xa0train_loader :\\n\\xa0\\xa0\\xa0\\xa0print('first\\xa0batch' )\\n\\xa0\\xa0\\xa0\\xa0print(img_batch.shape )\\n\\xa0\\xa0\\xa0\\xa0plt.imshow (img_batch [0][0],\\xa0cmap='gray')\\n\\xa0\\xa0\\xa0\\xa0print(label_batch )\\n\\xa0\\xa0\\xa0\\xa0break\\nStep 2. Train GAN t o produce images \\ue313\\nimage_size\\xa0\\xa0=\\xa0 784\\nhidden_size\\xa0=\\xa0 256\\nlatent_size\\xa0=\\xa0 645/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 3/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 2}), Document(page_content=\"D\\xa0=\\xa0nn.Sequential (\\n\\xa0\\xa0\\xa0\\xa0nn.Linear (image_size ,\\xa0hidden_size ),\\n\\xa0\\xa0\\xa0\\xa0nn.LeakyReLU (0.2),\\n\\xa0\\xa0\\xa0\\xa0nn.Linear (hidden_size ,\\xa0hidden_size ),\\n\\xa0\\xa0\\xa0\\xa0nn.LeakyReLU (0.2),\\n\\xa0\\xa0\\xa0\\xa0nn.Linear (hidden_size ,\\xa01),\\n\\xa0\\xa0\\xa0\\xa0nn.Sigmoid ())\\nG\\xa0=\\xa0nn.Sequential (\\n\\xa0\\xa0\\xa0\\xa0nn.Linear (latent_size ,\\xa0hidden_size ),\\n\\xa0\\xa0\\xa0\\xa0nn.ReLU (),\\n\\xa0\\xa0\\xa0\\xa0nn.Linear (hidden_size ,\\xa0hidden_size ),\\n\\xa0\\xa0\\xa0\\xa0nn.ReLU (),\\n\\xa0\\xa0\\xa0\\xa0nn.Linear (hidden_size ,\\xa0image_size ),\\n\\xa0\\xa0\\xa0\\xa0nn.Tanh ())\\nInstantiate the gener ator and gener age r andom fak e images \\ue313\\ny\\xa0=\\xa0G(torch.randn (2,\\xa0latent_size ))\\ngen_imgs\\xa0=\\xa0denorm (y.reshape ((-1,\\xa028,28)).detach())\\nplt.imshow (gen_imgs [0],\\xa0cmap='gray');5/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 4/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 3}), Document(page_content='Build the network and mo ve gener ator and discriminat or to the de vice \\ue313\\nD.to(device);\\nG.to(device);\\nG\\nSequential(\\n  (0): Linear(in_features=64, out_features=256, bias=True)\\n  (1): ReLU()\\n  (2): Linear(in_features=256, out_features=256, bias=True)\\n  (3): ReLU()\\n  (4): Linear(in_features=256, out_features=784, bias=True)\\n  (5): Tanh()\\n)\\nD\\nSequential(\\n  (0): Linear(in_features=784, out_features=256, bias=True)\\n  (1): LeakyReLU(negative_slope=0.2)\\n  (2): Linear(in_features=256, out_features=256, bias=True)\\n  (3): LeakyReLU(negative_slope=0.2)\\n  (4): Linear(in_features=256, out_features=1, bias=True)\\n  (5): Sigmoid()\\n)\\n#\\xa0loss\\ncriterion\\xa0=\\xa0nn.BCELoss ()\\n#\\xa0optimizer\\nlr\\xa0=\\xa00.0002\\nG_optimizer\\xa0=\\xa0optim.Adam (G.parameters (),\\xa0lr\\xa0=\\xa0lr )\\nD_optimizer\\xa0=\\xa0optim.Adam (D.parameters (),\\xa0lr\\xa0=\\xa0lr )5/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 5/25', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 4}), Document(page_content='def\\xa0reset_grad ():\\n\\xa0\\xa0\\xa0\\xa0D_optimizer.zero_grad ()\\n\\xa0\\xa0\\xa0\\xa0G_optimizer.zero_grad ()\\ndef\\xa0D_train(images):\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Create\\xa0the\\xa0labels\\xa0which\\xa0are\\xa0later\\xa0used\\xa0as\\xa0input\\xa0 for\\xa0the\\xa0BCE\\xa0loss\\n\\xa0\\xa0\\xa0\\xa0real_labels\\xa0=\\xa0torch.ones (batch_size ,\\xa01).to(device)\\n\\xa0\\xa0\\xa0\\xa0fake_labels\\xa0=\\xa0torch.zeros (batch_size ,\\xa01).to(device)\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Loss\\xa0for\\xa0real\\xa0images\\n\\xa0\\xa0\\xa0\\xa0outputs\\xa0=\\xa0D (images)\\n\\xa0\\xa0\\xa0\\xa0d_loss_real\\xa0=\\xa0criterion (outputs,\\xa0real_labels )\\n\\xa0\\xa0\\xa0\\xa0real_score\\xa0=\\xa0outputs\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Loss\\xa0for\\xa0fake\\xa0images\\n\\xa0\\xa0\\xa0\\xa0z\\xa0=\\xa0torch.randn (batch_size ,\\xa0latent_size ).to(device)\\n\\xa0\\xa0\\xa0\\xa0fake_images\\xa0=\\xa0G (z)\\n\\xa0\\xa0\\xa0\\xa0outputs\\xa0=\\xa0D (fake_images )\\n\\xa0\\xa0\\xa0\\xa0d_loss_fake\\xa0=\\xa0criterion (outputs,\\xa0fake_labels )\\n\\xa0\\xa0\\xa0\\xa0fake_score\\xa0=\\xa0outputs\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Combine\\xa0losses\\n\\xa0\\xa0\\xa0\\xa0d_loss\\xa0=\\xa0d_loss_real\\xa0+\\xa0d_loss_fake\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Reset\\xa0gradients\\n\\xa0\\xa0\\xa0\\xa0reset_grad ()\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Compute\\xa0gradients\\n\\xa0\\xa0\\xa0\\xa0d_loss.backward ()\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Adjust\\xa0the\\xa0parameters\\xa0using\\xa0backprop\\n\\xa0\\xa0\\xa0\\xa0D_optimizer.step ()\\n\\xa0\\xa0\\xa0\\xa0return\\xa0d_loss,\\xa0real_score ,\\xa0fake_score\\ndef\\xa0G_train():\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Generate\\xa0fake\\xa0images\\xa0and\\xa0calculate\\xa0loss\\n\\xa0\\xa0\\xa0\\xa0z\\xa0=\\xa0torch.randn (batch_size ,\\xa0latent_size ).to(device)\\n\\xa0\\xa0\\xa0\\xa0fake_images\\xa0=\\xa0G (z)\\n\\xa0\\xa0\\xa0\\xa0labels\\xa0=\\xa0torch.ones (batch_size ,\\xa01).to(device)\\n\\xa0\\xa0\\xa0\\xa0g_loss\\xa0=\\xa0criterion (D(fake_images ),\\xa0labels)\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Backprop\\xa0and\\xa0optimize\\n\\xa0\\xa0\\xa0\\xa0reset_grad ()\\n\\xa0\\xa0\\xa0\\xa0g_loss.backward ()\\n\\xa0\\xa0\\xa0\\xa0G_optimizer.step ()\\n\\xa0\\xa0\\xa0\\xa0return\\xa0g_loss,\\xa0fake_images\\nTrain the model\\ue3135/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 6/25', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 5}), Document(page_content=\"Create and sa ve intermediate outputs fr om the gener ator for visual inspection later \\ue313\\nimport\\xa0os\\nsample_dir\\xa0=\\xa0 'sample_data'\\nif\\xa0not\\xa0os.path.exists (sample_dir ):\\n\\xa0\\xa0\\xa0\\xa0os.makedirs (sample_dir )\\nBatch of r eal tr aining images \\ue313\\nfrom\\xa0IPython.display\\xa0 import\\xa0Image\\n#\\xa0Save\\xa0some\\xa0real\\xa0images\\nfor\\xa0images,\\xa0_\\xa0in\\xa0train_loader :\\n\\xa0\\xa0\\xa0\\xa0images\\xa0=\\xa0images.reshape (images.size (0),\\xa01,\\xa028,\\xa028)\\n\\xa0\\xa0\\xa0\\xa0save_image (denorm(images),\\xa0os.path.join (sample_dir ,\\xa0'real_images.png' ),\\xa0nrow=10)\\n\\xa0\\xa0\\xa0\\xa0break\\nImage(os.path.join (sample_dir ,\\xa0'real_images.png' ))\\nHelper function t o sa ve batch of gener ated images at end of ea epoch t o see\\nevolution o ver time\\ue3135/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 7/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 6}), Document(page_content=\"Saving fake_images-0000.png\\nsample_vectors\\xa0=\\xa0torch.randn (batch_size ,\\xa0latent_size ).to(device)\\ndef\\xa0save_fake_images (index):\\n\\xa0\\xa0\\xa0\\xa0fake_images\\xa0=\\xa0G (sample_vectors )\\n\\xa0\\xa0\\xa0\\xa0fake_images\\xa0=\\xa0fake_images.reshape (fake_images.size (0),\\xa01,\\xa028,\\xa028)\\n\\xa0\\xa0\\xa0\\xa0fake_fname\\xa0=\\xa0 'fake_images-{0:0=4d}.png' .format(index)\\n\\xa0\\xa0\\xa0\\xa0print('Saving' ,\\xa0fake_fname )\\n\\xa0\\xa0\\xa0\\xa0save_image (denorm(fake_images ),\\xa0os.path.join (sample_dir ,\\xa0fake_fname ),\\xa0nrow=10)\\n#\\xa0Before\\xa0training\\nsave_fake_images (0)\\nImage(os.path.join (sample_dir ,\\xa0'fake_images-0000.png' ))\\nTrain Model\\ue3135/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 8/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 7}), Document(page_content=\"%%time\\nnum_epochs\\xa0=\\xa0 300\\ntotal_step\\xa0=\\xa0 len(train_loader )\\nd_losses ,\\xa0g_losses ,\\xa0real_scores ,\\xa0fake_scores\\xa0=\\xa0 [],\\xa0[],\\xa0[],\\xa0[]\\nfor\\xa0epoch\\xa0in\\xa0range(num_epochs ):\\n\\xa0\\xa0\\xa0\\xa0for\\xa0i,\\xa0(images,\\xa0_)\\xa0in\\xa0enumerate (train_loader ):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Load\\xa0a\\xa0batch\\xa0&\\xa0transform\\xa0to\\xa0vectors\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0images\\xa0=\\xa0images.reshape (batch_size ,\\xa0-1).to(device)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Train\\xa0the\\xa0discriminator\\xa0and\\xa0generator\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0d_loss ,\\xa0real_score ,\\xa0fake_score\\xa0=\\xa0D_train (images)\\xa0#\\xa0discriminator\\xa0ingests\\xa0rea\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0g_loss ,\\xa0fake_images\\xa0=\\xa0G_train ()\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0generator\\xa0creates\\xa0images\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Inspect\\xa0the\\xa0losses\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\xa0(i+1)\\xa0%\\xa0200\\xa0==\\xa00:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0d_losses.append (d_loss.item ())\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0g_losses.append (g_loss.item ())\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0real_scores.append (real_score.mean ().item())\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0fake_scores.append (fake_score.mean ().item())\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 print('Epoch\\xa0[{}/{}],\\xa0Step\\xa0[{}/{}],\\xa0d_loss:\\xa0{:.4f},\\xa0g_lo ss:\\xa0{:.4f},\\xa0D(x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0. format(epoch,\\xa0num_epochs ,\\xa0i+1,\\xa0total_step ,\\xa0d_loss.item (),\\xa0g_loss.\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0real_score.mean ().item(),\\xa0fake_score.mean ().item()))\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Sample\\xa0and\\xa0save\\xa0images\\n\\xa0\\xa0\\xa0\\xa0save_fake_images (epoch+1)\\nEpoch [0/300], Step [200/600], d_loss: 0.0899, g_loss: 4.3959, D(x): 0.96, D(G(z\\nEpoch [0/300], Step [400/600], d_loss: 0.2149, g_loss: 5.8951, D(x): 0.90, D(G(z\\nEpoch [0/300], Step [600/600], d_loss: 0.1693, g_loss: 4.2356, D(x): 0.92, D(G(z\\nSaving fake_images-0001.png\\nEpoch [1/300], Step [200/600], d_loss: 0.2010, g_loss: 3.4154, D(x): 0.92, D(G(z\\nEpoch [1/300], Step [400/600], d_loss: 0.4308, g_loss: 2.9578, D(x): 0.85, D(G(z\\nEpoch [1/300], Step [600/600], d_loss: 0.3155, g_loss: 3.7022, D(x): 0.89, D(G(z\\nSaving fake_images-0002.png\\nEpoch [2/300], Step [200/600], d_loss: 0.8614, g_loss: 1.9833, D(x): 0.73, D(G(z\\nEpoch [2/300], Step [400/600], d_loss: 0.8980, g_loss: 3.6903, D(x): 0.67, D(G(z\\nEpoch [2/300], Step [600/600], d_loss: 0.1770, g_loss: 3.2625, D(x): 0.93, D(G(z\\nSaving fake_images-0003.png\\nEpoch [3/300], Step [200/600], d_loss: 0.4630, g_loss: 3.2015, D(x): 0.87, D(G(z\\nEpoch [3/300], Step [400/600], d_loss: 0.3005, g_loss: 3.7343, D(x): 0.94, D(G(z\\nEpoch [3/300], Step [600/600], d_loss: 0.2058, g_loss: 3.7432, D(x): 0.92, D(G(z\\nSaving fake_images-0004.png\\nEpoch [4/300], Step [200/600], d_loss: 0.7285, g_loss: 3.8536, D(x): 0.72, D(G(z\\nEpoch [4/300], Step [400/600], d_loss: 0.5269, g_loss: 3.9735, D(x): 0.84, D(G(z\\nEpoch [4/300], Step [600/600], d_loss: 0.5230, g_loss: 3.1785, D(x): 0.79, D(G(z\\nSaving fake_images-0005.png\\nEpoch [5/300], Step [200/600], d_loss: 0.4228, g_loss: 3.7234, D(x): 0.84, D(G(z\\nEpoch [5/300], Step [400/600], d_loss: 0.3359, g_loss: 3.7561, D(x): 0.88, D(G(z\\nEpoch [5/300], Step [600/600], d_loss: 0.3470, g_loss: 2.8234, D(x): 0.88, D(G(z\\nSaving fake_images-0006.png\\nEpoch [6/300], Step [200/600], d_loss: 0.1924, g_loss: 3.9460, D(x): 0.92, D(G(z5/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 9/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 8}), Document(page_content=\"Epoch [6/300], Step [400/600], d_loss: 0.3272, g_loss: 4.2030, D(x): 0.87, D(G(z\\nEpoch [6/300], Step [600/600], d_loss: 1.0456, g_loss: 3.3530, D(x): 0.71, D(G(z\\nSaving fake_images-0007.png\\nEpoch [7/300], Step [200/600], d_loss: 0.2176, g_loss: 3.3354, D(x): 0.94, D(G(z\\nEpoch [7/300], Step [400/600], d_loss: 0.4283, g_loss: 3.0464, D(x): 0.83, D(G(z\\nEpoch [7/300], Step [600/600], d_loss: 0.5261, g_loss: 3.3285, D(x): 0.87, D(G(z\\nSaving fake_images-0008.png\\nEpoch [8/300], Step [200/600], d_loss: 0.3052, g_loss: 4.3178, D(x): 0.90, D(G(z\\nEpoch [8/300], Step [400/600], d_loss: 0.4412, g_loss: 4.2961, D(x): 0.85, D(G(z\\nEpoch [8/300], Step [600/600], d_loss: 0.3429, g_loss: 3.0841, D(x): 0.90, D(G(z\\nSaving fake_images-0009.png\\nEpoch [9/300], Step [200/600], d_loss: 0.3318, g_loss: 3.2190, D(x): 0.91, D(G(z\\nEpoch [9/300], Step [400/600], d_loss: 0.3514, g_loss: 3.9454, D(x): 0.90, D(G(z\\nEpoch [9/300], Step [600/600], d_loss: 0.4781, g_loss: 4.5425, D(x): 0.79, D(G(z\\nSaving fake_images-0010.png\\nEpoch [10/300], Step [200/600], d_loss: 0.6771, g_loss: 4.1380, D(x): 0.77, D(G(\\nEpoch [10/300], Step [400/600], d_loss: 0.2611, g_loss: 3.6003, D(x): 0.91, D(G(\\nEpoch [10/300], Step [600/600], d_loss: 0.3264, g_loss: 2.7118, D(x): 0.90, D(G(\\nSaving fake_images-0011.png\\nEpoch [11/300], Step [200/600], d_loss: 0.3563, g_loss: 3.4108, D(x): 0.90, D(G(\\nEpoch [11/300], Step [400/600], d_loss: 0.4287, g_loss: 3.2734, D(x): 0.90, D(G(\\nEpoch [11/300], Step [600/600], d_loss: 0.5221, g_loss: 5.1330, D(x): 0.78, D(G(\\nSaving fake_images-0012.png\\nEpoch [12/300], Step [200/600], d_loss: 0.7478, g_loss: 3.0103, D(x): 0.84, D(G(\\nEpoch [12/300], Step [400/600], d_loss: 0.3199, g_loss: 3.5907, D(x): 0.91, D(G(\\nEpoch [12/300], Step [600/600], d_loss: 0.5902, g_loss: 4.5121, D(x): 0.77, D(G(\\nSaving fake_images-0013.png\\nEpoch [13/300], Step [200/600], d_loss: 0.6364, g_loss: 2.8478, D(x): 0.77, D(G(\\nEpoch [13/300], Step [400/600], d_loss: 0.5108, g_loss: 2.3321, D(x): 0.86, D(G(\\nEpoch [13/300], Step [600/600], d_loss: 1.1140, g_loss: 3.8358, D(x): 0.65, D(G(\\nSaving fake_images-0014.png\\nEpoch [14/300], Step [200/600], d_loss: 0.5655, g_loss: 2.7899, D(x): 0.78, D(G(\\nEh[14/300] S[400/600] dl04094 l30086D()086D(G(\\nimport\\xa0matplotlib.pyplot\\xa0 as\\xa0plt\\nplt.plot (d_losses ,\\xa0'-')\\nplt.plot (g_losses ,\\xa0'-')\\nplt.xlabel ('epoch')\\nplt.ylabel ('loss')\\nplt.legend (['Discriminator' ,\\xa0'Generator' ])\\nplt.title ('Losses' );5/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 10/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 9}), Document(page_content=\"plt.plot (real_scores ,\\xa0'-')\\nplt.plot (fake_scores ,\\xa0'-')\\nplt.xlabel ('epoch')\\nplt.ylabel ('score')\\nplt.legend (['Real\\xa0Score' ,\\xa0'Fake\\xa0score' ])\\nplt.title ('Scores' );5/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 11/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 10}), Document(page_content='Evaluate P erformance of GAN model (\\x00rst 300 epochs) \\ue3135/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 12/25', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 11}), Document(page_content=\"def\\xa0evaluate_discriminator (discriminator ,\\xa0test_loader ):\\n\\xa0\\xa0\\xa0\\xa0discriminator. eval()\\xa0\\xa0#\\xa0Set\\xa0the\\xa0model\\xa0to\\xa0evaluation\\xa0mode\\n\\xa0\\xa0\\xa0\\xa0correct_real\\xa0=\\xa0 0\\n\\xa0\\xa0\\xa0\\xa0correct_fake\\xa0=\\xa0 0\\n\\xa0\\xa0\\xa0\\xa0total\\xa0=\\xa0 0\\n\\xa0\\xa0\\xa0\\xa0with\\xa0torch.no_grad ():\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for\\xa0images,\\xa0_\\xa0in\\xa0test_loader :\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0images\\xa0=\\xa0images.reshape (-1,\\xa0image_size ).to(device)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0outputs\\xa0=\\xa0discriminator (images)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0predicted_labels\\xa0=\\xa0 (outputs\\xa0>=\\xa0 0.5).float()\\xa0\\xa0#\\xa0Threshold\\xa0at\\xa00.5\\xa0for\\xa0bina\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Compute\\xa0accuracy\\xa0for\\xa0real\\xa0and\\xa0fake\\xa0images\\xa0separa tely\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0correct_real\\xa0+=\\xa0 (predicted_labels [:len(images)//2]\\xa0==\\xa01).sum().item()\\xa0\\xa0#\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0correct_fake\\xa0+=\\xa0 (predicted_labels [len(images)//2:]\\xa0==\\xa00).sum().item()\\xa0\\xa0#\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0total\\xa0+=\\xa0 len(images)\\n\\xa0\\xa0\\xa0\\xa0accuracy_real\\xa0=\\xa0 100\\xa0*\\xa0correct_real\\xa0/\\xa0 (total\\xa0//\\xa0 2)\\n\\xa0\\xa0\\xa0\\xa0accuracy_fake\\xa0=\\xa0 100\\xa0*\\xa0correct_fake\\xa0/\\xa0 (total\\xa0//\\xa0 2)\\n\\xa0\\xa0\\xa0\\xa0accuracy_total\\xa0=\\xa0 100\\xa0*\\xa0(correct_real\\xa0+\\xa0correct_fake )\\xa0/\\xa0total\\n\\xa0\\xa0\\xa0\\xa0print('Accuracy\\xa0on\\xa0real\\xa0images:\\xa0{:.2f}%' .format(accuracy_real ))\\n\\xa0\\xa0\\xa0\\xa0print('Accuracy\\xa0on\\xa0fake\\xa0images:\\xa0{:.2f}%' .format(accuracy_fake ))\\n\\xa0\\xa0\\xa0\\xa0print('Overall\\xa0accuracy:\\xa0{:.2f}%' .format(accuracy_total ))\\n\\xa0\\xa0\\xa0\\xa0return\\xa0accuracy_real ,\\xa0accuracy_fake ,\\xa0accuracy_total\\n#\\xa0Usage\\nGAN_real ,\\xa0GAN_fake ,\\xa0GAN_overall\\xa0=\\xa0evaluate_discriminator (D,\\xa0test_loader )\\nAccuracy on real images: 65.14%\\nAccuracy on fake images: 36.76%\\nOverall accuracy: 50.95%\\nStep 3. Show 3+ samples cr eated b y GAN. Shar e lessons learned. \\ue313\\nShow fak e images after 10th, 50th, 100th and 300th epochs of tr aining \\ue3135/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 13/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 12}), Document(page_content=\"import\\xa0matplotlib.image\\xa0 as\\xa0mpimg\\n#\\xa0List\\xa0of\\xa0image\\xa0paths\\nimage_paths\\xa0=\\xa0 [\\n\\xa0\\xa0\\xa0\\xa0'./sample_data/fake_images-0010.png' ,\\n\\xa0\\xa0\\xa0\\xa0'./sample_data/fake_images-0050.png' ,\\n\\xa0\\xa0\\xa0\\xa0'./sample_data/fake_images-0100.png' ,\\n\\xa0\\xa0\\xa0\\xa0'./sample_data/fake_images-0300.png'\\n]\\n#\\xa0Plot\\xa0images\\xa0side\\xa0by\\xa0side\\nplt.figure (figsize= (28,\\xa028))\\nfor\\xa0i,\\xa0path\\xa0in\\xa0enumerate (image_paths ):\\n\\xa0\\xa0\\xa0\\xa0plt.subplot (1,\\xa0len(image_paths ),\\xa0i\\xa0+\\xa01)\\n\\xa0\\xa0\\xa0\\xa0img\\xa0=\\xa0mpimg.imread (path)\\n\\xa0\\xa0\\xa0\\xa0plt.imshow (img)\\n\\xa0\\xa0\\xa0\\xa0plt.axis ('off')\\nplt.show ()\\nGener ation of image samples called individually yields usable r esults.\\nGener ator impr oves with pr actice.\\nGets close t o real images.\\nGener ate a time series video. NO TE: Images must be r esized to match the\\ndimensions of the video fr ames.\\ue3135/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 14/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 13}), Document(page_content=\"gans_training.mp4import\\xa0cv2\\nimport\\xa0os\\nfrom\\xa0IPython.display\\xa0 import\\xa0FileLink\\nvid_fname\\xa0\\xa0=\\xa0 'gans_training.mp4'\\nfiles\\xa0=\\xa0 [os.path.join (sample_dir ,\\xa0f)\\xa0for\\xa0f\\xa0in\\xa0os.listdir (sample_dir )\\xa0if\\xa0'fake_images\\nfiles.sort ()\\nout\\xa0=\\xa0cv2.VideoWriter (vid_fname ,cv2.VideoWriter_fourcc (*'MP4V'),\\xa08,\\xa0(302,302))\\n[out.write (cv2.imread (fname))\\xa0for\\xa0fname\\xa0in\\xa0files]\\nout.release ()\\nFileLink ('gans_training.mp4' )\\nStep 4. Sa ve model weights with checkpoints \\ue313\\ntorch.save (G.state_dict (),\\xa0'generator_weights_checkpoints.ckpt' )\\ntorch.save (D.state_dict (),\\xa0'discriminator_weights_checkpoints.ckpt' )\\nStep 5. Load model\\ue313\\n#\\xa0Load\\xa0checkpoint\\xa0models\\xa0for\\xa0generator\\xa0and\\xa0discrim inator\\ngenerator_checkpoint_path\\xa0\\xa0\\xa0\\xa0\\xa0=\\xa0 'generator_weights_checkpoints.ckpt'\\ndiscriminator_checkpoint_path\\xa0=\\xa0 'discriminator_weights_checkpoints.ckpt'\\n#\\xa0Load\\xa0weights\\xa0into\\xa0the\\xa0models\\nG.load_state_dict (torch.load (generator_checkpoint_path ))\\nD.load_state_dict (torch.load (discriminator_checkpoint_path ))\\n<All keys matched successfully>\\n#\\xa0Set\\xa0models\\xa0to\\xa0evaluation\\xa0mode\\nG.eval()\\nSequential(\\n  (0): Linear(in_features=64, out_features=256, bias=True)\\n  (1): ReLU()\\n  (2): Linear(in_features=256, out_features=256, bias=True)\\n  (3): ReLU()\\n  (4): Linear(in_features=256, out_features=784, bias=True)\\n  (5): Tanh()\\n)\\nD.eval()5/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 15/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 14}), Document(page_content=\"Sequential(\\n  (0): Linear(in_features=784, out_features=256, bias=True)\\n  (1): LeakyReLU(negative_slope=0.2)\\n  (2): Linear(in_features=256, out_features=256, bias=True)\\n  (3): LeakyReLU(negative_slope=0.2)\\n  (4): Linear(in_features=256, out_features=1, bias=True)\\n  (5): Sigmoid()\\n)\\nStep 6. Re-tr ain GAN (100 epochs) \\ue313\\nSaving transf_fake_images-0000.png\\ndef\\xa0save_transf_fake_images (index):\\n\\xa0\\xa0\\xa0\\xa0fake_images\\xa0=\\xa0G (sample_vectors )\\n\\xa0\\xa0\\xa0\\xa0fake_images\\xa0=\\xa0fake_images.reshape (fake_images.size (0),\\xa01,\\xa028,\\xa028)\\n\\xa0\\xa0\\xa0\\xa0fake_fname\\xa0=\\xa0 'transf_fake_images-{0:0=4d}.png' .format(index)\\n\\xa0\\xa0\\xa0\\xa0print('Saving' ,\\xa0fake_fname )\\n\\xa0\\xa0\\xa0\\xa0save_image (denorm(fake_images ),\\xa0os.path.join (sample_dir ,\\xa0fake_fname ),\\xa0nrow=10)\\n#\\xa0Before\\xa0training\\nsave_transf_fake_images (0)\\nImage(os.path.join (sample_dir ,\\xa0'transf_fake_images-0000.png' ))5/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 16/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 15}), Document(page_content=\"%%time\\nnum_epochs\\xa0=\\xa0 100\\ntotal_step\\xa0=\\xa0 len(train_loader )\\nd_losses ,\\xa0g_losses ,\\xa0real_scores ,\\xa0fake_scores\\xa0=\\xa0 [],\\xa0[],\\xa0[],\\xa0[]\\nfor\\xa0epoch\\xa0in\\xa0range(num_epochs ):\\n\\xa0\\xa0\\xa0\\xa0for\\xa0i,\\xa0(images,\\xa0_)\\xa0in\\xa0enumerate (train_loader ):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Load\\xa0a\\xa0batch\\xa0&\\xa0transform\\xa0to\\xa0vectors\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0images\\xa0=\\xa0images.reshape (batch_size ,\\xa0-1).to(device)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Train\\xa0the\\xa0discriminator\\xa0and\\xa0generator\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0d_loss ,\\xa0real_score ,\\xa0fake_score\\xa0=\\xa0D_train (images)\\xa0#\\xa0discriminator\\xa0ingests\\xa0rea\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0g_loss ,\\xa0fake_images\\xa0=\\xa0G_train ()\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0generator\\xa0creates\\xa0images\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Inspect\\xa0the\\xa0losses\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\xa0(i+1)\\xa0%\\xa0200\\xa0==\\xa00:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0d_losses.append (d_loss.item ())\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0g_losses.append (g_loss.item ())\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0real_scores.append (real_score.mean ().item())\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0fake_scores.append (fake_score.mean ().item())\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 print('Epoch\\xa0[{}/{}],\\xa0Step\\xa0[{}/{}],\\xa0d_loss:\\xa0{:.4f},\\xa0g_lo ss:\\xa0{:.4f},\\xa0D(x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0. format(epoch,\\xa0num_epochs ,\\xa0i+1,\\xa0total_step ,\\xa0d_loss.item (),\\xa0g_loss.\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0real_score.mean ().item(),\\xa0fake_score.mean ().item()))\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Sample\\xa0and\\xa0save\\xa0images\\n\\xa0\\xa0\\xa0\\xa0save_transf_fake_images (epoch+1)\\nEpoch [0/100], Step [200/600], d_loss: 0.8590, g_loss: 1.7160, D(x): 0.68, D(G(z\\nEpoch [0/100], Step [400/600], d_loss: 0.8768, g_loss: 1.4377, D(x): 0.77, D(G(z\\nEpoch [0/100], Step [600/600], d_loss: 0.9805, g_loss: 1.4882, D(x): 0.64, D(G(z\\nSaving transf_fake_images-0001.png\\nEpoch [1/100], Step [200/600], d_loss: 0.9452, g_loss: 1.2370, D(x): 0.73, D(G(z\\nEpoch [1/100], Step [400/600], d_loss: 0.9002, g_loss: 1.7187, D(x): 0.63, D(G(z\\nEpoch [1/100], Step [600/600], d_loss: 0.7542, g_loss: 1.4563, D(x): 0.79, D(G(z\\nSaving transf_fake_images-0002.png\\nEpoch [2/100], Step [200/600], d_loss: 1.0868, g_loss: 1.3415, D(x): 0.72, D(G(z\\nEpoch [2/100], Step [400/600], d_loss: 0.9292, g_loss: 1.7201, D(x): 0.69, D(G(z\\nEpoch [2/100], Step [600/600], d_loss: 0.8240, g_loss: 1.6917, D(x): 0.73, D(G(z\\nSaving transf_fake_images-0003.png\\nEpoch [3/100], Step [200/600], d_loss: 0.9090, g_loss: 1.5493, D(x): 0.69, D(G(z\\nEpoch [3/100], Step [400/600], d_loss: 1.0896, g_loss: 1.7720, D(x): 0.64, D(G(z\\nEpoch [3/100], Step [600/600], d_loss: 0.7860, g_loss: 1.6753, D(x): 0.73, D(G(z\\nSaving transf_fake_images-0004.png\\nEpoch [4/100], Step [200/600], d_loss: 0.8495, g_loss: 1.4874, D(x): 0.74, D(G(z\\nEpoch [4/100], Step [400/600], d_loss: 1.1451, g_loss: 1.6324, D(x): 0.71, D(G(z\\nEpoch [4/100], Step [600/600], d_loss: 1.0033, g_loss: 1.4655, D(x): 0.69, D(G(z\\nSaving transf_fake_images-0005.png\\nEpoch [5/100], Step [200/600], d_loss: 0.9414, g_loss: 1.7263, D(x): 0.63, D(G(z\\nEpoch [5/100], Step [400/600], d_loss: 0.9534, g_loss: 1.6562, D(x): 0.66, D(G(z\\nEpoch [5/100], Step [600/600], d_loss: 0.7930, g_loss: 1.6494, D(x): 0.80, D(G(z\\nSaving transf_fake_images-0006.png\\nEpoch [6/100], Step [200/600], d_loss: 0.8373, g_loss: 1.4738, D(x): 0.68, D(G(z5/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 17/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 16}), Document(page_content=\"Epoch [6/100], Step [400/600], d_loss: 0.9604, g_loss: 1.4123, D(x): 0.69, D(G(z\\nEpoch [6/100], Step [600/600], d_loss: 0.7964, g_loss: 2.2006, D(x): 0.70, D(G(z\\nSaving transf_fake_images-0007.png\\nEpoch [7/100], Step [200/600], d_loss: 0.8545, g_loss: 1.6371, D(x): 0.75, D(G(z\\nEpoch [7/100], Step [400/600], d_loss: 1.1041, g_loss: 1.2763, D(x): 0.66, D(G(z\\nEpoch [7/100], Step [600/600], d_loss: 1.0368, g_loss: 1.4407, D(x): 0.70, D(G(z\\nSaving transf_fake_images-0008.png\\nEpoch [8/100], Step [200/600], d_loss: 0.9560, g_loss: 1.3123, D(x): 0.71, D(G(z\\nEpoch [8/100], Step [400/600], d_loss: 0.8008, g_loss: 2.0138, D(x): 0.68, D(G(z\\nEpoch [8/100], Step [600/600], d_loss: 1.0397, g_loss: 1.1578, D(x): 0.66, D(G(z\\nSaving transf_fake_images-0009.png\\nEpoch [9/100], Step [200/600], d_loss: 0.8883, g_loss: 1.8156, D(x): 0.69, D(G(z\\nEpoch [9/100], Step [400/600], d_loss: 0.9205, g_loss: 1.4928, D(x): 0.70, D(G(z\\nEpoch [9/100], Step [600/600], d_loss: 0.8847, g_loss: 1.5528, D(x): 0.66, D(G(z\\nSaving transf_fake_images-0010.png\\nEpoch [10/100], Step [200/600], d_loss: 1.0144, g_loss: 1.5923, D(x): 0.66, D(G(\\nEpoch [10/100], Step [400/600], d_loss: 0.9205, g_loss: 1.4008, D(x): 0.73, D(G(\\nEpoch [10/100], Step [600/600], d_loss: 0.8550, g_loss: 1.5597, D(x): 0.76, D(G(\\nSaving transf_fake_images-0011.png\\nEpoch [11/100], Step [200/600], d_loss: 1.0416, g_loss: 1.6035, D(x): 0.62, D(G(\\nEpoch [11/100], Step [400/600], d_loss: 1.0777, g_loss: 1.4967, D(x): 0.64, D(G(\\nEpoch [11/100], Step [600/600], d_loss: 0.9896, g_loss: 1.6013, D(x): 0.73, D(G(\\nSaving transf_fake_images-0012.png\\nEpoch [12/100], Step [200/600], d_loss: 0.8026, g_loss: 1.3385, D(x): 0.76, D(G(\\nEpoch [12/100], Step [400/600], d_loss: 0.7611, g_loss: 1.9218, D(x): 0.78, D(G(\\nEpoch [12/100], Step [600/600], d_loss: 1.0435, g_loss: 1.4299, D(x): 0.67, D(G(\\nSaving transf_fake_images-0013.png\\nEpoch [13/100], Step [200/600], d_loss: 0.8468, g_loss: 1.7631, D(x): 0.73, D(G(\\nEpoch [13/100], Step [400/600], d_loss: 0.8582, g_loss: 1.5410, D(x): 0.67, D(G(\\nEpoch [13/100], Step [600/600], d_loss: 0.9419, g_loss: 1.8377, D(x): 0.75, D(G(\\nSaving transf_fake_images-0014.png\\nEpoch [14/100], Step [200/600], d_loss: 0.9698, g_loss: 1.5276, D(x): 0.70, D(G(\\nEh[14/100] S[400/600] dl10664 l17683D()070D(G(\\nimport\\xa0matplotlib.pyplot\\xa0 as\\xa0plt\\nplt.plot (d_losses ,\\xa0'-')\\nplt.plot (g_losses ,\\xa0'-')\\nplt.xlabel ('epoch')\\nplt.ylabel ('loss')\\nplt.legend (['Discriminator' ,\\xa0'Generator' ])\\nplt.title ('Losses' );5/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 18/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 17}), Document(page_content=\"plt.plot (real_scores ,\\xa0'-')\\nplt.plot (fake_scores ,\\xa0'-')\\nplt.xlabel ('epoch')\\nplt.ylabel ('score')\\nplt.legend (['Real\\xa0Score' ,\\xa0'Fake\\xa0score' ])\\nplt.title ('Scores' );5/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 19/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 18}), Document(page_content=\"import\\xa0matplotlib.image\\xa0 as\\xa0mpimg\\n#\\xa0List\\xa0of\\xa0image\\xa0paths\\nimage_paths\\xa0=\\xa0 [\\n\\xa0\\xa0\\xa0\\xa0'./sample_data/transf_fake_images-0010.png' ,\\n\\xa0\\xa0\\xa0\\xa0'./sample_data/transf_fake_images-0050.png' ,\\n\\xa0\\xa0\\xa0\\xa0'./sample_data/transf_fake_images-0100.png'\\n]\\n#\\xa0Plot\\xa0images\\xa0side\\xa0by\\xa0side\\nplt.figure (figsize= (28,\\xa028))\\nfor\\xa0i,\\xa0path\\xa0in\\xa0enumerate (image_paths ):\\n\\xa0\\xa0\\xa0\\xa0plt.subplot (1,\\xa0len(image_paths ),\\xa0i\\xa0+\\xa01)\\n\\xa0\\xa0\\xa0\\xa0img\\xa0=\\xa0mpimg.imread (path)\\n\\xa0\\xa0\\xa0\\xa0plt.imshow (img)\\n\\xa0\\xa0\\xa0\\xa0plt.axis ('off')\\nplt.show ()5/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 20/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 19}), Document(page_content='Evaluate P erformance of tr anfer learned model \\ue3135/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 21/25', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 20}), Document(page_content=\"def\\xa0evaluate_discriminator (discriminator ,\\xa0test_loader ):\\n\\xa0\\xa0\\xa0\\xa0discriminator. eval()\\xa0\\xa0#\\xa0Set\\xa0the\\xa0model\\xa0to\\xa0evaluation\\xa0mode\\n\\xa0\\xa0\\xa0\\xa0correct_real\\xa0=\\xa0 0\\n\\xa0\\xa0\\xa0\\xa0correct_fake\\xa0=\\xa0 0\\n\\xa0\\xa0\\xa0\\xa0total\\xa0=\\xa0 0\\n\\xa0\\xa0\\xa0\\xa0with\\xa0torch.no_grad ():\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for\\xa0images,\\xa0_\\xa0in\\xa0test_loader :\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0images\\xa0=\\xa0images.reshape (-1,\\xa0image_size ).to(device)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0outputs\\xa0=\\xa0discriminator (images)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0predicted_labels\\xa0=\\xa0 (outputs\\xa0>=\\xa0 0.5).float()\\xa0\\xa0#\\xa0Threshold\\xa0at\\xa00.5\\xa0for\\xa0bina\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Compute\\xa0accuracy\\xa0for\\xa0real\\xa0and\\xa0fake\\xa0images\\xa0separa tely\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0correct_real\\xa0+=\\xa0 (predicted_labels [:len(images)//2]\\xa0==\\xa01).sum().item()\\xa0\\xa0#\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0correct_fake\\xa0+=\\xa0 (predicted_labels [len(images)//2:]\\xa0==\\xa00).sum().item()\\xa0\\xa0#\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0total\\xa0+=\\xa0 len(images)\\n\\xa0\\xa0\\xa0\\xa0accuracy_real\\xa0=\\xa0 100\\xa0*\\xa0correct_real\\xa0/\\xa0 (total\\xa0//\\xa0 2)\\n\\xa0\\xa0\\xa0\\xa0accuracy_fake\\xa0=\\xa0 100\\xa0*\\xa0correct_fake\\xa0/\\xa0 (total\\xa0//\\xa0 2)\\n\\xa0\\xa0\\xa0\\xa0accuracy_total\\xa0=\\xa0 100\\xa0*\\xa0(correct_real\\xa0+\\xa0correct_fake )\\xa0/\\xa0total\\n\\xa0\\xa0\\xa0\\xa0print('Accuracy\\xa0on\\xa0real\\xa0images:\\xa0{:.2f}%' .format(accuracy_real ))\\n\\xa0\\xa0\\xa0\\xa0print('Accuracy\\xa0on\\xa0fake\\xa0images:\\xa0{:.2f}%' .format(accuracy_fake ))\\n\\xa0\\xa0\\xa0\\xa0print('Overall\\xa0accuracy:\\xa0{:.2f}%' .format(accuracy_total ))\\n\\xa0\\xa0\\xa0\\xa0return\\xa0accuracy_real ,\\xa0accuracy_fake ,\\xa0accuracy_total\\n#\\xa0Usage\\ntrans_real ,\\xa0trans_fake ,\\xa0trans_overall\\xa0=\\xa0evaluate_discriminator (D,\\xa0test_loader )\\nAccuracy on real images: 69.58%\\nAccuracy on fake images: 29.14%\\nOverall accuracy: 49.36%\\nStep 7. Sa ve w/o checkpoints \\ue313\\ntorch.save (G.state_dict (),\\xa0'generator_weights_wo_checkpoints.pth' )\\ntorch.save (D.state_dict (),\\xa0'discriminator_weights_wo_checkpoints.pth' )\\nStep 8. Load w/o checkpoints\\ue3135/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 22/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 21}), Document(page_content=\"#\\xa0Load\\xa0checkpoint\\xa0models\\xa0for\\xa0generator\\xa0and\\xa0discrim inator\\ngenerator_checkpoint_path\\xa0\\xa0\\xa0\\xa0\\xa0=\\xa0 'generator_weights_wo_checkpoints.pth'\\ndiscriminator_checkpoint_path\\xa0=\\xa0 'discriminator_weights_wo_checkpoints.pth'\\n#\\xa0Load\\xa0weights\\xa0into\\xa0the\\xa0models\\nG.load_state_dict (torch.load (generator_checkpoint_path ))\\nD.load_state_dict (torch.load (discriminator_checkpoint_path ))\\n<All keys matched successfully>\\nG\\nSequential(\\n  (0): Linear(in_features=64, out_features=256, bias=True)\\n  (1): ReLU()\\n  (2): Linear(in_features=256, out_features=256, bias=True)\\n  (3): ReLU()\\n  (4): Linear(in_features=256, out_features=784, bias=True)\\n  (5): Tanh()\\n)\\nD\\nSequential(\\n  (0): Linear(in_features=784, out_features=256, bias=True)\\n  (1): LeakyReLU(negative_slope=0.2)\\n  (2): Linear(in_features=256, out_features=256, bias=True)\\n  (3): LeakyReLU(negative_slope=0.2)\\n  (4): Linear(in_features=256, out_features=1, bias=True)\\n  (5): Sigmoid()\\n)\\nPart 2 - LSGAN\\ue313\\nRepeat the steps 1-6 with Least Squar e GAN and compar e it with GAN r esults\\nUnsuppor ted Cell Type. Double-Click t o inspect/edit the content.\\nReinitializ e the model \\ue313\\nG.to(device);\\nD.to(device);5/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 23/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 22}), Document(page_content=\"#\\xa0For\\xa0LSGAN\\ncriterion\\xa0=\\xa0nn.MSELoss ()\\nSaving fake_LSGAN_images-0000.png\\ndef\\xa0save_fake_LSGAN_images (index):\\n\\xa0\\xa0\\xa0\\xa0fake_images\\xa0=\\xa0G (sample_vectors )\\n\\xa0\\xa0\\xa0\\xa0fake_images\\xa0=\\xa0fake_images.reshape (fake_images.size (0),\\xa01,\\xa028,\\xa028)\\n\\xa0\\xa0\\xa0\\xa0fake_fname\\xa0=\\xa0 'fake_LSGAN_images-{0:0=4d}.png' .format(index)\\n\\xa0\\xa0\\xa0\\xa0print('Saving' ,\\xa0fake_fname )\\n\\xa0\\xa0\\xa0\\xa0save_image (denorm(fake_images ),\\xa0os.path.join (sample_dir ,\\xa0fake_fname ),\\xa0nrow=10)\\n#\\xa0Before\\xa0training\\nsave_fake_LSGAN_images (0)\\nImage(os.path.join (sample_dir ,\\xa0'fake_LSGAN_images-0000.png' ))5/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 24/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 23}), Document(page_content=\"%%time\\nnum_epochs\\xa0=\\xa0 300\\ntotal_step\\xa0=\\xa0 len(train_loader )\\nd_losses ,\\xa0g_losses ,\\xa0real_scores ,\\xa0fake_scores\\xa0=\\xa0 [],\\xa0[],\\xa0[],\\xa0[]\\nfor\\xa0epoch\\xa0in\\xa0range(num_epochs ):\\n\\xa0\\xa0\\xa0\\xa0for\\xa0i,\\xa0(images,\\xa0_)\\xa0in\\xa0enumerate (train_loader ):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Load\\xa0a\\xa0batch\\xa0&\\xa0transform\\xa0to\\xa0vectors\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0images\\xa0=\\xa0images.reshape (batch_size ,\\xa0-1).to(device)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Train\\xa0the\\xa0discriminator\\xa0and\\xa0generator\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0d_loss ,\\xa0real_score ,\\xa0fake_score\\xa0=\\xa0D_train (images)\\xa0#\\xa0discriminator\\xa0ingests\\xa0rea\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0g_loss ,\\xa0fake_images\\xa0=\\xa0G_train ()\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0generator\\xa0creates\\xa0images\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Inspect\\xa0the\\xa0losses\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\xa0(i+1)\\xa0%\\xa0200\\xa0==\\xa00:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0d_losses.append (d_loss.item ())\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0g_losses.append (g_loss.item ())\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0real_scores.append (real_score.mean ().item())\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0fake_scores.append (fake_score.mean ().item())\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 print('Epoch\\xa0[{}/{}],\\xa0Step\\xa0[{}/{}],\\xa0d_loss:\\xa0{:.4f},\\xa0g_lo ss:\\xa0{:.4f},\\xa0D(x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0. format(epoch,\\xa0num_epochs ,\\xa0i+1,\\xa0total_step ,\\xa0d_loss.item (),\\xa0g_loss.\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0real_score.mean ().item(),\\xa0fake_score.mean ().item()))\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Sample\\xa0and\\xa0save\\xa0images\\n\\xa0\\xa0\\xa0\\xa0save_fake_LSGAN_images (epoch+1)\\nEpoch [0/300], Step [200/600], d_loss: 0.2916, g_loss: 0.4707, D(x): 0.67, D(G(z\\nEpoch [0/300], Step [400/600], d_loss: 0.3390, g_loss: 0.4438, D(x): 0.67, D(G(z\\nEpoch [0/300], Step [600/600], d_loss: 0.3594, g_loss: 0.4308, D(x): 0.68, D(G(z\\nSaving fake_LSGAN_images-0001.png\\nEpoch [1/300], Step [200/600], d_loss: 0.3522, g_loss: 0.4785, D(x): 0.60, D(G(z\\nEpoch [1/300], Step [400/600], d_loss: 0.3391, g_loss: 0.4873, D(x): 0.63, D(G(z\\nEpoch[1/300] Step[600/600] dloss:03226gloss:04723D(x):068D(G(z5/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 25/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 24}), Document(page_content='Homework 09: Transformer for German t o English Translation P ytorch TutorialSJSU MSDS 255 DL, Spring 2024 - Transformers \\ue313\\nGit: https:/ /github.com/jr gosalv ez/data255_DL\\n%matplotlib\\xa0 inline\\nimport\\xa0warnings\\nwarnings.filterwarnings (\"ignore\" )\\nTrain a tr anslation model fr om scr atch using Transformer .\\nUse t orchtext libr ary to access Multi30k  dataset t o train a German t o Englishtr anslation model.\\nBased on Tensor\\x00ow/K eras Tutorial: https:/ /www .tensor\\x00ow .org/text/tut orials/tr ansformerLanguage Translation with nn.Transformer and t orchtext\\nData Sour cing and Pr ocessing\\ntorchtext libr ary has utilities for cr eating datasets that can be easily iter ated thr ough for the purposes of cr eating a language tr anslation model.\\nIn this example, we show how t o use t orchtext\\' s inbuilt datasets, t okenize a r aw text sentence, build v ocabular y, and numericaliz e tokens int o\\ntensor . We will use Multi30k dataset fr om t orchtext libr ary that yields a pair of sour ce-tar get r aw sentences.\\nTo access t orchtext datasets, please install t orchdata following instructions at https:/ /github.com/p ytorch/data .Data Sour cing and Pr ocessing \\ue313\\nfrom\\xa0torchtext.data.utils\\xa0 import\\xa0get_tokenizer\\nfrom\\xa0torchtext.vocab\\xa0 import\\xa0build_vocab_from_iterator\\nfrom\\xa0torchtext.datasets\\xa0 import\\xa0multi30k ,\\xa0Multi30k\\nfrom\\xa0typing\\xa0 import\\xa0Iterable ,\\xa0List\\n#\\xa0We\\xa0need\\xa0to\\xa0modify\\xa0the\\xa0URLs\\xa0for\\xa0the\\xa0dataset\\xa0since \\xa0the\\xa0links\\xa0to\\xa0the\\xa0original\\xa0dataset\\xa0are\\xa0broken\\n#\\xa0Refer\\xa0to\\xa0https://github.com/pytorch/text/issues/ 1756#issuecomment-1163664163\\xa0for\\xa0more\\xa0info\\nmulti30k.URL [\"train\"]\\xa0=\\xa0\"https://raw.githubusercontent.com/neychev/small_D L_repo/master/datasets/Multi30k/training.tar.gz\"\\nmulti30k.URL [\"valid\"]\\xa0=\\xa0\"https://raw.githubusercontent.com/neychev/small_D L_repo/master/datasets/Multi30k/validation.tar.gz\"\\nSRC_LANGUAGE\\xa0=\\xa0 \\'de\\'\\nTGT_LANGUAGE\\xa0=\\xa0 \\'en\\'\\n#\\xa0Place-holders\\ntoken_transform\\xa0=\\xa0 {}\\nvocab_transform\\xa0=\\xa0 {}\\nInstall t orchdata and spacy , then reset k ernel  (Jup yter) or runtime (Colab) and skip this step  in the next run.\\n!pip\\xa0install\\xa0-U\\xa0torchdata\\n!pip\\xa0install\\xa0-U\\xa0spacy\\n!python\\xa0-m\\xa0spacy\\xa0download\\xa0en_core_web_sm\\n!python\\xa0-m\\xa0spacy\\xa0download\\xa0de_core_news_sm\\nRequirement already satisfied: torchdata in /usr/local/lib/python3.10/dist-packages (0.7.1)\\nRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.0.7)\\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.31.0)\\nRequirement already satisfied: torch>=2 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.2.1+cu121)\\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.13.4)\\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata\\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (1.12)\\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.3)\\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.1.3)\\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2023.6.0)\\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2->torchdata)\\n  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2->torchdata)5/5/24, 12:58 PM Jorge_Gosalvez_255_HW9_trans_de_en_Pytorch.ipynb - Colab\\nhttps://colab.research.google.com/drive/1RIjNTsxbQRFg1ofAdsZ4UrwOC0oyKZHy#printMode=true 1/8', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW9_trans_de_en.pdf', 'page': 0}), Document(page_content='  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2->torchdata)\\n  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\\nCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2->torchdata)\\n  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2->torchdata)\\n  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2->torchdata)\\n  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=2->torchdata)\\n  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2->torchdata)\\n  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2->torchdata)\\n  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\\nCollecting nvidia-nccl-cu12==2.19.3 (from torch>=2->torchdata)\\n  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=2->torchdata)\\n  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\\nRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2.2.0)\\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2->torchdata)\\n  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata\\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.6)\\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (202\\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2->torchdata)\\nRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2->torchdata) (1.\\nInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-c\\nSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cu\\nRequirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.4)\\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\\nRequirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.4)\\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.2)\\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) \\nSkip back t o this install of por tallock er step after r eimpor ting e verything but the t orchdata and spacy\\n!pip\\xa0install\\xa0portalocker>= 2.0.0\\nRestar t the k ernel (Jup yter) or runtime (Colab)\\nimport\\xa0os\\nos.kill(os.getpid (),\\xa09)5/5/24, 12:58 PM Jorge_Gosalvez_255_HW9_trans_de_en_Pytorch.ipynb - Colab\\nhttps://colab.research.google.com/drive/1RIjNTsxbQRFg1ofAdsZ4UrwOC0oyKZHy#printMode=true 2/8', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW9_trans_de_en.pdf', 'page': 1}), Document(page_content='token_transform [SRC_LANGUAGE ]\\xa0=\\xa0get_tokenizer (\\'spacy\\',\\xa0language= \\'de_core_news_sm\\' )\\ntoken_transform [TGT_LANGUAGE ]\\xa0=\\xa0get_tokenizer (\\'spacy\\',\\xa0language= \\'en_core_web_sm\\' )\\n#\\xa0helper\\xa0function\\xa0to\\xa0yield\\xa0list\\xa0of\\xa0tokens\\ndef\\xa0yield_tokens (data_iter :\\xa0Iterable ,\\xa0language :\\xa0str)\\xa0->\\xa0List [str]:\\n\\xa0\\xa0\\xa0\\xa0language_index\\xa0=\\xa0 {SRC_LANGUAGE :\\xa00,\\xa0TGT_LANGUAGE :\\xa01}\\n\\xa0\\xa0\\xa0\\xa0for\\xa0data_sample\\xa0 in\\xa0data_iter :\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 yield\\xa0token_transform [language ](data_sample [language_index [language ]])\\n#\\xa0Define\\xa0special\\xa0symbols\\xa0and\\xa0indices\\nUNK_IDX,\\xa0PAD_IDX ,\\xa0BOS_IDX ,\\xa0EOS_IDX\\xa0=\\xa0 0,\\xa01,\\xa02,\\xa03\\n#\\xa0Make\\xa0sure\\xa0the\\xa0tokens\\xa0are\\xa0in\\xa0order\\xa0of\\xa0their\\xa0indic es\\xa0to\\xa0properly\\xa0insert\\xa0them\\xa0in\\xa0vocab\\nspecial_symbols\\xa0=\\xa0 [\\'<unk>\\',\\xa0\\'<pad>\\',\\xa0\\'<bos>\\',\\xa0\\'<eos>\\']\\n\"\"\"\\nPurpose:\\xa0Defines\\xa0special\\xa0tokens\\xa0used\\xa0in\\xa0the\\xa0vocabu lary\\xa0for\\xa0machine\\xa0learning\\xa0tasks\\xa0with\\xa0text\\xa0data.\\nSpecial\\xa0Tokens:\\n<unk>:\\xa0\"Unknown\"\\xa0token\\xa0(represents\\xa0words\\xa0not\\xa0in\\xa0th e\\xa0vocabulary)\\n<pad>:\\xa0Padding\\xa0token\\xa0(to\\xa0make\\xa0sequences\\xa0the\\xa0same\\xa0l ength)\\n<bos>:\\xa0\"Beginning\\xa0of\\xa0Sequence\"\\n<eos>:\\xa0\"End\\xa0of\\xa0Sequence\"\\n\"\"\"\\nfor\\xa0ln\\xa0in\\xa0[SRC_LANGUAGE ,\\xa0TGT_LANGUAGE ]:\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Training\\xa0data\\xa0Iterator\\n\\xa0\\xa0\\xa0\\xa0train_iter\\xa0=\\xa0Multi30k (split=\\'train\\',\\xa0language_pair= (SRC_LANGUAGE ,\\xa0TGT_LANGUAGE ))\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Create\\xa0torchtext\\'s\\xa0Vocab\\xa0object\\n\\xa0\\xa0\\xa0\\xa0vocab_transform [ln]\\xa0=\\xa0build_vocab_from_iterator (yield_tokens (train_iter ,\\xa0ln),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0min_freq= 1,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0specials=special_symbols ,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0special_first= True)\\n#\\xa0Set\\xa0``UNK_IDX``\\xa0as\\xa0the\\xa0default\\xa0index.\\xa0This\\xa0index \\xa0is\\xa0returned\\xa0when\\xa0the\\xa0token\\xa0is\\xa0not\\xa0found.\\n#\\xa0If\\xa0not\\xa0set,\\xa0it\\xa0throws\\xa0``RuntimeError``\\xa0when\\xa0the\\xa0 queried\\xa0token\\xa0is\\xa0not\\xa0found\\xa0in\\xa0the\\xa0Vocabulary.\\nfor\\xa0ln\\xa0in\\xa0[SRC_LANGUAGE ,\\xa0TGT_LANGUAGE ]:\\n\\xa0\\xa0vocab_transform [ln].set_default_index (UNK_IDX)\\nThe original Transformer diagr am A representation of a 4-la yer Transformer\\nEach of the components in these two diagr ams will be explained as y ou pr ogress thr ough the tut orial.5/5/24, 12:58 PM Jorge_Gosalvez_255_HW9_trans_de_en_Pytorch.ipynb - Colab\\nhttps://colab.research.google.com/drive/1RIjNTsxbQRFg1ofAdsZ4UrwOC0oyKZHy#printMode=true 3/8', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW9_trans_de_en.pdf', 'page': 2}), Document(page_content='Transformer is a Seq2Seq model intr oduced in \"Attention is all y ou need\"  paper for solving machine tr anslation tasks.\\nCreate a Seq2Seq network that uses Transformer . The network consists of thr ee par ts. First par t is the embedding la yer which conv erts tensor\\nof input indices int o corr esponding tensor of input embeddings. These embedding ar e fur ther augmented with positional encodings t o provide\\nposition information of input t okens t o the model. The second par t is the actual Transformer  model. Finally , the output of the Transformer\\nmodel is passed thr ough linear la yer that giv es unnormaliz ed pr obabilities for each t oken in the tar get language.Seq2Seq Network using Transformer \\ue3135/5/24, 12:58 PM Jorge_Gosalvez_255_HW9_trans_de_en_Pytorch.ipynb - Colab\\nhttps://colab.research.google.com/drive/1RIjNTsxbQRFg1ofAdsZ4UrwOC0oyKZHy#printMode=true 4/8', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW9_trans_de_en.pdf', 'page': 3}), Document(page_content=\"from\\xa0torch\\xa0import\\xa0Tensor\\nimport\\xa0torch\\nimport\\xa0torch.nn\\xa0 as\\xa0nn\\nfrom\\xa0torch.nn\\xa0 import\\xa0Transformer\\nimport\\xa0math\\nDEVICE\\xa0=\\xa0torch.device ('cuda'\\xa0if\\xa0torch.cuda.is_available ()\\xa0else\\xa0'cpu')\\n#\\xa0helper\\xa0Module\\xa0that\\xa0adds\\xa0positional\\xa0encoding\\xa0to\\xa0t he\\xa0token\\xa0embedding\\xa0to\\xa0introduce\\xa0a\\xa0notion\\xa0of\\xa0word\\xa0o rder.\\nclass\\xa0PositionalEncoding (nn.Module):\\n\\xa0\\xa0\\xa0\\xa0def\\xa0__init__ (self,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 emb_size :\\xa0int,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 dropout:\\xa0float,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 maxlen:\\xa0int\\xa0=\\xa05000):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0super (PositionalEncoding ,\\xa0self).__init__ ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0den\\xa0=\\xa0torch.exp (-\\xa0torch.arange (0,\\xa0emb_size ,\\xa02)*\\xa0math.log (10000)\\xa0/\\xa0emb_size )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0pos\\xa0=\\xa0torch.arange (0,\\xa0maxlen).reshape (maxlen,\\xa01)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0pos_embedding\\xa0=\\xa0torch.zeros ((maxlen,\\xa0emb_size ))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0pos_embedding [:,\\xa00::2]\\xa0=\\xa0torch.sin (pos\\xa0*\\xa0den )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0pos_embedding [:,\\xa01::2]\\xa0=\\xa0torch.cos (pos\\xa0*\\xa0den )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0pos_embedding\\xa0=\\xa0pos_embedding.unsqueeze (-2)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.dropout\\xa0=\\xa0nn.Dropout (dropout)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.register_buffer ('pos_embedding' ,\\xa0pos_embedding )\\n\\xa0\\xa0\\xa0\\xa0def\\xa0forward(self,\\xa0token_embedding :\\xa0Tensor):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return\\xa0self.dropout (token_embedding\\xa0+\\xa0 self.pos_embedding [:token_embedding.size (0),\\xa0:])\\n#\\xa0helper\\xa0Module\\xa0to\\xa0convert\\xa0tensor\\xa0of\\xa0input\\xa0indices \\xa0into\\xa0corresponding\\xa0tensor\\xa0of\\xa0token\\xa0embeddings\\nclass\\xa0TokenEmbedding (nn.Module):\\n\\xa0\\xa0\\xa0\\xa0def\\xa0__init__ (self,\\xa0vocab_size :\\xa0int,\\xa0emb_size ):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0super (TokenEmbedding ,\\xa0self).__init__ ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.embedding\\xa0=\\xa0nn.Embedding (vocab_size ,\\xa0emb_size )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.emb_size\\xa0=\\xa0emb_size\\n\\xa0\\xa0\\xa0\\xa0def\\xa0forward(self,\\xa0tokens:\\xa0Tensor):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return\\xa0self.embedding (tokens.long ())\\xa0*\\xa0math.sqrt (self.emb_size )\\n#\\xa0Seq2Seq\\xa0Network\\nclass\\xa0Seq2SeqTransformer (nn.Module):\\n\\xa0\\xa0\\xa0\\xa0def\\xa0__init__ (self,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 num_encoder_layers :\\xa0int,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 num_decoder_layers :\\xa0int,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 emb_size :\\xa0int,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 nhead:\\xa0int,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 src_vocab_size :\\xa0int,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 tgt_vocab_size :\\xa0int,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 dim_feedforward :\\xa0int\\xa0=\\xa0512,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 dropout:\\xa0float\\xa0=\\xa00.1):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0super (Seq2SeqTransformer ,\\xa0self).__init__ ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.transformer\\xa0=\\xa0Transformer (d_model=emb_size ,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0nhead=nhead ,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0num_encoder _layers=num_encoder_layers ,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0num_decoder _layers=num_decoder_layers ,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0dim_feedfor ward=dim_feedforward ,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0dropout=dro pout)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.generator\\xa0=\\xa0nn.Linear (emb_size ,\\xa0tgt_vocab_size )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.src_tok_emb\\xa0=\\xa0TokenEmbedding (src_vocab_size ,\\xa0emb_size )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.tgt_tok_emb\\xa0=\\xa0TokenEmbedding (tgt_vocab_size ,\\xa0emb_size )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.positional_encoding\\xa0=\\xa0PositionalEncoding (\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0emb_size ,\\xa0dropout=dropout )\\n\\xa0\\xa0\\xa0\\xa0def\\xa0forward(self,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 src:\\xa0Tensor,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 trg:\\xa0Tensor,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 src_mask :\\xa0Tensor,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 tgt_mask :\\xa0Tensor,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 src_padding_mask :\\xa0Tensor,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 tgt_padding_mask :\\xa0Tensor,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 memory_key_padding_mask :\\xa0Tensor):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0src_emb\\xa0=\\xa0 self.positional_encoding (self.src_tok_emb (src))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0tgt_emb\\xa0=\\xa0 self.positional_encoding (self.tgt_tok_emb (trg))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0outs\\xa0=\\xa0 self.transformer (src_emb,\\xa0tgt_emb ,\\xa0src_mask ,\\xa0tgt_mask ,\\xa0None,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0src_padding_mask ,\\xa0tgt_padding_mask ,\\xa0memory_key_padding_mask )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return\\xa0self.generator (outs)\\n\\xa0\\xa0\\xa0\\xa0def\\xa0encode(self,\\xa0src:\\xa0Tensor,\\xa0src_mask :\\xa0Tensor):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return\\xa0self.transformer.encoder (self.positional_encoding (5/5/24, 12:58 PM Jorge_Gosalvez_255_HW9_trans_de_en_Pytorch.ipynb - Colab\\nhttps://colab.research.google.com/drive/1RIjNTsxbQRFg1ofAdsZ4UrwOC0oyKZHy#printMode=true 5/8\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW9_trans_de_en.pdf', 'page': 4}), Document(page_content=\"\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.src_tok_emb (src)),\\xa0src_mask )\\n\\xa0\\xa0\\xa0\\xa0def\\xa0decode(self,\\xa0tgt:\\xa0Tensor,\\xa0memory:\\xa0Tensor,\\xa0tgt_mask :\\xa0Tensor):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return\\xa0self.transformer.decoder (self.positional_encoding (\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.tgt_tok_emb (tgt)),\\xa0memory,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0tgt_mask )\\nDuring tr aining, use a subsequent wor d mask that will pr event the model fr om looking int o the futur e wor ds when making pr edictions. Use\\nmasks t o hide sour ce and tar get padding t okens.\\ndef\\xa0generate_square_subsequent_mask (sz):\\n\\xa0\\xa0\\xa0\\xa0mask\\xa0=\\xa0 (torch.triu (torch.ones ((sz,\\xa0sz),\\xa0device=DEVICE ))\\xa0==\\xa01).transpose (0,\\xa01)\\n\\xa0\\xa0\\xa0\\xa0mask\\xa0=\\xa0mask. float().masked_fill (mask\\xa0==\\xa0 0,\\xa0float('-inf')).masked_fill (mask\\xa0==\\xa0 1,\\xa0float(0.0))\\n\\xa0\\xa0\\xa0\\xa0return\\xa0mask\\ndef\\xa0create_mask (src,\\xa0tgt):\\n\\xa0\\xa0\\xa0\\xa0src_seq_len\\xa0=\\xa0src.shape [0]\\n\\xa0\\xa0\\xa0\\xa0tgt_seq_len\\xa0=\\xa0tgt.shape [0]\\n\\xa0\\xa0\\xa0\\xa0tgt_mask\\xa0=\\xa0generate_square_subsequent_mask (tgt_seq_len )\\n\\xa0\\xa0\\xa0\\xa0src_mask\\xa0=\\xa0torch.zeros ((src_seq_len ,\\xa0src_seq_len ),device=DEVICE ).type(torch.bool)\\n\\xa0\\xa0\\xa0\\xa0src_padding_mask\\xa0=\\xa0 (src\\xa0==\\xa0PAD_IDX ).transpose (0,\\xa01)\\n\\xa0\\xa0\\xa0\\xa0tgt_padding_mask\\xa0=\\xa0 (tgt\\xa0==\\xa0PAD_IDX ).transpose (0,\\xa01)\\n\\xa0\\xa0\\xa0\\xa0return\\xa0src_mask ,\\xa0tgt_mask ,\\xa0src_padding_mask ,\\xa0tgt_padding_mask\\nDe\\x00ne the par ameters of the model and instantiate the same.\\nDe\\x00ne the loss function as cr oss-entr opy loss and the Adam optimiz er used for tr aining.\\ntorch.manual_seed (0)\\nSRC_VOCAB_SIZE\\xa0=\\xa0 len(vocab_transform [SRC_LANGUAGE ])\\nTGT_VOCAB_SIZE\\xa0=\\xa0 len(vocab_transform [TGT_LANGUAGE ])\\nEMB_SIZE\\xa0=\\xa0 512\\nNHEAD\\xa0=\\xa0 8\\nFFN_HID_DIM\\xa0=\\xa0 512\\nBATCH_SIZE\\xa0=\\xa0 128\\nNUM_ENCODER_LAYERS\\xa0=\\xa0 3\\nNUM_DECODER_LAYERS\\xa0=\\xa0 3\\ntransformer\\xa0=\\xa0Seq2SeqTransformer (NUM_ENCODER_LAYERS ,\\xa0NUM_DECODER_LAYERS ,\\xa0EMB_SIZE ,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0NHEAD ,\\xa0SRC_VOCAB_SIZE ,\\xa0TGT_VOCAB_SIZE ,\\xa0FFN_HID_DIM )\\nfor\\xa0p\\xa0in\\xa0transformer.parameters ():\\n\\xa0\\xa0\\xa0\\xa0if\\xa0p.dim()\\xa0>\\xa01:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0nn.init.xavier_uniform_ (p)\\ntransformer\\xa0=\\xa0transformer.to (DEVICE)\\nloss_fn\\xa0=\\xa0torch.nn.CrossEntropyLoss (ignore_index=PAD_IDX )\\noptimizer\\xa0=\\xa0torch.optim.Adam (transformer.parameters (),\\xa0lr=0.0001,\\xa0betas=(0.9,\\xa00.98),\\xa0eps=1e-9)\\nAs seen in the Data Sourcing and Processing section, the data iter ator yields a pair of r aw strings. Conv ert string pairs int o the batched\\ntensors that can be pr ocessed b y our Seq2Seq network de\\x00ned pr eviously . De\\x00ne our collate function that conv erts a batch of r aw strings int o\\nbatch tensors t o feed dir ectly int o the model.Collation\\ue3135/5/24, 12:58 PM Jorge_Gosalvez_255_HW9_trans_de_en_Pytorch.ipynb - Colab\\nhttps://colab.research.google.com/drive/1RIjNTsxbQRFg1ofAdsZ4UrwOC0oyKZHy#printMode=true 6/8\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW9_trans_de_en.pdf', 'page': 5}), Document(page_content='from\\xa0torch.nn.utils.rnn\\xa0 import\\xa0pad_sequence\\n#\\xa0helper\\xa0function\\xa0to\\xa0club\\xa0together\\xa0sequential\\xa0oper ations\\ndef\\xa0sequential_transforms (*transforms ):\\n\\xa0\\xa0\\xa0\\xa0def\\xa0func(txt_input ):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for\\xa0transform\\xa0 in\\xa0transforms :\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0txt_input\\xa0=\\xa0transform (txt_input )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return\\xa0txt_input\\n\\xa0\\xa0\\xa0\\xa0return\\xa0func\\n#\\xa0function\\xa0to\\xa0add\\xa0BOS/EOS\\xa0and\\xa0create\\xa0tensor\\xa0for\\xa0in put\\xa0sequence\\xa0indices\\ndef\\xa0tensor_transform (token_ids :\\xa0List[int]):\\n\\xa0\\xa0\\xa0\\xa0return\\xa0torch.cat ((torch.tensor ([BOS_IDX]),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0torch.tensor (token_ids ),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0torch.tensor ([EOS_IDX])))\\n#\\xa0``src``\\xa0and\\xa0``tgt``\\xa0language\\xa0text\\xa0transforms\\xa0to\\xa0 convert\\xa0raw\\xa0strings\\xa0into\\xa0tensors\\xa0indices\\ntext_transform\\xa0=\\xa0 {}\\nfor\\xa0ln\\xa0in\\xa0[SRC_LANGUAGE ,\\xa0TGT_LANGUAGE ]:\\n\\xa0\\xa0\\xa0\\xa0text_transform [ln]\\xa0=\\xa0sequential_transforms (token_transform [ln],\\xa0#Tokenization\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0voc ab_transform [ln],\\xa0#Numericalization\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0ten sor_transform )\\xa0#\\xa0Add\\xa0BOS/EOS\\xa0and\\xa0create\\xa0tensor\\n#\\xa0function\\xa0to\\xa0collate\\xa0data\\xa0samples\\xa0into\\xa0batch\\xa0tens ors\\ndef\\xa0collate_fn (batch):\\n\\xa0\\xa0\\xa0\\xa0src_batch ,\\xa0tgt_batch\\xa0=\\xa0 [],\\xa0[]\\n\\xa0\\xa0\\xa0\\xa0for\\xa0src_sample ,\\xa0tgt_sample\\xa0 in\\xa0batch:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0src_batch.append (text_transform [SRC_LANGUAGE ](src_sample.rstrip (\"\\\\n\")))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0tgt_batch.append (text_transform [TGT_LANGUAGE ](tgt_sample.rstrip (\"\\\\n\")))\\n\\xa0\\xa0\\xa0\\xa0src_batch\\xa0=\\xa0pad_sequence (src_batch ,\\xa0padding_value=PAD_IDX )\\n\\xa0\\xa0\\xa0\\xa0tgt_batch\\xa0=\\xa0pad_sequence (tgt_batch ,\\xa0padding_value=PAD_IDX )\\n\\xa0\\xa0\\xa0\\xa0return\\xa0src_batch ,\\xa0tgt_batch\\nDe\\x00ne tr aining and e valuation loop that will be called for each epoch.5/5/24, 12:58 PM Jorge_Gosalvez_255_HW9_trans_de_en_Pytorch.ipynb - Colab\\nhttps://colab.research.google.com/drive/1RIjNTsxbQRFg1ofAdsZ4UrwOC0oyKZHy#printMode=true 7/8', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW9_trans_de_en.pdf', 'page': 6}), Document(page_content='from\\xa0torch.utils.data\\xa0 import\\xa0DataLoader\\ndef\\xa0train_epoch (model,\\xa0optimizer ):\\n\\xa0\\xa0\\xa0\\xa0model.train ()\\n\\xa0\\xa0\\xa0\\xa0losses\\xa0=\\xa0 0\\n\\xa0\\xa0\\xa0\\xa0train_iter\\xa0=\\xa0Multi30k (split=\\'train\\',\\xa0language_pair= (SRC_LANGUAGE ,\\xa0TGT_LANGUAGE ))\\n\\xa0\\xa0\\xa0\\xa0train_dataloader\\xa0=\\xa0DataLoader (train_iter ,\\xa0batch_size=BATCH_SIZE ,\\xa0collate_fn=collate_fn )\\n\\xa0\\xa0\\xa0\\xa0for\\xa0src,\\xa0tgt\\xa0in\\xa0train_dataloader :\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0src\\xa0=\\xa0src.to (DEVICE)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0tgt\\xa0=\\xa0tgt.to (DEVICE)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0tgt_input\\xa0=\\xa0tgt [:-1,\\xa0:]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0src_mask ,\\xa0tgt_mask ,\\xa0src_padding_mask ,\\xa0tgt_padding_mask\\xa0=\\xa0create_mask (src,\\xa0tgt_input )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0logits\\xa0=\\xa0model (src,\\xa0tgt_input ,\\xa0src_mask ,\\xa0tgt_mask ,src_padding_mask ,\\xa0tgt_padding_mask ,\\xa0src_padding_mask )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0optimizer.zero_grad ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0tgt_out\\xa0=\\xa0tgt [1:,\\xa0:]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0loss\\xa0=\\xa0loss_fn (logits.reshape (-1,\\xa0logits.shape [-1]),\\xa0tgt_out.reshape (-1))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0loss.backward ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0optimizer.step ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0losses\\xa0+=\\xa0loss.item ()\\n\\xa0\\xa0\\xa0\\xa0return\\xa0losses\\xa0/\\xa0 len(list(train_dataloader ))\\ndef\\xa0evaluate (model):\\n\\xa0\\xa0\\xa0\\xa0model. eval()\\n\\xa0\\xa0\\xa0\\xa0losses\\xa0=\\xa0 0\\n\\xa0\\xa0\\xa0\\xa0val_iter\\xa0=\\xa0Multi30k (split=\\'valid\\',\\xa0language_pair= (SRC_LANGUAGE ,\\xa0TGT_LANGUAGE ))\\n\\xa0\\xa0\\xa0\\xa0val_dataloader\\xa0=\\xa0DataLoader (val_iter ,\\xa0batch_size=BATCH_SIZE ,\\xa0collate_fn=collate_fn )\\n\\xa0\\xa0\\xa0\\xa0for\\xa0src,\\xa0tgt\\xa0in\\xa0val_dataloader :\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0src\\xa0=\\xa0src.to (DEVICE)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0tgt\\xa0=\\xa0tgt.to (DEVICE)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0tgt_input\\xa0=\\xa0tgt [:-1,\\xa0:]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0src_mask ,\\xa0tgt_mask ,\\xa0src_padding_mask ,\\xa0tgt_padding_mask\\xa0=\\xa0create_mask (src,\\xa0tgt_input )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0logits\\xa0=\\xa0model (src,\\xa0tgt_input ,\\xa0src_mask ,\\xa0tgt_mask ,src_padding_mask ,\\xa0tgt_padding_mask ,\\xa0src_padding_mask )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0tgt_out\\xa0=\\xa0tgt [1:,\\xa0:]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0loss\\xa0=\\xa0loss_fn (logits.reshape (-1,\\xa0logits.shape [-1]),\\xa0tgt_out.reshape (-1))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0losses\\xa0+=\\xa0loss.item ()\\n\\xa0\\xa0\\xa0\\xa0return\\xa0losses\\xa0/\\xa0 len(list(val_dataloader ))Train the model.\\nfrom\\xa0timeit\\xa0 import\\xa0default_timer\\xa0 as\\xa0timer\\nNUM_EPOCHS\\xa0=\\xa0 25\\nfor\\xa0epoch\\xa0in\\xa0range(1,\\xa0NUM_EPOCHS+ 1):\\n\\xa0\\xa0\\xa0\\xa0start_time\\xa0=\\xa0timer ()\\n\\xa0\\xa0\\xa0\\xa0train_loss\\xa0=\\xa0train_epoch (transformer ,\\xa0optimizer )\\n\\xa0\\xa0\\xa0\\xa0end_time\\xa0=\\xa0timer ()\\n\\xa0\\xa0\\xa0\\xa0val_loss\\xa0=\\xa0evaluate (transformer )\\n\\xa0\\xa0\\xa0\\xa0print((f\"Epoch:\\xa0 {epoch},\\xa0Train\\xa0loss:\\xa0 {train_loss :.3f},\\xa0Val\\xa0loss:\\xa0 {val_loss :.3f},\\xa0\"f\"Epoch\\xa0time\\xa0=\\xa0 {(end_time\\xa0-\\xa0start_time ):.3\\n#\\xa0function\\xa0to\\xa0generate\\xa0output\\xa0sequence\\xa0using\\xa0greed y\\xa0algorithm\\ndef\\xa0greedy_decode (model,\\xa0src,\\xa0src_mask ,\\xa0max_len,\\xa0start_symbol ):\\n\\xa0\\xa0\\xa0\\xa0src\\xa0=\\xa0src.to (DEVICE)\\n\\xa0\\xa0\\xa0\\xa0src_mask\\xa0=\\xa0src_mask.to (DEVICE)\\n\\xa0\\xa0\\xa0\\xa0memory\\xa0=\\xa0model.encode (src,\\xa0src_mask )\\n\\xa0\\xa0\\xa0\\xa0ys\\xa0=\\xa0torch.ones (1,\\xa01).fill_(start_symbol ).type(torch.long).to(DEVICE)\\n\\xa0\\xa0\\xa0\\xa0for\\xa0i\\xa0in\\xa0range(max_len-1):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0memory\\xa0=\\xa0memory.to (DEVICE)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0tgt_mask\\xa0=\\xa0 (generate_square_subsequent_mask (ys.size(0))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0. type(torch.bool)).to(DEVICE)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0out\\xa0=\\xa0model.decode (ys,\\xa0memory,\\xa0tgt_mask )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0out\\xa0=\\xa0out.transpose (0,\\xa01)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0prob\\xa0=\\xa0model.generator (out[:,\\xa0-1])\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0_ ,\\xa0next_word\\xa0=\\xa0torch. max(prob,\\xa0dim=1)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0next_word\\xa0=\\xa0next_word.item ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0ys\\xa0=\\xa0torch.cat ([ys,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0torch.ones (1,\\xa01).type_as (src.data ).fill_(next_word )],\\xa0dim=0)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\xa0next_word\\xa0==\\xa0EOS_IDX :\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 break\\n\\xa0\\xa0\\xa0\\xa0return\\xa0ys\\n#\\xa0actual\\xa0function\\xa0to\\xa0translate\\xa0input\\xa0sentence\\xa0into \\xa0target\\xa0language\\ndef\\xa0translate (model:\\xa0torch.nn.Module,\\xa0src_sentence :\\xa0str):\\n\\xa0\\xa0\\xa0\\xa0model. eval()\\n\\xa0\\xa0\\xa0\\xa0src\\xa0=\\xa0text_transform [SRC_LANGUAGE ](src_sentence ).view(-1,\\xa01)\\n\\xa0\\xa0\\xa0\\xa0num_tokens\\xa0=\\xa0src.shape [0]\\n\\xa0\\xa0\\xa0\\xa0src_mask\\xa0=\\xa0 (torch.zeros (num_tokens ,\\xa0num_tokens )).type(torch.bool)\\n\\xa0\\xa0\\xa0\\xa0tgt_tokens\\xa0=\\xa0greedy_decode (\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0model ,\\xa0\\xa0src,\\xa0src_mask ,\\xa0max_len=num_tokens\\xa0+\\xa0 5,\\xa0start_symbol=BOS_IDX ).flatten ()\\n\\xa0\\xa0\\xa0\\xa0return\\xa0\"\\xa0\".join(vocab_transform [TGT_LANGUAGE ].lookup_tokens (list(tgt_tokens.cpu ().numpy()))).replace (\"<bos>\",\\xa0\"\").replace (\"<\\nEpoch: 1, Train loss: 5.337, Val loss: 4.102, Epoch time = 23.990s\\nEpoch: 2, Train loss: 3.760, Val loss: 3.312, Epoch time = 21.813s\\nEpoch: 3, Train loss: 3.159, Val loss: 2.882, Epoch time = 21.874s\\nEpoch: 4, Train loss: 2.768, Val loss: 2.631, Epoch time = 21.996s\\nEpoch: 5, Train loss: 2.479, Val loss: 2.439, Epoch time = 22.149s\\nEpoch: 6, Train loss: 2.251, Val loss: 2.317, Epoch time = 22.230s\\nEpoch: 7, Train loss: 2.059, Val loss: 2.212, Epoch time = 22.057s\\nEpoch: 8, Train loss: 1.897, Val loss: 2.119, Epoch time = 21.400s\\nEpoch: 9, Train loss: 1.757, Val loss: 2.052, Epoch time = 21.840s5/5/24, 12:58 PM Jorge_Gosalvez_255_HW9_trans_de_en_Pytorch.ipynb - Colab\\nhttps://colab.research.google.com/drive/1RIjNTsxbQRFg1ofAdsZ4UrwOC0oyKZHy#printMode=true 8/8', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW9_trans_de_en.pdf', 'page': 7}), Document(page_content=\"Homework 03: Cust om CNNsSJSU MSDS 255 DL, Spring 2024\\ue313\\nGit: https:/ /github.com/jr gosalv ez/data255_DL\\n#\\xa0Setup\\xa0experiment\\xa0and\\xa0import\\xa0CIFAR\\xa0data\\xa0from\\xa0Pyto rch\\n#\\xa0https://pytorch.org/vision/stable/datasets.html\\nimport\\xa0numpy\\xa0as\\xa0np\\nimport\\xa0time\\nimport\\xa0torch\\nimport\\xa0torch.nn\\xa0 as\\xa0nn\\nimport\\xa0torch.optim\\xa0 as\\xa0optim\\nimport\\xa0torch.nn.functional\\xa0 as\\xa0F\\nimport\\xa0torchvision\\nimport\\xa0torchvision.transforms\\xa0 as\\xa0transforms\\nfrom\\xa0torchvision.datasets\\xa0 import\\xa0CIFAR10\\nfrom\\xa0torch.utils.data\\xa0 import\\xa0DataLoader ,\\xa0Subset\\nfrom\\xa0torchvision\\xa0 import\\xa0datasets ,\\xa0models\\nimport\\xa0os\\nimport\\xa0matplotlib.pyplot\\xa0 as\\xa0plt\\nWithout a pr etrained model \\ue313\\n1. Cr eate cust om dataset of 3 categories fr om CIF AR10 with at least 100 images each \\ue313\\nMSD A 255, F all demo_03-classi\\x00cation examples r eferenced\\nUnsuppor ted Cell Type. Double-Click t o inspect/edit the content.\\ndef\\xa0show_data (img):\\n\\xa0\\xa0\\xa0\\xa0try:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0plt.imshow (img[0])\\n\\xa0\\xa0\\xa0\\xa0except\\xa0Exception\\xa0 as\\xa0e:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 print(e)\\n\\xa0\\xa0\\xa0\\xa0print(img[0].shape,\\xa0img[0].permute (1,2,0).shape)\\n\\xa0\\xa0\\xa0\\xa0plt.imshow (img[0].permute (1,2,0))\\n\\xa0\\xa0\\xa0\\xa0plt.title ('y\\xa0=\\xa0'+\\xa0str(img[1]))\\n\\xa0\\xa0\\xa0\\xa0plt.show ()\\n#\\xa0Define\\xa0transform\\xa0to\\xa0preprocess\\xa0the\\xa0data.\\xa0NOTE:\\xa0c an\\xa0define\\xa0transformations\\xa0for\\xa0train\\xa0and\\xa0test\\xa0(vali dation)\\xa0sets.\\xa0\\n#\\xa0Will\\xa0keep\\xa0simple\\xa0for\\xa0this\\xa0model\\ntransform\\xa0=\\xa0transforms.Compose ([\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.RandomResizedCrop (224),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.RandomHorizontalFlip (),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.ToTensor (),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.Normalize ([0.485,\\xa00.456,\\xa00.406],\\xa0[0.229,\\xa00.224,\\xa00.225])\\n\\xa0\\xa0\\xa0\\xa0])\\n#\\xa0Load\\xa0CIFAR10\\xa0dataset\\ntrainset\\xa0=\\xa0datasets.CIFAR10 (root='./data' ,\\xa0train=True,\\xa0download= True,\\xa0transform=transform )\\ntestset\\xa0\\xa0=\\xa0datasets.CIFAR10 (root='./data' ,\\xa0train=False,\\xa0download= True,\\xa0transform=transform )\\nFiles already downloaded and verified\\nFiles already downloaded and verified\\ntrainset\\nDataset CIFAR10\\n    Number of datapoints: 50000\\n    Root location: ./data\\n    Split: Train5/5/24, 1:00 PM Jorge-Gosalvez_255_HW3 (1).ipynb - Colab\\nhttps://colab.research.google.com/drive/1P3xSSReWugbWYgunQEEK6ktGeMY3734S#printMode=true 1/14\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW3_CNN.pdf', 'page': 0}), Document(page_content=\"    StandardTransform\\nTransform: Compose(\\n               RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear,  \\nantialias=True)\\n               RandomHorizontalFlip(p=0.5)\\n               ToTensor()\\n               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\\n           )\\ntestset\\nDataset CIFAR10\\n    Number of datapoints: 10000\\n    Root location: ./data\\n    Split: Test\\n    StandardTransform\\nTransform: Compose(\\n               RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear,  \\nantialias=True)\\n               RandomHorizontalFlip(p=0.5)\\n               ToTensor()\\n               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\\n           )\\nclass_names\\xa0=\\xa0trainset.classes\\nprint(class_names )\\n['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\\nnum_classes\\xa0=\\xa0 len(class_names )\\nprint(num_classes )\\n10\\n10 classes as par t of CIF AR10 dataset\\n#\\xa0select\\xa0classes\\xa0you\\xa0want\\xa0to\\xa0include\\xa0in\\xa0your\\xa0subse t\\n#\\xa0https://www.cs.toronto.edu/~kriz/cifar.html\\xa0\\n#\\xa010\\xa0classes\\xa0total,\\xa0picked\\xa0three;\\xa00:\\xa0airplane,\\xa02:\\xa0 bird,\\xa06:\\xa0frog\\nclasses\\xa0=\\xa0torch.tensor ([0,\\xa02,\\xa06])\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n#\\xa0get\\xa0indices\\xa0that\\xa0correspond\\xa0to\\xa0one\\xa0of\\xa0the\\xa0select ed\\xa0classes\\nindices\\xa0=\\xa0 (torch.tensor (trainset.targets )[...,\\xa0None]\\xa0==\\xa0classes ).any(-1).nonzero (as_tuple= True)[0]\\ntest_indices\\xa0=\\xa0 (torch.tensor (testset.targets )[...,\\xa0None]\\xa0==\\xa0classes ).any(-1).nonzero (as_tuple= True)[0]\\n#\\xa0subset\\xa0the\\xa0dataset\\xa0and\\xa0reduce\\xa0to\\xa01/4th\\xa0the\\xa0size\\xa0 and\\xa0randamize\\ndata\\xa0=\\xa0Subset (trainset ,\\xa0indices )\\n#\\xa01/7th\\xa0the\\xa0datapoints\\xa0of\\xa0each\\xa0of\\xa0the\\xa0three\\xa0classe s\\xa0from\\xa0the\\xa0trainset\\ntotal\\xa0=\\xa0 len(data)\\ntrain_data\\xa0=\\xa0torch.utils.data.random_split (data,\\xa0[total//7,\\xa0total-total// 7])[0]\\nt_data\\xa0=\\xa0torch.utils.data.random_split (data,\\xa0[total//7,\\xa0total-total// 7])[0]\\nprint(len(train_data ))\\nprint(len(t_data))\\n2142\\n2142\\nshow_data (train_data [8])5/5/24, 1:00 PM Jorge-Gosalvez_255_HW3 (1).ipynb - Colab\\nhttps://colab.research.google.com/drive/1P3xSSReWugbWYgunQEEK6ktGeMY3734S#printMode=true 2/14\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW3_CNN.pdf', 'page': 1}), Document(page_content='Clipping input data to the valid range for imshow with RGB data ([0..1] for floa\\nInvalid shape (3, 224, 224) for image data\\ntorch.Size([3, 224, 224]) torch.Size([224, 224, 3])\\n2. Split cust om data 80% tr aining and 20% v alidation set for tr aining and testing \\ue313\\n#\\xa0Split\\xa0the\\xa0dataset\\xa0into\\xa0training\\xa0and\\xa0validation\\xa0s ets\\xa0(80-20\\xa0split)\\ntrain_size\\xa0=\\xa0 int(0.8\\xa0*\\xa0len(train_data ))\\nval_size\\xa0\\xa0\\xa0=\\xa0 len(train_data )\\xa0-\\xa0train_size\\ntrain_data ,\\xa0valset\\xa0=\\xa0torch.utils.data.random_split (train_data ,\\xa0[train_size ,\\xa0val_size ])\\n#\\xa0Define\\xa0data\\xa0loaders\\ntrain_dataloader\\xa0=\\xa0DataLoader (train_data ,\\xa0batch_size= 64,\\xa0shuffle= True,\\xa0num_workers= 2)\\nval_dataloader\\xa0\\xa0\\xa0=\\xa0DataLoader (valset,\\xa0batch_size= 64,\\xa0shuffle= False,\\xa0num_workers= 2)\\ntest_dataloader\\xa0\\xa0=\\xa0DataLoader (t_data,\\xa0batch_size= 64,\\xa0shuffle= False,\\xa0num_workers= 2)\\n#\\xa0Check\\xa0the\\xa0size\\xa0of\\xa0each\\xa0dataset\\nprint(\"Training\\xa0set\\xa0size:\" ,\\xa0len(train_data ))\\nprint(\"Validation\\xa0set\\xa0size:\" ,\\xa0len(valset))\\nprint(\"Test\\xa0set\\xa0size:\" ,\\xa0len(t_data))\\nprint(f\"Train\\xa0+\\xa0Val\\xa0 {len(train_data )\\xa0+\\xa0len(valset)}\\xa0|\\xa0Test\\xa0Set\\xa0 {len(t_data)}\")\\nTraining set size: 1713\\nValidation set size: 429\\nTest set size: 2142\\nTrain + Val 2142 | Test Set 2142\\n3. Pr eprocess data \\ue313\\n#\\xa0train_dataloader\\xa0is\\xa0PyTorch\\xa0DataLoader\\ndata_iter\\xa0=\\xa0 iter(train_dataloader )\\n#\\xa0Fetch\\xa0the\\xa0first\\xa0batch\\xa0of\\xa0data\\nimages,\\xa0labels\\xa0=\\xa0 next(data_iter )\\n#\\xa0Plot\\xa0the\\xa0first\\xa012\\xa0images\\nplt.figure (figsize= (8,\\xa08))\\nfor\\xa0i\\xa0in\\xa0range(12):\\n\\xa0\\xa0\\xa0\\xa0image\\xa0=\\xa0images [i].permute (1,\\xa02,\\xa00).numpy()\\xa0\\xa0#\\xa0Convert\\xa0tensor\\xa0to\\xa0numpy\\xa0array\\n\\xa0\\xa0\\xa0\\xa0plt.subplot (3,\\xa04,\\xa0i\\xa0+\\xa01)\\n\\xa0\\xa0\\xa0\\xa0plt.imshow (image)\\n\\xa0\\xa0\\xa0\\xa0plt.title (f\"Label:\\xa0 {class_names [labels[i].item()]}\")\\n\\xa0\\xa0\\xa0\\xa0plt.axis (\"off\")\\nplt.show ()5/5/24, 1:00 PM Jorge-Gosalvez_255_HW3 (1).ipynb - Colab\\nhttps://colab.research.google.com/drive/1P3xSSReWugbWYgunQEEK6ktGeMY3734S#printMode=true 3/14', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW3_CNN.pdf', 'page': 2}), Document(page_content=\"Clipping input data to the valid range for imshow with RGB data ([0..1] for floa\\nClipping input data to the valid range for imshow with RGB data ([0..1] for floa\\nClipping input data to the valid range for imshow with RGB data ([0..1] for floa\\nClipping input data to the valid range for imshow with RGB data ([0..1] for floa\\nClipping input data to the valid range for imshow with RGB data ([0..1] for floa\\nClipping input data to the valid range for imshow with RGB data ([0..1] for floa\\nClipping input data to the valid range for imshow with RGB data ([0..1] for floa\\nClipping input data to the valid range for imshow with RGB data ([0..1] for floa\\nClipping input data to the valid range for imshow with RGB data ([0..1] for floa\\nClipping input data to the valid range for imshow with RGB data ([0..1] for floa\\nClipping input data to the valid range for imshow with RGB data ([0..1] for floa\\nClipping input data to the valid range for imshow with RGB data ([0..1] for floa\\nprint(f'Length\\xa0of\\xa0train\\xa0dataloader:\\xa0 {len(train_dataloader.dataset )}')\\nfor\\xa0image_batch ,\\xa0labels_batch\\xa0 in\\xa0train_dataloader :\\n\\xa0\\xa0print(image_batch.shape )\\n\\xa0\\xa0print(labels_batch.shape )\\n\\xa0\\xa0break\\nLength of train dataloader: 1713\\ntorch.Size([64, 3, 224, 224])\\ntorch.Size([64])\\ntorch.siz e (batch, channel, image siz e X, image siz e Y), wher e the image batch is tensor of shape ([64, 3, 224, 224]) with a batch of 64 images.\\nThe shape of these images is 244x244x3 based on center out tr ansformation. Images ar e blur y b/c the y are str etched, nativ e images ar e\\n32x32. 3 indicates the channel (e.g. RBG, not gr ay or other).\\nprint(train_dataloader.dataset [0][0].shape)\\nprint(train_dataloader.dataset [1][0].shape)\\nprint(images.shape )\\nprint(labels.shape )\\ntorch.Size([3, 224, 224])\\ntorch.Size([3, 224, 224])\\ntorch.Size([64, 3, 224, 224])\\ntorch.Size([64])\\n4. Cr eate CNN t o learn tr aining set \\ue3135/5/24, 1:00 PM Jorge-Gosalvez_255_HW3 (1).ipynb - Colab\\nhttps://colab.research.google.com/drive/1P3xSSReWugbWYgunQEEK6ktGeMY3734S#printMode=true 4/14\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW3_CNN.pdf', 'page': 3}), Document(page_content='class\\xa0CustomModel (nn.Module):\\n\\xa0\\xa0\\xa0\\xa0def\\xa0__init__ (self,\\xa0num_classes ):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0super (CustomModel ,\\xa0self).__init__ ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.rescale\\xa0=\\xa0nn.Sequential (\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0nn.Conv2d (3,\\xa03,\\xa0kernel_size= 1),\\xa0\\xa0#\\xa0Rescaling\\xa0layer,\\xa01x1\\xa0conv\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0nn.BatchNorm2d (3),\\xa0\\xa0#\\xa0Batch\\xa0normalization\\xa0to\\xa0maintain\\xa0scale\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0nn.ReLU (inplace= True)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.conv1\\xa0=\\xa0nn.Conv2d (3,\\xa016,\\xa0kernel_size= 3)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.pool1\\xa0=\\xa0nn.MaxPool2d (kernel_size= 2,\\xa0stride= 2)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.conv2\\xa0=\\xa0nn.Conv2d (16,\\xa032,\\xa0kernel_size= 3)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.pool2\\xa0=\\xa0nn.MaxPool2d (kernel_size= 2,\\xa0stride= 2)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.conv3\\xa0=\\xa0nn.Conv2d (32,\\xa064,\\xa0kernel_size= 3)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.pool3\\xa0=\\xa0nn.MaxPool2d (kernel_size= 2,\\xa0stride= 2)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.flatten\\xa0=\\xa0nn.Flatten ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.fc1\\xa0=\\xa0nn.Linear (224\\xa0*\\xa0224\\xa0*\\xa032,\\xa0128)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.fc2\\xa0=\\xa0nn.Linear (128,\\xa0num_classes )\\n\\xa0\\xa0\\xa0\\xa0def\\xa0forward(self,\\xa0x):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0 self.rescale (x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0 self.conv1(x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0nn.ReLU (inplace= True)(x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0 self.pool1(x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0 self.conv2(x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0nn.ReLU (inplace= True)(x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0 self.pool2(x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0 self.conv3(x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0nn.ReLU (inplace= True)(x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0 self.pool3(x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0 self.flatten (x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0 self.fc1(x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0nn.ReLU (inplace= True)(x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0 self.fc2(x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return\\xa0x\\n#\\xa0Instantiate\\xa0the\\xa0model\\n#\\xa0num_classes\\xa0=\\xa010\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0#\\xa010\\xa0categories\\xa0 in\\xa0CIFAR\\xa0dataset\\nmodel\\xa0=\\xa0CustomModel (num_classes )\\n#\\xa0Print\\xa0the\\xa0model\\xa0architecture\\nprint(model)\\nCustomModel(\\n  (rescale): Sequential(\\n    (0): Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1))\\n    (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\\n    (2): ReLU(inplace=True)\\n  )\\n  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\\n  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\\n  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\\n  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\\n  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\\n  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\\n  (flatten): Flatten(start_dim=1, end_dim=-1)\\n  (fc1): Linear(in_features=1605632, out_features=128, bias=True)\\n  (fc2): Linear(in_features=128, out_features=10, bias=True)\\n)5/5/24, 1:00 PM Jorge-Gosalvez_255_HW3 (1).ipynb - Colab\\nhttps://colab.research.google.com/drive/1P3xSSReWugbWYgunQEEK6ktGeMY3734S#printMode=true 5/14', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW3_CNN.pdf', 'page': 4}), Document(page_content='class\\xa0CNNModel (nn.Module):\\n\\xa0\\xa0\\xa0\\xa0def\\xa0__init__ (self,\\xa0num_classes ,\\xa0img_height ,\\xa0img_width ):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0super (CNNModel ,\\xa0self).__init__ ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.rescaling\\xa0=\\xa0nn.Sequential (\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0nn.Conv2d (3,\\xa03,\\xa0kernel_size= 1),\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0nn.BatchNorm2d (3),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0nn.ReLU (inplace= True)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.conv1\\xa0=\\xa0nn.Conv2d (3,\\xa016,\\xa0kernel_size= 3,\\xa0padding= 1)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.pool\\xa0=\\xa0nn.MaxPool2d (2,\\xa02)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.conv2\\xa0=\\xa0nn.Conv2d (16,\\xa032,\\xa0kernel_size= 3,\\xa0padding= 1)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.conv3\\xa0=\\xa0nn.Conv2d (32,\\xa064,\\xa0kernel_size= 3,\\xa0padding= 1)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.flatten\\xa0=\\xa0nn.Flatten ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.fc1\\xa0=\\xa0nn.Linear (64\\xa0*\\xa0(img_height\\xa0//\\xa0 8)\\xa0*\\xa0(img_width\\xa0//\\xa0 8),\\xa0128)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.fc2\\xa0=\\xa0nn.Linear (128,\\xa0num_classes )\\n\\xa0\\xa0\\xa0\\xa0def\\xa0forward(self,\\xa0x):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0 self.rescaling (x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0 self.pool(nn.functional.relu (self.conv1(x)))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0 self.pool(nn.functional.relu (self.conv2(x)))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0 self.pool(nn.functional.relu (self.conv3(x)))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0 self.flatten (x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0nn.functional.relu (self.fc1(x))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0 self.fc2(x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return\\xa0x\\n#\\xa0Define\\xa0input\\xa0dimensions\\nimg_height ,\\xa0img_width\\xa0=\\xa0 224,\\xa0224\\n#\\xa0Define\\xa0the\\xa0number\\xa0of\\xa0classes\\n#\\xa0num_classes\\xa0=\\xa010\\n#\\xa0Instantiate\\xa0the\\xa0model\\xa0woTransfer\\nmodel\\xa0=\\xa0CNNModel (num_classes ,\\xa0img_height ,\\xa0img_width )\\n#\\xa0optimizer\\noptimizer\\xa0=\\xa0optim.Adam (model.parameters ())\\n#\\xa0loss\\xa0function\\ncriterion\\xa0=\\xa0nn.CrossEntropyLoss ()\\n#\\xa0Define\\xa0the\\xa0metrics\\xa0(accuracy)\\ndef\\xa0accuracy (outputs,\\xa0labels):\\n\\xa0\\xa0\\xa0\\xa0_,\\xa0preds\\xa0=\\xa0torch. max(outputs,\\xa0dim=1)\\n\\xa0\\xa0\\xa0\\xa0return\\xa0torch.tensor (torch.sum(preds\\xa0==\\xa0labels ).item()\\xa0/\\xa0len(preds))\\n#\\xa0Define\\xa0the\\xa0training\\xa0loop\\ndef\\xa0train_model (model,\\xa0train_dl ,\\xa0criterion ,\\xa0optimizer ,\\xa0num_epochs =10):\\n\\xa0\\xa0\\xa0\\xa0for\\xa0epoch\\xa0in\\xa0range(num_epochs ):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0model.train ()\\xa0\\xa0#\\xa0Set\\xa0the\\xa0model\\xa0to\\xa0train\\xa0mode\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0train_loss\\xa0=\\xa0 0.0\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0train_acc\\xa0\\xa0=\\xa0 0.0\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for\\xa0images,\\xa0labels\\xa0 in\\xa0train_dl :\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0optimizer.zero_grad ()\\xa0\\xa0#\\xa0Zero\\xa0the\\xa0parameter\\xa0gradients\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0outputs\\xa0=\\xa0model (images)\\xa0\\xa0#\\xa0Forward\\xa0pass\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0loss\\xa0=\\xa0criterion (outputs,\\xa0labels)\\xa0\\xa0#\\xa0Calculate\\xa0the\\xa0loss\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0loss.backward ()\\xa0\\xa0#\\xa0Backward\\xa0pass\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0optimizer.step ()\\xa0\\xa0#\\xa0Optimize\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Compute\\xa0training\\xa0accuracy\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0acc\\xa0=\\xa0accuracy (outputs,\\xa0labels)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0train_acc\\xa0+=\\xa0acc.item ()\\xa0*\\xa0images.size (0)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Track\\xa0training\\xa0loss\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0train_loss\\xa0+=\\xa0loss.item ()\\xa0*\\xa0images.size (0)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Print\\xa0training\\xa0statistics\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0epoch_loss\\xa0=\\xa0train_loss\\xa0/\\xa0 len(train_dl.dataset )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0epoch_acc\\xa0=\\xa0train_acc\\xa0/\\xa0 len(train_dl.dataset )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 print(f\"Epoch\\xa0{epoch\\xa0+\\xa0 1}/{num_epochs },\\xa0Loss:\\xa0 {epoch_loss :.4f},\\xa0Accuracy:\\xa0 {epoch_acc :.4f}\")5/5/24, 1:00 PM Jorge-Gosalvez_255_HW3 (1).ipynb - Colab\\nhttps://colab.research.google.com/drive/1P3xSSReWugbWYgunQEEK6ktGeMY3734S#printMode=true 6/14', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW3_CNN.pdf', 'page': 5}), Document(page_content='%%time\\n#\\xa0Train\\xa0the\\xa0model\\nnum_epochs\\xa0=\\xa0 3\\xa0\\xa0#\\xa0Choose\\xa0the\\xa0number\\xa0of\\xa0epochs\\ntrain_model (model,\\xa0train_dataloader ,\\xa0criterion ,\\xa0optimizer ,\\xa0num_epochs )\\nEpoch 1/3, Loss: 1.3408, Accuracy: 0.3497\\nEpoch 2/3, Loss: 1.0268, Accuracy: 0.4781\\nEpoch 3/3, Loss: 0.9585, Accuracy: 0.5406\\nCPU times: user 5min 24s, sys: 1min 29s, total: 6min 53s\\nWall time: 4min 15s\\nRetrain with data augmentation\\ue313\\n%%time\\n#\\xa0Define\\xa0the\\xa0training\\xa0loop\\ndef\\xa0train_model (model,\\xa0train_dl ,\\xa0criterion ,\\xa0optimizer ,\\xa0num_epochs =10):\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0for\\xa0epoch\\xa0in\\xa0range(num_epochs ):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0model.train ()\\xa0\\xa0#\\xa0Set\\xa0the\\xa0model\\xa0to\\xa0train\\xa0mode\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0train_loss\\xa0=\\xa0 0.0\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0train_acc\\xa0=\\xa0 0.0\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for\\xa0images,\\xa0labels\\xa0 in\\xa0train_dl :\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0data_augmented_inputs\\xa0=\\xa0torch.stack ([transforms.RandomHorizontalFlip ()(img)\\xa0for\\xa0img\\xa0in\\xa0images])\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0data_augmented_inputs\\xa0=\\xa0torch.stack ([transforms.RandomRotation (degrees= 10)(img)\\xa0for\\xa0img\\xa0in\\xa0data_augmented_inputs ])\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0optimizer.zero_grad ()\\xa0\\xa0#\\xa0Zero\\xa0the\\xa0parameter\\xa0gradients\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0outputs\\xa0=\\xa0model (images)\\xa0\\xa0#\\xa0Forward\\xa0pass\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0loss\\xa0=\\xa0criterion (outputs,\\xa0labels)\\xa0\\xa0#\\xa0Calculate\\xa0the\\xa0loss\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0loss.backward ()\\xa0\\xa0#\\xa0Backward\\xa0pass\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0optimizer.step ()\\xa0\\xa0#\\xa0Optimize\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Compute\\xa0training\\xa0accuracy\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0acc\\xa0=\\xa0accuracy (outputs,\\xa0labels)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0train_acc\\xa0+=\\xa0acc.item ()\\xa0*\\xa0images.size (0)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Track\\xa0training\\xa0loss\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0train_loss\\xa0+=\\xa0loss.item ()\\xa0*\\xa0images.size (0)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Print\\xa0training\\xa0statistics\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0epoch_loss\\xa0=\\xa0train_loss\\xa0/\\xa0 len(train_dl.dataset )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0epoch_acc\\xa0\\xa0=\\xa0train_acc\\xa0/\\xa0 len(train_dl.dataset )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 print(f\"Epoch\\xa0{epoch\\xa0+\\xa0 1}/{num_epochs },\\xa0Loss:\\xa0 {epoch_loss :.4f},\\xa0Accuracy:\\xa0 {epoch_acc :.4f}\")\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0return\\xa0epoch_loss ,\\xa0epoch_acc\\xa0\\xa0\\xa0\\xa0\\n#\\xa0Train\\xa0the\\xa0model\\nnum_epochs\\xa0=\\xa0 3\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Choose\\xa0the\\xa0number\\xa0of\\xa0epochs\\nm1_epoch_loss ,\\xa0m1_epoch_acc\\xa0=\\xa0train_model (model,\\xa0train_dataloader ,\\xa0criterion ,\\xa0optimizer ,\\xa0num_epochs )\\nprint(f\\'Model\\xa0Final:\\xa0loss\\xa0 {m1_epoch_loss :.4f}\\xa0|\\xa0accuracy\\xa0 {m1_epoch_acc :.4f}\\')\\nEpoch 1/3, Loss: 0.8994, Accuracy: 0.5867\\nEpoch 2/3, Loss: 0.8749, Accuracy: 0.6013\\nEpoch 3/3, Loss: 0.8522, Accuracy: 0.6025\\nModel Final: loss 0.8522 | accuracy 0.6025\\nCPU times: user 5min 44s, sys: 1min 59s, total: 7min 43s\\nWall time: 4min 39s\\n5. Mak e predictions on test dataset and compar e accur acy t o expected categories \\ue313\\nfrom\\xa0PIL\\xa0import\\xa0Image\\nimport\\xa0requests\\nfrom\\xa0io\\xa0import\\xa0BytesIO\\nfrom\\xa0torch\\xa0import\\xa0argmax5/5/24, 1:00 PM Jorge-Gosalvez_255_HW3 (1).ipynb - Colab\\nhttps://colab.research.google.com/drive/1P3xSSReWugbWYgunQEEK6ktGeMY3734S#printMode=true 7/14', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW3_CNN.pdf', 'page': 6}), Document(page_content='img_height ,\\xa0img_width\\xa0=\\xa0 224,\\xa0224\\n#\\xa0Define\\xa0the\\xa0location\\xa0of\\xa0the\\xa0image\\n#\\xa0https://cdn.britannica.com/q:60/69/155469-131-14 083F59/airplane-flight.jpg\\n#\\xa0https://external-content.duckduckgo.com/iu/?u=ht tp%3A%2F%2Fwallpaperping.com%2Fwp-content%2Fupload s%2F2018%2F12%2FBluebirdTote\\n#\\xa0another\\xa0frog:\\xa0https://www.welcomewildlife.com/wp -content/uploads/2015/01/Frog-compare.jpg\\nfrog_url\\xa0=\\xa0 \"https://zoo.bca.ac.uk/wp-content/uploads/sites/2/ 2020/02/BCA-Small-Size-101.jpg\"\\n#\\xa0Download\\xa0the\\xa0image\\xa0and\\xa0convert\\xa0it\\xa0to\\xa0a\\xa0PyTorch\\xa0t ensor\\nresponse\\xa0=\\xa0requests.get (frog_url )\\nimg\\xa0=\\xa0Image. open(BytesIO(response.content )).convert (\\'RGB\\')\\nimg\\xa0=\\xa0img.resize ((img_width ,\\xa0img_height ))\\xa0\\xa0#\\xa0Resize\\xa0the\\xa0image\\xa0to\\xa0match\\xa0the\\xa0model\\'s\\xa0input\\xa0size\\nimg_tensor\\xa0=\\xa0transforms.ToTensor ()(img)\\nimg_tensor\\xa0=\\xa0img_tensor.unsqueeze (0)\\xa0\\xa0#\\xa0Add\\xa0batch\\xa0dimension\\n#\\xa0Move\\xa0the\\xa0tensor\\xa0to\\xa0the\\xa0appropriate\\xa0device\\xa0(GPU\\xa0i f\\xa0available)\\ndevice\\xa0=\\xa0torch.device (\"cuda\"\\xa0if\\xa0torch.cuda.is_available ()\\xa0else\\xa0\"cpu\")\\nimg_tensor\\xa0=\\xa0img_tensor.to (device)\\n#\\xa0Make\\xa0predictions\\nmodel.eval()\\xa0\\xa0#\\xa0Set\\xa0the\\xa0model\\xa0to\\xa0evaluation\\xa0mode\\nwith\\xa0torch.no_grad ():\\n\\xa0\\xa0\\xa0\\xa0outputs\\xa0=\\xa0model (img_tensor )\\n\\xa0\\xa0\\xa0\\xa0probabilities\\xa0=\\xa0torch.softmax (outputs,\\xa0dim=1)\\n#\\xa0Convert\\xa0tensor\\xa0to\\xa0numpy\\xa0array\\npredicted_class\\xa0=\\xa0class_names [argmax(probabilities )]\\n#\\xa0Print\\xa0the\\xa0predicted\\xa0class\\xa0and\\xa0confidence\\npredicted_class\\xa0=\\xa0class_names [torch.argmax (probabilities )]\\nconfidence\\xa0=\\xa0torch. max(probabilities ).item()\\nprint(f\"This\\xa0image\\xa0most\\xa0likely\\xa0belongs\\xa0to\\xa0 {predicted_class }\\xa0with\\xa0a\\xa0 {confidence* 100:.2f}%\\xa0confidence.\" )\\nThis image most likely belongs to frog with a 57.76% confidence.\\nTest pr ediction on single image \\ue313\\n#\\xa0Load\\xa0and\\xa0preprocess\\xa0the\\xa0unseen\\xa0image\\nimage_path\\xa0=\\xa0 \\'frog.jpg\\' \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Replace\\xa0with\\xa0the\\xa0path\\xa0to\\xa0your\\xa0image\\nimage\\xa0=\\xa0Image. open(image_path )\\npreprocess\\xa0=\\xa0transforms.Compose ([\\n\\xa0\\xa0\\xa0\\xa0transforms.Resize (256),\\n\\xa0\\xa0\\xa0\\xa0transforms.CenterCrop (224),\\n\\xa0\\xa0\\xa0\\xa0transforms.ToTensor (),\\n\\xa0\\xa0\\xa0\\xa0transforms.Normalize ([0.485,\\xa00.456,\\xa00.406],\\xa0[0.229,\\xa00.224,\\xa00.225])\\n])\\ninput_tensor\\xa0=\\xa0preprocess (image)\\ninput_batch\\xa0=\\xa0input_tensor.unsqueeze (0)\\xa0\\xa0#\\xa0Add\\xa0a\\xa0batch\\xa0dimension\\n#\\xa0Perform\\xa0inference\\nwith\\xa0torch.no_grad ():\\n\\xa0\\xa0\\xa0\\xa0output\\xa0=\\xa0model (input_batch )\\n#\\xa0Get\\xa0the\\xa0predicted\\xa0class\\n_,\\xa0predicted_class\\xa0=\\xa0output. max(1)\\n#\\xa0Map\\xa0the\\xa0predicted\\xa0class\\xa0to\\xa0the\\xa0class\\xa0name\\npredicted_class_name\\xa0=\\xa0class_names [predicted_class.item ()]\\nprobabilities\\xa0=\\xa0torch.softmax (outputs,\\xa0dim=1)\\nconfidence\\xa0=\\xa0torch. max(probabilities ).item()\\nprint(f\"This\\xa0image\\xa0most\\xa0likely\\xa0belongs\\xa0to\\xa0 {predicted_class_name }\\xa0with\\xa0a\\xa0 {confidence* 100:.2f}%\\xa0confidence.\" )\\nThis image most likely belongs to frog with a 57.76% confidence.\\n#\\xa0Display\\xa0the\\xa0image\\xa0with\\xa0the\\xa0predicted\\xa0class\\xa0name\\nimage\\xa0=\\xa0np.array (image)\\nplt.imshow (image)\\nplt.axis (\\'off\\')\\nplt.text (10,\\xa010,\\xa0f\\'Predicted:\\xa0 {predicted_class_name }\\',\\xa0fontsize= 12,\\xa0color=\\'white\\',\\xa0backgroundcolor= \\'red\\')\\nplt.show ()5/5/24, 1:00 PM Jorge-Gosalvez_255_HW3 (1).ipynb - Colab\\nhttps://colab.research.google.com/drive/1P3xSSReWugbWYgunQEEK6ktGeMY3734S#printMode=true 8/14', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW3_CNN.pdf', 'page': 7}), Document(page_content='With input batch cr eated, cr eate function t o apply t o multiple images \\ue313\\ndef\\xa0check(image,\\xa0input_batch ):\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Load\\xa0and\\xa0preprocess\\xa0the\\xa0unseen\\xa0image\\n\\xa0\\xa0\\xa0\\xa0image_path\\xa0=\\xa0image\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Replace\\xa0with\\xa0the\\xa0path\\xa0to\\xa0your\\xa0image\\n\\xa0\\xa0\\xa0\\xa0image\\xa0=\\xa0Image. open(image_path )\\n\\xa0\\xa0\\xa0\\xa0preprocess\\xa0=\\xa0transforms.Compose ([\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.Resize (256),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.CenterCrop (224),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.ToTensor (),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.Normalize ([0.485,\\xa00.456,\\xa00.406],\\xa0[0.229,\\xa00.224,\\xa00.225])\\n\\xa0\\xa0\\xa0\\xa0])\\n\\xa0\\xa0\\xa0\\xa0input_tensor\\xa0=\\xa0preprocess (image)\\n\\xa0\\xa0\\xa0\\xa0input_batch\\xa0=\\xa0input_tensor.unsqueeze (0)\\xa0\\xa0#\\xa0Add\\xa0a\\xa0batch\\xa0dimension\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Perform\\xa0inference\\n\\xa0\\xa0\\xa0\\xa0with\\xa0torch.no_grad ():\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0output\\xa0=\\xa0model (input_batch )\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Get\\xa0the\\xa0predicted\\xa0class\\n\\xa0\\xa0\\xa0\\xa0_,\\xa0predicted_class\\xa0=\\xa0output. max(1)\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Map\\xa0the\\xa0predicted\\xa0class\\xa0to\\xa0the\\xa0class\\xa0name\\n\\xa0\\xa0\\xa0\\xa0predicted_class_name\\xa0=\\xa0class_names [predicted_class.item ()]\\n\\xa0\\xa0\\xa0\\xa0probabilities\\xa0=\\xa0torch.softmax (outputs,\\xa0dim=1)\\n\\xa0\\xa0\\xa0\\xa0confidence\\xa0=\\xa0torch. max(probabilities ).item()\\n\\xa0\\xa0\\xa0\\xa0print(f\"This\\xa0image\\xa0most\\xa0likely\\xa0belongs\\xa0to\\xa0 {predicted_class_name }\\xa0with\\xa0a\\xa0 {confidence* 100:.2f}%\\xa0confidence.\" )\\n\\xa0\\xa0\\xa0\\xa0print()\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Display\\xa0the\\xa0image\\xa0with\\xa0the\\xa0predicted\\xa0class\\xa0name\\n\\xa0\\xa0\\xa0\\xa0image\\xa0=\\xa0np.array (image)\\n\\xa0\\xa0\\xa0\\xa0plt.imshow (image)\\n\\xa0\\xa0\\xa0\\xa0plt.axis (\\'off\\')\\n\\xa0\\xa0\\xa0\\xa0plt.text (10,\\xa010,\\xa0f\\'Predicted:\\xa0 {predicted_class_name }\\',\\xa0fontsize= 12,\\xa0color=\\'white\\',\\xa0backgroundcolor= \\'red\\')\\n\\xa0\\xa0\\xa0\\xa0plt.show ()\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0return\\xa0input_batch\\nnew_images\\xa0=\\xa0 [\\'frog.jpg\\' ,\\xa0\\'airplane.jpg\\' ,\\xa0\\'bird.jpg\\' ,\\xa0\\'frog2.jpg\\' ]\\nfor\\xa0i\\xa0in\\xa0new_images :\\n\\xa0\\xa0\\xa0\\xa0check (i,\\xa0input_batch )5/5/24, 1:00 PM Jorge-Gosalvez_255_HW3 (1).ipynb - Colab\\nhttps://colab.research.google.com/drive/1P3xSSReWugbWYgunQEEK6ktGeMY3734S#printMode=true 9/14', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW3_CNN.pdf', 'page': 8}), Document(page_content='This image most likely belongs to frog with a 57.76% confidence.\\nThis image most likely belongs to airplane with a 57.76% confidence.\\nThis image most likely belongs to bird with a 57.76% confidence.\\nThis image most likely belongs to airplane with a 57.76% confidence.\\n\\x005/5/24, 1:00 PM Jorge-Gosalvez_255_HW3 (1).ipynb - Colab\\nhttps://colab.research.google.com/drive/1P3xSSReWugbWYgunQEEK6ktGeMY3734S#printMode=true 10/14', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW3_CNN.pdf', 'page': 9}), Document(page_content=\"Event with augmenting data t o incr ease accur acy and con\\x00dence.\\nImages must be same siz e (e.g. uniform or pr ocessed t o be uniform. F or example, tr anspar ent images can cause challenges)\\nNow with a pr etrained model...\\n6. Use GoogleNet (InceptionNet) and add a LinearLa yer \\ue313\\nUnsuppor ted Cell Type. Double-Click t o inspect/edit the content.\\nExperiment b y creating a local cop y of data \\ue313\\n#\\xa0Define\\xa0the\\xa0data\\xa0directory\\xa0\\xa0#\\xa0train_dataloader\\ndata_dir\\xa0=\\xa0 './data'\\n#\\xa0Create\\xa0a\\xa0directory\\xa0to\\xa0save\\xa0the\\xa0images\\xa0if\\xa0it\\xa0does n't\\xa0exist\\nsave_dir\\xa0=\\xa0os.path.join (data_dir ,\\xa0'train')\\nos.makedirs (save_dir ,\\xa0exist_ok= True)\\nfor\\xa0i,\\xa0(image,\\xa0label)\\xa0in\\xa0enumerate (train_data ):\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Remove\\xa0the\\xa0batch\\xa0dimension\\xa0if\\xa0it\\xa0exists\\n\\xa0\\xa0\\xa0\\xa0image\\xa0=\\xa0image.squeeze (0)\\xa0if\\xa0len(image.size ())\\xa0==\\xa04\\xa0else\\xa0image\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Convert\\xa0the\\xa0tensor\\xa0image\\xa0to\\xa0PIL\\xa0image\\n\\xa0\\xa0\\xa0\\xa0pil_image\\xa0=\\xa0transforms.ToPILImage ()(image)\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Define\\xa0the\\xa0subdirectory\\xa0based\\xa0on\\xa0the\\xa0label\\n\\xa0\\xa0\\xa0\\xa0label_dir\\xa0=\\xa0os.path.join (save_dir ,\\xa0str(label))\\n\\xa0\\xa0\\xa0\\xa0os.makedirs (label_dir ,\\xa0exist_ok= True)\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Define\\xa0the\\xa0filename\\xa0for\\xa0saving\\n\\xa0\\xa0\\xa0\\xa0filename\\xa0=\\xa0os.path.join (label_dir ,\\xa0f'image_{i}.jpg')\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Save\\xa0the\\xa0PIL\\xa0image\\xa0as\\xa0a\\xa0JPEG\\xa0file\\n\\xa0\\xa0\\xa0\\xa0pil_image.save (filename )5/5/24, 1:00 PM Jorge-Gosalvez_255_HW3 (1).ipynb - Colab\\nhttps://colab.research.google.com/drive/1P3xSSReWugbWYgunQEEK6ktGeMY3734S#printMode=true 11/14\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW3_CNN.pdf', 'page': 10}), Document(page_content=\"#\\xa0Create\\xa0a\\xa0val\\xa0directory\\xa0with\\xa0sub\\xa0folders\\xa0for\\xa03\\xa0cl asses\\xa0to\\xa0save\\xa0the\\xa0images\\xa0if\\xa0it\\xa0doesn't\\xa0exist\\nsave_dir\\xa0=\\xa0os.path.join (data_dir ,\\xa0'val')\\nos.makedirs (save_dir ,\\xa0exist_ok= True)\\nfor\\xa0i,\\xa0(image,\\xa0label)\\xa0in\\xa0enumerate (valset):\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Remove\\xa0the\\xa0batch\\xa0dimension\\xa0if\\xa0it\\xa0exists\\n\\xa0\\xa0\\xa0\\xa0image\\xa0=\\xa0image.squeeze (0)\\xa0if\\xa0len(image.size ())\\xa0==\\xa04\\xa0else\\xa0image\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0#for\\xa0i\\xa0in\\xa0range(3):\\n\\xa0\\xa0\\xa0\\xa0#\\xa0\\xa0\\xa0\\xa0print(label)\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Convert\\xa0the\\xa0tensor\\xa0image\\xa0to\\xa0PIL\\xa0image\\n\\xa0\\xa0\\xa0\\xa0pil_image\\xa0=\\xa0transforms.ToPILImage ()(image)\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Define\\xa0the\\xa0subdirectory\\xa0based\\xa0on\\xa0the\\xa0label\\n\\xa0\\xa0\\xa0\\xa0label_dir\\xa0=\\xa0os.path.join (save_dir ,\\xa0str(label))\\n\\xa0\\xa0\\xa0\\xa0os.makedirs (label_dir ,\\xa0exist_ok= True)\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Define\\xa0the\\xa0filename\\xa0for\\xa0saving\\n\\xa0\\xa0\\xa0\\xa0filename\\xa0=\\xa0os.path.join (label_dir ,\\xa0f'image_{i}.jpg')\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Save\\xa0the\\xa0PIL\\xa0image\\xa0as\\xa0a\\xa0JPEG\\xa0file\\n\\xa0\\xa0\\xa0\\xa0pil_image.save (filename )\\n#\\xa0Define\\xa0data\\xa0transformations\\xa0for\\xa0data\\xa0augmentatio n\\xa0and\\xa0normalization\\ndata_transforms\\xa0=\\xa0 {\\n\\xa0\\xa0\\xa0\\xa0'train':\\xa0transforms.Compose ([\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.RandomResizedCrop (299),\\xa0\\xa0\\xa0\\xa0#\\xa0resize\\xa0for\\xa0inception\\xa0v3\\xa0<<<<\\xa0required!\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.RandomHorizontalFlip (),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.ToTensor (),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.Normalize ([0.485,\\xa00.456,\\xa00.406],\\xa0[0.229,\\xa00.224,\\xa00.225])\\n\\xa0\\xa0\\xa0\\xa0]),\\n\\xa0\\xa0\\xa0\\xa0'val':\\xa0transforms.Compose ([\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.Resize (299),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.CenterCrop (299),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.ToTensor (),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.Normalize ([0.485,\\xa00.456,\\xa00.406],\\xa0[0.229,\\xa00.224,\\xa00.225])\\n\\xa0\\xa0\\xa0\\xa0]),\\n}\\n#\\xa0Create\\xa0data\\xa0loaders\\nimage_datasets\\xa0=\\xa0 {x:\\xa0datasets.ImageFolder (os.path.join (data_dir ,\\xa0x),\\xa0data_transforms [x])\\xa0for\\xa0x\\xa0in\\xa0['train',\\xa0'val']}\\nimage_datasets\\n{'train': Dataset ImageFolder\\n     Number of datapoints: 4968\\n     Root location: ./data/train\\n     StandardTransform\\n Transform: Compose(\\n                RandomResizedCrop(size=(299, 299), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear,  \\nantialias=True)\\n                RandomHorizontalFlip(p=0.5)\\n                ToTensor()\\n                Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\\n            ),\\n 'val': Dataset ImageFolder\\n     Number of datapoints: 1232\\n     Root location: ./data/val\\n     StandardTransform\\n Transform: Compose(\\n                Resize(size=299, interpolation=bilinear, max_size=None, antialias=True)\\n                CenterCrop(size=(299, 299))\\n                ToTensor()\\n                Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\\n            )}\\ndataloaders\\xa0\\xa0\\xa0=\\xa0 {x:\\xa0DataLoader (image_datasets [x],\\xa0batch_size= 4,\\xa0shuffle= True,\\xa0num_workers= 4)\\xa0for\\xa0x\\xa0in\\xa0['train',\\xa0'val']}\\ndataset_sizes\\xa0=\\xa0 {x:\\xa0len(image_datasets [x])\\xa0for\\xa0x\\xa0in\\xa0['train',\\xa0'val']}\\nprint(dataset_sizes )\\nclass_names\\xa0=\\xa0image_datasets ['train'].classes\\nclass_names\\n{'train': 4968, 'val': 1232}\\n['0', '2', '6']5/5/24, 1:00 PM Jorge-Gosalvez_255_HW3 (1).ipynb - Colab\\nhttps://colab.research.google.com/drive/1P3xSSReWugbWYgunQEEK6ktGeMY3734S#printMode=true 12/14\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW3_CNN.pdf', 'page': 11}), Document(page_content='#\\xa0Load\\xa0the\\xa0pre-trained\\xa0ResNet-18\\xa0model\\xa0-\\xa0works\\xa0b/c \\xa0kernel\\xa0and\\xa0input\\xa0sizes\\xa0fit\\xa03x3\\n#\\xa0model\\xa0=\\xa0models.resnet18(pretrained=True)\\n#\\xa0Load\\xa0the\\xa0pre-trained\\xa0inception_v3\\xa0model\\xa0-\\xa0does\\xa0n ot\\xa0work\\xa0b/c\\xa0kernel\\xa0is\\xa05x5\\xa0versus\\xa03x3\\xa0inputs,\\xa0need\\xa0 to\\xa0set\\xa0final\\xa0\\nmodel\\xa0=\\xa0models.inception_v3 (pretrained= True)\\xa0\\xa0\\xa0\\nmodel.aux_logits\\xa0=\\xa0 False\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0inception\\xa0v3\\xa0\\n#\\xa0print(model)\\n/opt/anaconda3/envs/python311/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter \\'pre\\n  warnings.warn(\\n/opt/anaconda3/envs/python311/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other th\\n  warnings.warn(msg)\\n#\\xa0Freeze\\xa0all\\xa0layers\\xa0except\\xa0the\\xa0final\\xa0classificatio n\\xa0layer\\nfor\\xa0name,\\xa0param\\xa0in\\xa0model.named_parameters ():\\n\\xa0\\xa0\\xa0\\xa0if\\xa0\"fc\"\\xa0in\\xa0name:\\xa0\\xa0#\\xa0Unfreeze\\xa0the\\xa0final\\xa0classification\\xa0layer\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0param.requires_grad\\xa0=\\xa0 True\\n\\xa0\\xa0\\xa0\\xa0else:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0param.requires_grad\\xa0=\\xa0 False\\n#\\xa0Define\\xa0the\\xa0loss\\xa0function\\xa0and\\xa0optimizer\\ncriterion\\xa0=\\xa0nn.CrossEntropyLoss ()\\noptimizer\\xa0=\\xa0optim.SGD (model.parameters (),\\xa0lr=0.001,\\xa0momentum= 0.9)\\xa0\\xa0#\\xa0Use\\xa0all\\xa0parameters\\n#\\xa0Move\\xa0the\\xa0model\\xa0to\\xa0the\\xa0GPU\\xa0if\\xa0available\\ndevice\\xa0=\\xa0torch.device (\"cuda:0\" \\xa0if\\xa0torch.cuda.is_available ()\\xa0else\\xa0\"cpu\")\\nmodel\\xa0=\\xa0model.to (device)\\n%%time\\n#\\xa0Training\\xa0loop\\nnew_acc\\xa0\\xa0\\xa0\\xa0=\\xa0 0.0\\nnew_acc\\xa0\\xa0\\xa0\\xa0=\\xa0 0.0\\nnum_epochs\\xa0=\\xa0 3\\nfor\\xa0epoch\\xa0in\\xa0range(num_epochs ):\\n\\xa0\\xa0\\xa0\\xa0for\\xa0phase\\xa0in\\xa0[\\'train\\',\\xa0\\'val\\']:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\xa0phase\\xa0==\\xa0 \\'train\\':\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0model.train ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 else:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0model. eval()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0running_loss\\xa0=\\xa0 0.0\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0running_corrects\\xa0=\\xa0 0\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for\\xa0inputs,\\xa0labels\\xa0 in\\xa0dataloaders [phase]:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0inputs\\xa0=\\xa0inputs.to (device)\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0inception\\xa0v3\\xa0requires\\xa0images\\xa0of\\xa0size\\xa0299x299,\\xa0wh ich\\xa0is\\xa0done\\xa0in\\xa0the\\xa0encoder/d\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0labels\\xa0=\\xa0labels.to (device)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #print(inputs.size())\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0optimizer.zero_grad ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 with\\xa0torch.set_grad_enabled (phase\\xa0==\\xa0 \\'train\\'):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0outputs\\xa0=\\xa0model (inputs)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0_ ,\\xa0preds\\xa0=\\xa0torch. max(outputs,\\xa01)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0loss\\xa0=\\xa0criterion (outputs,\\xa0labels)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\xa0phase\\xa0==\\xa0 \\'train\\':\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0loss.backward ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0optimizer.step ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0running_loss\\xa0+=\\xa0loss.item ()\\xa0*\\xa0inputs.size (0)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0running_corrects\\xa0+=\\xa0torch. sum(preds\\xa0==\\xa0labels.data )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0epoch_loss\\xa0=\\xa0running_loss\\xa0/\\xa0dataset_sizes [phase]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0epoch_acc\\xa0\\xa0=\\xa0running_corrects.double ()\\xa0/\\xa0dataset_sizes [phase]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 print(f\\'{phase}\\xa0Loss:\\xa0{epoch_loss :.4f}\\xa0Acc:\\xa0{epoch_acc :.4f}\\')\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0new_acc\\xa0\\xa0=\\xa0epoch_acc\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0new_loss\\xa0=\\xa0epoch_loss\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nprint(f\\'Transfer\\xa0Learning\\xa0Model\\xa0Final:\\xa0loss\\xa0 {new_loss :.4f}\\xa0|\\xa0accuracy\\xa0 {new_acc:.4f}\\')\\nprint(\"Training\\xa0complete!\" )\\ntrain Loss: 1.1920 Acc: 0.4605\\nval Loss: 0.9323 Acc: 0.5771\\ntrain Loss: 1.1299 Acc: 0.50525/5/24, 1:00 PM Jorge-Gosalvez_255_HW3 (1).ipynb - Colab\\nhttps://colab.research.google.com/drive/1P3xSSReWugbWYgunQEEK6ktGeMY3734S#printMode=true 13/14', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW3_CNN.pdf', 'page': 12}), Document(page_content='val Loss: 0.8110 Acc: 0.6356\\ntrain Loss: 1.1087 Acc: 0.5139\\nval Loss: 0.9193 Acc: 0.6144\\nTransfer Learning Model Final: loss 0.9193 | accuracy 0.6144\\nTraining complete!\\nCPU times: user 2h 7min 39s, sys: 9min 45s, total: 2h 17min 24s\\nWall time: 1h 12min 57s\\n#\\xa0Save\\xa0the\\xa0model\\ntorch.save (model.state_dict (),\\xa0\\'3_class_CIFAR_classification_model.pth\\' )\\n#\\xa0Load\\xa0the\\xa0saved\\xa0modelmodel\\xa0=\\xa0models.inception_v3( pretrained=True)\\nmodel.fc\\xa0=\\xa0nn.Linear (model.fc.in_features ,\\xa01000)\\xa0\\xa0#\\xa0Adjust\\xa0to\\xa0match\\xa0the\\xa0original\\xa0model\\'s\\xa0output\\xa0unit s\\nmodel.load_state_dict (torch.load (\\'3_class_CIFAR_classification_model.pth\\' ))\\nmodel.eval()\\n#\\xa0Create\\xa0a\\xa0new\\xa0model\\xa0with\\xa0the\\xa0correct\\xa0final\\xa0layer\\n#new_model\\xa0=\\xa0models.resnet18(pretrained=True)\\nnew_model\\xa0=\\xa0models.inception_v3 (pretrained= True)\\nnew_model.fc\\xa0=\\xa0nn.Linear (new_model.fc.in_features ,\\xa02)\\xa0\\xa0#\\xa0Adjust\\xa0to\\xa0match\\xa0the\\xa0desired\\xa0output\\xa0units\\n#\\xa0Copy\\xa0the\\xa0weights\\xa0and\\xa0biases\\xa0from\\xa0the\\xa0loaded\\xa0mode l\\xa0to\\xa0the\\xa0new\\xa0model\\nnew_model.fc.weight.data\\xa0=\\xa0model.fc.weight.data [0:2]\\xa0\\xa0#\\xa0Copy\\xa0only\\xa0the\\xa0first\\xa02\\xa0output\\xa0units\\nnew_model.fc.bias.data\\xa0\\xa0\\xa0=\\xa0model.fc.bias.data [0:2]\\ndef\\xa0check_new (image,\\xa0input_batch ):\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Load\\xa0and\\xa0preprocess\\xa0the\\xa0unseen\\xa0image\\n\\xa0\\xa0\\xa0\\xa0image_path\\xa0=\\xa0image\\xa0\\xa0 #\\xa0Replace\\xa0with\\xa0the\\xa0path\\xa0to\\xa0your\\xa0image\\n\\xa0\\xa0\\xa0\\xa0image\\xa0=\\xa0Image. open(image_path )\\n\\xa0\\xa0\\xa0\\xa0preprocess\\xa0=\\xa0transforms.Compose ([\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.Resize (299),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.CenterCrop (299),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.ToTensor (),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.Normalize ([0.485,\\xa00.456,\\xa00.406],\\xa0[0.229,\\xa00.224,\\xa00.225])\\n\\xa0\\xa0\\xa0\\xa0])\\n\\xa0\\xa0\\xa0\\xa0input_tensor\\xa0=\\xa0preprocess (image)\\n\\xa0\\xa0\\xa0\\xa0input_batch\\xa0=\\xa0input_tensor.unsqueeze (0)\\xa0\\xa0#\\xa0Add\\xa0a\\xa0batch\\xa0dimension\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Perform\\xa0inference\\n\\xa0\\xa0\\xa0\\xa0with\\xa0torch.no_grad ():\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0output\\xa0=\\xa0new_model (input_batch )\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Get\\xa0the\\xa0predicted\\xa0class\\n\\xa0\\xa0\\xa0\\xa0_,\\xa0predicted_class\\xa0=\\xa0output. max(1)\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Map\\xa0the\\xa0predicted\\xa0class\\xa0to\\xa0the\\xa0class\\xa0name\\n\\xa0\\xa0\\xa0\\xa0class_names\\xa0=\\xa0 [\\'0\\',\\xa0\\'2\\',\\xa0\\'6\\']\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0#\\xa0Make\\xa0sure\\xa0these\\xa0class\\xa0names\\xa0match\\xa0your\\xa0training\\xa0 data\\n\\xa0\\xa0\\xa0\\xa0predicted_class_name\\xa0=\\xa0class_names [predicted_class.item ()]\\n\\xa0\\xa0\\xa0\\xa0probabilities\\xa0=\\xa0torch.softmax (outputs,\\xa0dim=1)\\n\\xa0\\xa0\\xa0\\xa0confidence\\xa0=\\xa0torch. max(probabilities ).item()\\n\\xa0\\xa0\\xa0\\xa0print(f\"This\\xa0image\\xa0most\\xa0likely\\xa0belongs\\xa0to\\xa0 {predicted_class_name }\\xa0with\\xa0a\\xa0 {confidence* 100:.2f}%\\xa0confidence.\" )\\n\\xa0\\xa0\\xa0\\xa0print()\\n\\xa0\\xa0\\xa0\\xa0for\\xa0i\\xa0in\\xa0predicted_class_name :\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\xa0i\\xa0==\\xa0\\'0\\':\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0predicted_class_name_text\\xa0=\\xa0 \\'airplane\\'\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\xa0i\\xa0==\\xa0\\'2\\':\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0predicted_class_name_text\\xa0=\\xa0 \\'bird\\'\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\xa0i\\xa0==\\xa0\\'6\\':\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0predicted_class_name_text\\xa0=\\xa0 \\'frog\\'\\n\\xa0\\xa0\\xa0\\xa0print(f\\'The\\xa0predicted\\xa0class\\xa0is:\\xa0 {predicted_class_name }\\')\\n\\xa0\\xa0\\xa0\\xa0print(f\\'The\\xa0predicted\\xa0class\\xa0is:\\xa0 {predicted_class_name_text }\\')\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Display\\xa0the\\xa0image\\xa0with\\xa0the\\xa0predicted\\xa0class\\xa0name\\n\\xa0\\xa0\\xa0\\xa0image\\xa0=\\xa0np.array (image)\\n\\xa0\\xa0\\xa0\\xa0plt.imshow (image)\\n\\xa0\\xa0\\xa0\\xa0plt.axis (\\'off\\')\\n\\xa0\\xa0\\xa0\\xa0plt.text (10,\\xa010,\\xa0f\\'Predicted:\\xa0 {predicted_class_name },\\xa0which\\xa0is\\xa0 {predicted_class_name_text }\\',\\xa0fontsize= 12,\\xa0color=\\'white\\',\\xa0bac\\n\\xa0\\xa0\\xa0\\xa0plt.show ()\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0return\\xa0input_batch5/5/24, 1:00 PM Jorge-Gosalvez_255_HW3 (1).ipynb - Colab\\nhttps://colab.research.google.com/drive/1P3xSSReWugbWYgunQEEK6ktGeMY3734S#printMode=true 14/14', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW3_CNN.pdf', 'page': 13}), Document(page_content=\"Homework 04: Transf er Learning and Bounding Bo xes and Y OLOV8SJSU MSDS 255 DL, Spring 2024\\ue313\\nGit: https:/ /github.com/jr gosalv ez/data255_DL\\nNO TE: YOLOv8 r elies on P yTorch as its deep learning fr amework. NVIDI A NGC Catalog off ers additional GPU optimiz ed fr ameworks for\\nalternativ e transf er learning models, fr ameworks, and containers (e.g. NeMO).\\nCollect a sour ce video.\\nYOLO8 accepts the following video formats: .asf, .a vi, .gif, .m4v , .mkv , .mo v, .mp4, .mpeg, .mpg, .ts, .wmv , .webm.\\niphone video cr eates .mp4 videos; I cr eated 5-10 sec str eet videos and sa ved them locally .\\nYOLO8 can classify , detect, and or segment with nano, small, medium, lar ge, or huge pr e-trained models; I use small detection for this\\nexperimentStep 1:\\nConduct inf erence on video, fr ame b y frame, dr awing bounding bo xes ar ound detected objects (speci\\x00cally v ehicles) and output a video of the\\nobject detection r esults.Steps 2 & 3:\\ue313\\n#\\xa0check\\xa0mac\\xa0systems\\xa0for\\xa0GPU\\nimport\\xa0torch\\nprint(torch.backends.mps.is_available ())\\xa0\\xa0\\xa0#\\xa0check\\xa0for\\xa0Mac\\xa0M1\\xa0GPU\\nFalse\\n#\\xa0import\\xa0openCV,\\xa0numpy,\\xa0and\\xa0ultralytics\\xa0YOLO8\\nimport\\xa0cv2\\nfrom\\xa0ultralytics\\xa0 import\\xa0YOLO\\nimport\\xa0numpy\\xa0as\\xa0np\\nPart 2\\ue313\\nFine Tune Object Detection fr om 10 Cust om IM AGES with Y OLO8 Transf er Learning \\ue313\\nSour ce:\\nhttps:/ /univ erse.r obo\\x00ow .com/y olo-a6y21/squid-bat-butter\\x00y\\nhttps:/ /blog.r obo\\x00ow .com/how-t o-train-y olov8-on-a-cust om-dataset/\\nhttps:/ /www .freecodecamp.or g/news/how-t o-detect-objects-in-images-using-y olov8/\\nhttps:/ /medium.com/@pat.x.guillen/a-step-b y-step-guide-t o-running-y olov8-on-windows-122cb586b567\\nRepeat Steps 1-3 but for images on 10 images of the Robo\\x00ow squid-bat-butter\\x00y datasets.\\nPrep cust om Robo\\x00ow squid-bat-butter\\x00y dataset \\ue313\\nUse r obo\\x00ow t o prepar e dataset and annotate. NO TE: can do it online with Robo\\x00ow work\\x00ow low-code work\\x00ow t ools or 'Cust om Train and\\nUpload' t o get Y OLOv8 code snippet t o do manually .\\nfrom\\xa0IPython\\xa0 import\\xa0display\\ndisplay.clear_output ()\\nfrom\\xa0IPython.display\\xa0 import\\xa0display ,\\xa0Image5/5/24, 1:02 PM Part_2_Jorge-Gosalvez_255_HW4.ipynb - Colab\\nhttps://colab.research.google.com/drive/1Cnfp7efAdIjABAjqlTbOWh4Lu_DvdCCT#printMode=true 1/7\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW4_ObjDetect_YOLO.pdf', 'page': 0}), Document(page_content='Get Cust om Dataset fr om Robo\\x00ow \\ue313\\n!yolo\\xa0task=detect\\xa0mode=predict\\xa0model=yolov8s .pt\\xa0conf= 0.25\\xa0source= \\'Part_2_Jorge_Gosalvez_255_boston_dog.jpeg\\' \\xa0save=True\\nUltralytics YOLOv8.1.17 🚀  Python-3.11.5 torch-2.2.0 CPU (Intel Core(TM) i5-8210Y 1.60GHz)\\nYOLOv8s summary (fused): 168 layers, 11156544 parameters, 0 gradients, 28.6 GFLOPs\\nimage 1/1 /Users/gosalvez/Desktop/Git/python_practice/python_practice/HW/DATA255_DL_SPRING2024/HW4/Part_2_Jorge_Gosalvez_255\\nSpeed: 3.2ms preprocess, 341.9ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\\nResults saved to runs/detect/predict\\n💡 Learn more at https://docs.ultralytics.com/modes/predict\\nTip: The \\x00rst time y ou run the pr ediction tr ain and pr edict folders ar e created. Subsequent runs append a # t o the folder . Be sur e to update the \\x00le path corr ectly t o\\nfetch the corr ect r esults.\\nImage(filename= \\'runs/detect/predict/Part_2_Jorge_Gosalvez_255_bos ton_dog.jpeg\\' ,\\xa0height= 400)\\n!pip\\xa0install\\xa0roboflow\\nfrom\\xa0roboflow\\xa0 import\\xa0Roboflow\\nrf\\xa0=\\xa0Roboflow (api_key= \"ytVnstTGTkiOHxfkYXsM\" )\\nproject\\xa0=\\xa0rf.workspace (\"sjsu-dl-255-squidbatbutterfly\" ).project (\"transfer-learning-and-fine-tune\" )\\ndataset\\xa0=\\xa0project.version (2).download (\"yolov8\" )\\nRequirement already satisfied: roboflow in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (1.1.19)\\nRequirement already satisfied: certifi==2023.7.22 in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from robofl\\nRequirement already satisfied: chardet==4.0.0 in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from roboflow) \\nRequirement already satisfied: cycler==0.10.0 in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from roboflow) \\nRequirement already satisfied: idna==2.10 in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from roboflow) (2.1\\nRequirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from roboflo\\nRequirement already satisfied: matplotlib in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from roboflow) (3.7\\nRequirement already satisfied: numpy>=1.18.5 in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from roboflow) (\\nRequirement already satisfied: opencv-python-headless==4.8.0.74 in /opt/anaconda3/envs/python311/lib/python3.11/site-package\\nRequirement already satisfied: Pillow>=7.1.2 in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from roboflow) (\\nRequirement already satisfied: python-dateutil in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from roboflow)\\nRequirement already satisfied: python-dotenv in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from roboflow) (\\nRequirement already satisfied: requests in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from roboflow) (2.31.\\nRequirement already satisfied: six in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from roboflow) (1.16.0)\\nRequirement already satisfied: supervision in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from roboflow) (0.\\nRequirement already satisfied: urllib3>=1.26.6 in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from roboflow)\\nRequirement already satisfied: tqdm>=4.41.0 in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from roboflow) (4\\nRequirement already satisfied: PyYAML>=5.3.1 in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from roboflow) (\\nRequirement already satisfied: requests-toolbelt in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from roboflo\\nRequirement already satisfied: python-magic in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from roboflow) (0\\nRequirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from matplotl\\nRequirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from matplot\\nRequirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from matplotli\\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from matplotl\\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from \\nRequirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from \\nRequirement already satisfied: scipy<2.0.0,>=1.10.0 in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from supe\\nloading Roboflow workspace...\\nloading Roboflow project...\\nDependency ultralytics==8.0.196 is required but found version=8.1.17, to fix: `pip install ultralytics==8.0.196`5/5/24, 1:02 PM Part_2_Jorge-Gosalvez_255_HW4.ipynb - Colab\\nhttps://colab.research.google.com/drive/1Cnfp7efAdIjABAjqlTbOWh4Lu_DvdCCT#printMode=true 2/7', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW4_ObjDetect_YOLO.pdf', 'page': 1}), Document(page_content=\"Cust om Train on Y OLOv8 using Robo\\x00ow pr epar ed squid-bat-butter\\x00y dataset \\ue313\\nNO TE: Robo\\x00ow outputs a data.location path defaulted t o your pr oject name; howe ver, it repeats it twice in the data.y aml \\x00le and should be corr ected t o match the\\n../test/images  path as t o avoid duplicating the dataset folder name twice in the data.y aml \\x00le for tr ain and v al paths t o data.\\n!yolo\\xa0task=detect\\xa0mode=train\\xa0model=yolov8s .pt\\xa0data= {dataset.location }/data.yaml\\xa0epochs= 12\\xa0imgsz=800\\xa0plots=True\\nNew https://pypi.org/project/ultralytics/8.1.18  available 😃  Update with 'pip install -U ultralytics'\\nUltralytics YOLOv8.1.17 🚀  Python-3.11.5 torch-2.2.0 CPU (Intel Core(TM) i5-8210Y 1.60GHz)\\nengine/trainer: task=detect, mode=train, model=yolov8s.pt, data=/Users/gosalvez/Desktop/Git/python_practice/python_practice/\\nOverriding model.yaml nc=80 with nc=3\\n                   from  n    params  module                                       arguments                     \\n  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \\n  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \\n  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \\n  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \\n  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \\n  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \\n  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \\n  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \\n  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \\n  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \\n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \\n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \\n 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \\n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \\n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \\n 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \\n 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \\n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \\n 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \\n 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \\n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \\n 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \\n 22        [15, 18, 21]  1   2117209  ultralytics.nn.modules.head.Detect           [3, [128, 256, 512]]          \\nModel summary: 225 layers, 11136761 parameters, 11136745 gradients, 28.7 GFLOPs\\nTransferred 349/355 items from pretrained weights\\nTensorBoard: Start with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\\nFreezing layer 'model.22.dfl.conv.weight'\\ntrain: Scanning /Users/gosalvez/Desktop/Git/python_practice/python_practice/HW/D\\nval: Scanning /Users/gosalvez/Desktop/Git/python_practice/python_practice/HW/DAT\\nPlotting labels to runs/detect/train/labels.jpg... \\noptimizer:  'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'mom\\noptimizer:  AdamW(lr=0.001429, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(dec\\nTensorBoard: model graph visualization added ✅\\nImage sizes 800 train, 800 val\\nUsing 0 dataloader workers\\nLogging results to runs/detect/train\\nStarting training for 12 epochs...\\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\\n       1/12         0G      1.643      3.517      2.141          4        800: 1\\n                 Class     Images  Instances      Box(P          R      mAP50  mWARNING ⚠  NMS time limit 3.150s exceeded\\n                 Class     Images  Instances      Box(P          R      mAP50  m\\n                   all         23         24      0.727      0.169      0.188      0.124\\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\\n       2/12         0G      1.192      2.366      1.813          3        800: 1\\n                 Class     Images  Instances      Box(P          R      mAP50  mWARNING ⚠  NMS time limit 3.150s exceeded\\n                 Class     Images  Instances      Box(P          R      mAP50  m\\n                   all         23         24      0.628      0.615      0.661      0.322\\nClosing dataloader mosaic\\nEhGPU bl lldfllI Si\\n!ls\\xa0runs/detect/train\\nF1_curve.png\\nPR_curve.png\\nP_curve.png\\nR_curve.png\\nargs.yaml\\nconfusion_matrix.png\\nconfusion_matrix_normalized.png\\nevents.out.tfevents.1708901625.Ricks-MacBook-Air-2.local.51669.0\\nlabels.jpg\\nlabels_correlogram.jpg\\nresults.csv\\nresults.png5/5/24, 1:02 PM Part_2_Jorge-Gosalvez_255_HW4.ipynb - Colab\\nhttps://colab.research.google.com/drive/1Cnfp7efAdIjABAjqlTbOWh4Lu_DvdCCT#printMode=true 3/7\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW4_ObjDetect_YOLO.pdf', 'page': 2}), Document(page_content=\"train_batch0.jpg\\ntrain_batch1.jpg\\ntrain_batch16.jpg\\ntrain_batch17.jpg\\ntrain_batch18.jpg\\ntrain_batch2.jpg\\nval_batch0_labels.jpg\\nval_batch0_pred.jpg\\nweights\\nImage(filename= f'runs/detect/train/confusion_matrix.png' ,\\xa0width=800)\\nImage(filename= f'runs/detect/train/F1_curve.png' ,\\xa0width=800)5/5/24, 1:02 PM Part_2_Jorge-Gosalvez_255_HW4.ipynb - Colab\\nhttps://colab.research.google.com/drive/1Cnfp7efAdIjABAjqlTbOWh4Lu_DvdCCT#printMode=true 4/7\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW4_ObjDetect_YOLO.pdf', 'page': 3}), Document(page_content=\"Image(filename= f'runs/detect/train/results.png' ,\\xa0width=800)\\nImage(filename= f'runs/detect/train/val_batch0_pred.jpg' ,\\xa0width=800)\\n!yolo\\xa0task=detect\\xa0mode=val\\xa0model=runs/detect/train/ weights/best .pt\\xa0data= {dataset.location }/data.yaml\\nUltralytics YOLOv8.1.17 🚀  Python-3.11.5 torch-2.2.0 CPU (Intel Core(TM) i5-8210Y 1.60GHz)\\nModel summary (fused): 168 layers, 11126745 parameters, 0 gradients, 28.4 GFLOPs\\nval: Scanning /Users/gosalvez/Desktop/Git/python_practice/python_practice/HW/DAT\\n                 Class     Images  Instances      Box(P          R      mAP50  m\\n                   all         23         24      0.924      0.889      0.925      0.6015/5/24, 1:02 PM Part_2_Jorge-Gosalvez_255_HW4.ipynb - Colab\\nhttps://colab.research.google.com/drive/1Cnfp7efAdIjABAjqlTbOWh4Lu_DvdCCT#printMode=true 5/7\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW4_ObjDetect_YOLO.pdf', 'page': 4}), Document(page_content='                   Bat         23          6      0.903      0.667      0.831      0.443\\n             Butterfly         23          8      0.965          1      0.995      0.515\\n                 Squid         23         10      0.904          1       0.95      0.845\\nSpeed: 7.7ms preprocess, 969.1ms inference, 0.0ms loss, 2.2ms postprocess per image\\nResults saved to runs/detect/val\\n💡 Learn more at https://docs.ultralytics.com/modes/val\\n!yolo\\xa0task=detect\\xa0mode=predict\\xa0model=runs/detect/tr ain/weights/best .pt\\xa0conf= 0.25\\xa0source= {dataset.location }/test/images\\xa0save=True\\nUltralytics YOLOv8.1.17 🚀  Python-3.11.5 torch-2.2.0 CPU (Intel Core(TM) i5-8210Y 1.60GHz)\\nModel summary (fused): 168 layers, 11126745 parameters, 0 gradients, 28.4 GFLOPs\\nimage 1/10 /Users/gosalvez/Desktop/Git/python_practice/python_practice/HW/DATA255_DL_SPRING2024/HW4/Transfer-Learning-and-Fi\\nimage 2/10 /Users/gosalvez/Desktop/Git/python_practice/python_practice/HW/DATA255_DL_SPRING2024/HW4/Transfer-Learning-and-Fi\\nimage 3/10 /Users/gosalvez/Desktop/Git/python_practice/python_practice/HW/DATA255_DL_SPRING2024/HW4/Transfer-Learning-and-Fi\\nimage 4/10 /Users/gosalvez/Desktop/Git/python_practice/python_practice/HW/DATA255_DL_SPRING2024/HW4/Transfer-Learning-and-Fi\\nimage 5/10 /Users/gosalvez/Desktop/Git/python_practice/python_practice/HW/DATA255_DL_SPRING2024/HW4/Transfer-Learning-and-Fi\\nimage 6/10 /Users/gosalvez/Desktop/Git/python_practice/python_practice/HW/DATA255_DL_SPRING2024/HW4/Transfer-Learning-and-Fi\\nimage 7/10 /Users/gosalvez/Desktop/Git/python_practice/python_practice/HW/DATA255_DL_SPRING2024/HW4/Transfer-Learning-and-Fi\\nimage 8/10 /Users/gosalvez/Desktop/Git/python_practice/python_practice/HW/DATA255_DL_SPRING2024/HW4/Transfer-Learning-and-Fi\\nimage 9/10 /Users/gosalvez/Desktop/Git/python_practice/python_practice/HW/DATA255_DL_SPRING2024/HW4/Transfer-Learning-and-Fi\\nimage 10/10 /Users/gosalvez/Desktop/Git/python_practice/python_practice/HW/DATA255_DL_SPRING2024/HW4/Transfer-Learning-and-F\\nSpeed: 8.6ms preprocess, 729.6ms inference, 1.4ms postprocess per image at shape (1, 3, 800, 800)\\nResults saved to runs/detect/predict2\\n💡 Learn more at https://docs.ultralytics.com/modes/predict\\nReview r esults \\ue313\\nimport\\xa0glob\\nfrom\\xa0IPython.display\\xa0 import\\xa0display ,\\xa0HTML\\nfrom\\xa0PIL\\xa0import\\xa0Image\\n#\\xa0Get\\xa0list\\xa0of\\xa0image\\xa0paths\\nimage_paths\\xa0=\\xa0glob.glob (f\\'runs/detect/predict2/*.jpg\\' )[:10]\\n#\\xa0Create\\xa0HTML\\xa0table\\ntable_html\\xa0=\\xa0 \"<table><tr>\"\\n#\\xa0Loop\\xa0through\\xa0image\\xa0paths\\nfor\\xa0i,\\xa0image_path\\xa0 in\\xa0enumerate (image_paths ):\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Open\\xa0image\\n\\xa0\\xa0\\xa0\\xa0img\\xa0=\\xa0Image. open(image_path )\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Resize\\xa0image\\xa0to\\xa0fit\\xa0in\\xa0table\\n\\xa0\\xa0\\xa0\\xa0img.thumbnail ((400,\\xa0400))\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Add\\xa0image\\xa0to\\xa0table\\n\\xa0\\xa0\\xa0\\xa0table_html\\xa0+=\\xa0 f\"<td><img\\xa0src=\\' {image_path }\\'\\xa0width=\\'400\\'/></td>\"\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Break\\xa0to\\xa0next\\xa0row\\xa0after\\xa0every\\xa02\\xa0images\\n\\xa0\\xa0\\xa0\\xa0if\\xa0(i\\xa0+\\xa01)\\xa0%\\xa05\\xa0==\\xa00:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0table_html\\xa0+=\\xa0 \"</tr><tr>\"\\n\\xa0\\xa0\\xa0\\xa0\\ntable_html\\xa0+=\\xa0 \"</tr></table>\"\\n#\\xa0Display\\xa0table\\ndisplay(HTML(table_html ))\\nIf satis\\x00ed, BINGO! Y ou now ha ve trained weights in the runs/detect/tr ain/weights/best.pt tr ained model that can be used for inf erence in\\nweb apps b y deplo ying!\\nCan deplo y locally for local de velopment and use or on hosted infr astructur ed, such as A WS, A zure, or GCS, Robo\\x00ow , etcReady t o Deplo y Your Trained Model5/5/24, 1:02 PM Part_2_Jorge-Gosalvez_255_HW4.ipynb - Colab\\nhttps://colab.research.google.com/drive/1Cnfp7efAdIjABAjqlTbOWh4Lu_DvdCCT#printMode=true 6/7', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW4_ObjDetect_YOLO.pdf', 'page': 5}), Document(page_content='5/5/24, 1:02 PM Part_2_Jorge-Gosalvez_255_HW4.ipynb - Colab\\nhttps://colab.research.google.com/drive/1Cnfp7efAdIjABAjqlTbOWh4Lu_DvdCCT#printMode=true 7/7', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW4_ObjDetect_YOLO.pdf', 'page': 6}), Document(page_content='Homework 08: LSTM for Time Series Pr edictionSJSU MSDS 255 DL, Spring 2024\\ue313\\nGit: https:/ /github.com/jr gosalv ez/data255_DL\\nimport\\xa0matplotlib.pyplot\\xa0 as\\xa0plt\\nimport\\xa0numpy\\xa0as\\xa0np\\nimport\\xa0pandas\\xa0 as\\xa0pd\\nimport\\xa0torch\\nimport\\xa0torch.nn\\xa0 as\\xa0nn\\nimport\\xa0torch.optim\\xa0 as\\xa0optim\\nimport\\xa0torch.utils.data\\xa0 as\\xa0data\\nfrom\\xa0sklearn.preprocessing\\xa0 import\\xa0MinMaxScaler\\nimport\\xa0warnings\\nwarnings.filterwarnings (\"ignore\" )\\nImpor t the data fr om .csv \\x00le \\ue313\\ndf\\xa0=\\xa0pd.read_csv (\\'Google_Stock_Price_Train.csv\\' )\\nprint(\\'Number\\xa0of\\xa0rows\\xa0and\\xa0columns:\\' ,\\xa0df.shape )\\nprint()\\ndf5/5/24, 1:05 PM Jorge_Gosalvez_255_HW8_LSTM.ipynb - Colab\\nhttps://colab.research.google.com/drive/1hwLhCVmpzyBOANySoVFXy6pvuf5Rndx8#printMode=true 1/7', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW8_LSTM.pdf', 'page': 0}), Document(page_content='Number of rows and columns: (1258, 6)\\nDateOpenHigh LowClose Volume\\n0 1/3/2012 325.25 332.83 324.97 663.59 7,380,500\\n1 1/4/2012 331.27 333.87 329.08 666.45 5,749,400\\n2 1/5/2012 329.83 330.75 326.89 657.21 6,590,300\\n3 1/6/2012 328.34 328.77 323.68 648.24 5,405,900\\n4 1/9/2012 322.04 322.29 309.46 620.76 11,688,800\\n... ... ... ... ... ... ...\\n1253 12/23/2016 790.90 792.74 787.28 789.91 623,400\\n1254 12/27/2016 790.68 797.86 787.66 791.55 789,100\\n1255 12/28/2016 793.70 794.23 783.20 785.05 1,153,800\\n1256 12/29/2016 783.33 785.93 778.92 782.79 744,300\\n1257 12/30/2016 782.75 782.78 770.41 771.82 1,770,000\\n1258 rows × 6 columns\\nSanity Check:  The Close  featur e is incorr ect b/c mor e than 2x the High v alue of the da y... does not mak e sense.\\nCannot close higher than the highest v alue of the da y.\\nPreprocess the data \\ue313\\n#\\xa0Get\\xa0and\\xa0plot\\xa0\\'Open\\'\\xa0value;\\xa0feature\\xa0that\\xa0model\\xa0wi ll\\xa0train\\xa0and\\xa0test\\xa0on\\ntimeseries\\xa0=\\xa0df [[\"Open\"]].values.astype (\\'float32\\' )\\nplt.plot (timeseries )\\nplt.show ()5/5/24, 1:05 PM Jorge_Gosalvez_255_HW8_LSTM.ipynb - Colab\\nhttps://colab.research.google.com/drive/1hwLhCVmpzyBOANySoVFXy6pvuf5Rndx8#printMode=true 2/7', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW8_LSTM.pdf', 'page': 1}), Document(page_content='#\\xa0Feature\\xa0Scaling\\nsc\\xa0=\\xa0MinMaxScaler (feature_range\\xa0=\\xa0 (0,\\xa01))\\ntimeseries_scaled\\xa0=\\xa0sc.fit_transform (timeseries )\\nBecause a small dataset, cr eating a P yTorch tensor dir ectly fr om a list of nump y arr ays; howe ver,\\nthis can be slow fr or complex models with lar ge datasets. F or mor e complicated models and lar ger\\ndatasets, conv ert list of nump y arr ays int o a single nump y arr ay befor e conv erting it a tensor .\\nPerformance is not an issue with this model, so did not pr eprocess furhter .\\nSplit int o 80% tr ain 20% pr ediction \\ue313\\n#\\xa0train-test\\xa0split\\xa0for\\xa0timeseries\\xa0data\\xa0frame\\ntrain_size\\xa0\\xa0=\\xa0 int(len(timeseries_scaled )\\xa0*\\xa00.8)\\ntest_size\\xa0\\xa0\\xa0=\\xa0 len(timeseries_scaled )\\xa0-\\xa0train_size\\ntrain,\\xa0test\\xa0=\\xa0timeseries_scaled [:train_size ],\\xa0timeseries_scaled [train_size :]\\nCreate the dataset\\ue3135/5/24, 1:05 PM Jorge_Gosalvez_255_HW8_LSTM.ipynb - Colab\\nhttps://colab.research.google.com/drive/1hwLhCVmpzyBOANySoVFXy6pvuf5Rndx8#printMode=true 3/7', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW8_LSTM.pdf', 'page': 2}), Document(page_content='def\\xa0create_dataset (dataset,\\xa0lookback ):\\n\\xa0\\xa0\\xa0\\xa0\"\"\"Transform\\xa0a\\xa0time\\xa0series\\xa0into\\xa0a\\xa0prediction\\xa0datas et\\n\\xa0\\xa0\\xa0\\xa0Args:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0dataset:\\xa0A\\xa0numpy\\xa0array\\xa0of\\xa0time\\xa0series,\\xa0fir st\\xa0dimension\\xa0is\\xa0the\\xa0time\\xa0steps\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0lookback:\\xa0Size\\xa0of\\xa0window\\xa0for\\xa0prediction\\n\\xa0\\xa0\\xa0\\xa0\"\"\"\\n\\xa0\\xa0\\xa0\\xa0X,\\xa0y\\xa0=\\xa0[],\\xa0[]\\n\\xa0\\xa0\\xa0\\xa0for\\xa0i\\xa0in\\xa0range(len(dataset)-lookback ):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0feature\\xa0=\\xa0dataset [i:i+lookback ]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0target\\xa0=\\xa0dataset [i+1:i+lookback+ 1]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0X.append (feature)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0y.append (target)\\n\\xa0\\xa0\\xa0\\xa0return\\xa0torch.tensor (X),\\xa0torch.tensor (y)\\nlookback\\xa0=\\xa0 4\\nX_train,\\xa0y_train\\xa0=\\xa0create_dataset (train,\\xa0lookback=lookback )\\nX_test,\\xa0y_test\\xa0\\xa0\\xa0=\\xa0create_dataset (test,\\xa0lookback=lookback )\\nprint(X_train.shape ,\\xa0y_train.shape )\\nprint(X_test.shape ,\\xa0y_test.shape )\\ntorch.Size([1002, 4, 1]) torch.Size([1002, 4, 1])\\ntorch.Size([248, 4, 1]) torch.Size([248, 4, 1])\\nCreate and tr ain the model on the data \\ue313\\nCreate the model\\ue313\\nUnsuppor ted Cell Type. Double-Click t o inspect/edit the content.5/5/24, 1:05 PM Jorge_Gosalvez_255_HW8_LSTM.ipynb - Colab\\nhttps://colab.research.google.com/drive/1hwLhCVmpzyBOANySoVFXy6pvuf5Rndx8#printMode=true 4/7', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW8_LSTM.pdf', 'page': 3}), Document(page_content='#\\xa0LSTM\\xa0model\\xa0with\\xa02\\xa0lstm\\xa0layers,\\xa0linear\\xa0function\\xa0l ayers\\xa0and\\xa0dropout\\xa0regularization\\xa0t\\nclass\\xa0my_LSTM(nn.Module):\\n\\xa0\\xa0\\xa0\\xa0def\\xa0__init__ (self,\\xa0input_size =1,\\xa0hidden_size =50,\\xa0num_layers =2,\\xa0dropout=0.2):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0super ().__init__ ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.lstm\\xa0=\\xa0nn.LSTM (input_size=input_size ,\\xa0hidden_size=hidden_size ,\\xa0num_laye\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.dropout\\xa0=\\xa0nn.Dropout (p=dropout )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.linear\\xa0=\\xa0nn.Linear (hidden_size ,\\xa01)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0def\\xa0forward(self,\\xa0x):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Apply\\xa0dropout\\xa0to\\xa0the\\xa0input\\xa0sequence\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0 self.dropout (x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Pass\\xa0through\\xa0LSTM\\xa0layers\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x ,\\xa0_\\xa0=\\xa0self.lstm(x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Apply\\xa0dropout\\xa0to\\xa0the\\xa0LSTM\\xa0output\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0 self.dropout (x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Pass\\xa0through\\xa0linear\\xa0layer\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0 self.linear(x)\\xa0\\xa0#\\xa0transform\\xa0the\\xa0entire\\xa0batch\\xa0of\\xa0data\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Squeeze\\xa0to\\xa0remove\\xa0the\\xa0extra\\xa0dimension\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0torch.squeeze (x,\\xa0dim=1)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return\\xa0x\\nInitiate the model, optimiz er, and loader \\ue313\\nmodel\\xa0\\xa0\\xa0\\xa0\\xa0=\\xa0my_LSTM ()\\noptimizer\\xa0=\\xa0optim.Adam (model.parameters ())\\nloss_fn\\xa0\\xa0\\xa0=\\xa0nn.MSELoss ()\\xa0\\xa0\\xa0#\\xa0linear,\\xa0so\\xa0MSE\\nloader\\xa0\\xa0\\xa0\\xa0=\\xa0data.DataLoader (data.TensorDataset (X_train,\\xa0y_train ),\\xa0shuffle= True,\\xa0batc\\nTrain and v erify LSTM model / network \\ue3135/5/24, 1:05 PM Jorge_Gosalvez_255_HW8_LSTM.ipynb - Colab\\nhttps://colab.research.google.com/drive/1hwLhCVmpzyBOANySoVFXy6pvuf5Rndx8#printMode=true 5/7', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW8_LSTM.pdf', 'page': 4}), Document(page_content='%%time\\nn_epochs\\xa0=\\xa0 1001\\nfor\\xa0epoch\\xa0in\\xa0range(n_epochs ):\\n\\xa0\\xa0\\xa0\\xa0model.train ()\\n\\xa0\\xa0\\xa0\\xa0for\\xa0X_batch ,\\xa0y_batch\\xa0 in\\xa0loader:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0y_pred\\xa0=\\xa0model (X_batch)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0loss\\xa0\\xa0\\xa0=\\xa0loss_fn (y_pred,\\xa0y_batch )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0optimizer.zero_grad ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0loss.backward ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0optimizer.step ()\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Validation\\n\\xa0\\xa0\\xa0\\xa0if\\xa0epoch\\xa0%\\xa0 100\\xa0!=\\xa00:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 continue\\n\\xa0\\xa0\\xa0\\xa0model. eval()\\n\\xa0\\xa0\\xa0\\xa0with\\xa0torch.no_grad ():\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0y_pred\\xa0=\\xa0model (X_train)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0train_rmse\\xa0=\\xa0np.sqrt (loss_fn(y_pred,\\xa0y_train ))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0y_pred\\xa0=\\xa0model (X_test)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0test_rmse\\xa0=\\xa0np.sqrt (loss_fn(y_pred,\\xa0y_test))\\n\\xa0\\xa0\\xa0\\xa0print(\"Epoch\\xa0%d:\\xa0train\\xa0RMSE\\xa0%.4f,\\xa0test\\xa0RMSE\\xa0%.4f\" \\xa0%\\xa0(epoch,\\xa0train_rmse ,\\xa0test_rms\\nEpoch 0: train RMSE 0.1420, test RMSE 0.2750\\nEpoch 100: train RMSE 0.0906, test RMSE 0.1789\\nEpoch 200: train RMSE 0.0909, test RMSE 0.1799\\nEpoch 300: train RMSE 0.0948, test RMSE 0.1866\\nEpoch 400: train RMSE 0.0914, test RMSE 0.1755\\nEpoch 500: train RMSE 0.0908, test RMSE 0.1776\\nEpoch 600: train RMSE 0.0943, test RMSE 0.1908\\nEpoch 700: train RMSE 0.0952, test RMSE 0.1734\\nEpoch 800: train RMSE 0.0884, test RMSE 0.1828\\nEpoch 900: train RMSE 0.0919, test RMSE 0.1810\\nEpoch 1000: train RMSE 0.0900, test RMSE 0.1794\\nCPU times: user 14min 42s, sys: 17.8 s, total: 15min\\nWall time: 7min 39s\\nRepor t on the r esults of the model \\ue3135/5/24, 1:05 PM Jorge_Gosalvez_255_HW8_LSTM.ipynb - Colab\\nhttps://colab.research.google.com/drive/1hwLhCVmpzyBOANySoVFXy6pvuf5Rndx8#printMode=true 6/7', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW8_LSTM.pdf', 'page': 5}), Document(page_content=\"#\\xa0Predict\\xa0Google\\xa0stock\\xa0price\\xa0on\\xa0test\\xa0dataset\\xa0and\\xa0c ompare\\xa0to\\xa0actuals\\nwith\\xa0torch.no_grad ():\\n\\xa0\\xa0\\xa0\\xa0#\\xa0shift\\xa0train\\xa0predictions\\xa0for\\xa0plotting\\n\\xa0\\xa0\\xa0\\xa0train_plot\\xa0=\\xa0np.ones_like (timeseries_scaled )\\xa0*\\xa0np.nan\\n\\xa0\\xa0\\xa0\\xa0y_pred\\xa0=\\xa0model (X_train)\\n\\xa0\\xa0\\xa0\\xa0y_pred\\xa0=\\xa0y_pred [:,\\xa0-1,\\xa0:]\\n\\xa0\\xa0\\xa0\\xa0train_plot [lookback :train_size ]\\xa0=\\xa0model (X_train)[:,\\xa0-1,\\xa0:]\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0#\\xa0shift\\xa0test\\xa0predictions\\xa0for\\xa0plotting\\n\\xa0\\xa0\\xa0\\xa0test_plot\\xa0=\\xa0np.ones_like (timeseries_scaled )\\xa0*\\xa0np.nan\\n\\xa0\\xa0\\xa0\\xa0test_plot [train_size+lookback :len(timeseries_scaled )]\\xa0=\\xa0model (X_test)[:,\\xa0-1,\\xa0:]\\n#\\xa0Inverse\\xa0transform\\xa0the\\xa0scaled\\xa0data\\nreal_stock_prices\\xa0=\\xa0sc.inverse_transform (timeseries_scaled )\\npredicted_train\\xa0\\xa0\\xa0=\\xa0sc.inverse_transform (train_plot.reshape (-1,\\xa01))\\npredicted_test\\xa0\\xa0\\xa0\\xa0=\\xa0sc.inverse_transform (test_plot.reshape (-1,\\xa01))\\n\\xa0\\xa0\\xa0\\xa0\\nplt.plot (real_stock_prices ,\\xa0c='b',\\xa0label='Real')\\nplt.plot (predicted_train ,\\xa0c='r',\\xa0label='Trained' )\\nplt.plot (predicted_test ,\\xa0c='g',\\xa0label='Predicted' )\\nplt.title ('Google\\xa0Stock\\xa0Price\\xa0Prediction\\xa0(Jan\\xa02012\\xa0to\\xa0Dec\\xa020 16)')\\npltxlabel('Business Days')5/5/24, 1:05 PM Jorge_Gosalvez_255_HW8_LSTM.ipynb - Colab\\nhttps://colab.research.google.com/drive/1hwLhCVmpzyBOANySoVFXy6pvuf5Rndx8#printMode=true 7/7\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW8_LSTM.pdf', 'page': 6}), Document(page_content='Homework 07: NLPSJSU MSDS 255 DL, Spring 2024\\ue313\\nGit: https:/ /github.com/jr gosalv ez/data255_DL\\nimport\\xa0numpy\\xa0as\\xa0np\\nimport\\xa0pandas\\xa0 as\\xa0pd\\nfrom\\xa0scipy\\xa0import\\xa0spatial\\nfrom\\xa0sklearn.metrics.pairwise\\xa0 import\\xa0cosine_similarity\\nfrom\\xa0sklearn.model_selection\\xa0 import\\xa0train_test_split\\nfrom\\xa0sklearn.naive_bayes\\xa0 import\\xa0MultinomialNB\\nfrom\\xa0sklearn.feature_extraction.text\\xa0 import\\xa0CountVectorizer\\nfrom\\xa0sklearn.metrics\\xa0 import\\xa0accuracy_score ,\\xa0confusion_matrix ,\\xa0classification_report\\nimport\\xa0scikitplot\\xa0 as\\xa0skplt\\nimport\\xa0matplotlib.pyplot\\xa0 as\\xa0plt\\nfrom\\xa0gensim.models\\xa0 import\\xa0KeyedVectors\\nimport\\xa0re\\nimport\\xa0nltk\\nfrom\\xa0nltk.corpus\\xa0 import\\xa0stopwords\\nStep 1: Load the Wikipedia GLoVE W ord2Vec (glo ve.6B.50d.txt) \\ue313\\nDownload GLoV e pretrained models: https:/ /nlp.stanfor d.edu/pr ojects/glo ve/\\nembeddings_dict\\xa0=\\xa0 {}\\nwith\\xa0open(\"glove.6B.50d.txt\" ,\\xa0\\'r\\',\\xa0encoding= \"utf-8\")\\xa0as\\xa0f:\\n\\xa0\\xa0\\xa0\\xa0for\\xa0line\\xa0in\\xa0f:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0values\\xa0=\\xa0line.split ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0word\\xa0\\xa0\\xa0=\\xa0values [0]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0vector\\xa0=\\xa0np.asarray (values[1:],\\xa0\"float32\" )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0embeddings_dict [word]\\xa0=\\xa0vector\\nCreate functions\\ue313\\ndef\\xa0find_closest_embeddings (embedding ):\\n\\xa0\\xa0\\xa0\\xa0return\\xa0sorted(embeddings_dict.keys (),\\xa0key=lambda\\xa0word:\\xa0spatial.distance.euclidean (embeddings_dict [word],\\xa0embedding ))\\ndef\\xa0analogy(wordvec1 ,\\xa0wordvec2 ,\\xa0wordvec3 ):\\n\\xa0\\xa0\\xa0\\xa0analogy\\xa0=\\xa0find_closest_embeddings (embeddings_dict [wordvec1 ]\\xa0-\\xa0embeddings_dict [wordvec3 ]\\xa0+\\xa0embeddings_dict [wordvec2 ])[1:2]\\n\\xa0\\xa0\\xa0\\xa0return\\xa0analogy [0]\\nExperimenting\\ue313\\n#\\xa0Find\\xa0closest\\xa0three\\xa0words\\xa0without\\xa0returning\\xa0the\\xa0w ord\\xa0itself\\nfind_closest_embeddings (embeddings_dict [\"king\"])[1:4]\\n[\\'prince\\', \\'queen\\', \\'uncle\\']\\nprint(find_closest_embeddings (\\n\\xa0\\xa0\\xa0\\xa0embeddings_dict [\"king\"]\\xa0-\\xa0embeddings_dict [\"man\"]\\xa0+\\xa0embeddings_dict [\"woman\"]\\n)[1:4])\\n[\\'queen\\', \\'prince\\', \\'elizabeth\\']\\nprint(find_closest_embeddings (\\n\\xa0\\xa0\\xa0\\xa0embeddings_dict [\"princess\" ]\\xa0-\\xa0embeddings_dict [\"woman\"]\\xa0+\\xa0embeddings_dict [\"man\"]\\n)[1:2])\\n[\\'prince\\']\\nanalogy(\"princess\" ,\\xa0\"man\",\\xa0\"woman\")5/5/24, 1:07 PM Jorge_Gosalvez_255_HW7_NLP.ipynb - Colab\\nhttps://colab.research.google.com/drive/1UsZg3WHJK7kuh4MNL0M1csOhlYXooiox#printMode=true 1/4', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW7_NLP.pdf', 'page': 0}), Document(page_content='\\'prince\\'\\nMan and W oman\\nChair and Throne\\nwater and bab yStep 2: Show how similar these wor ds ar e: \\ue313\\ndef\\xa0similar(wordvec1 ,\\xa0wordvec2 ):\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Get\\xa0the\\xa0embeddings\\xa0for\\xa0\"wordvec1\"\\n\\xa0\\xa0\\xa0\\xa0embedding\\xa0=\\xa0embeddings_dict [wordvec1 ].reshape (1,\\xa0-1)\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Get\\xa0embeddings\\xa0for\\xa0the\\xa0predicted\\xa0words\\n\\xa0\\xa0\\xa0\\xa0predicted_words\\xa0=\\xa0 [wordvec2 ]\\n\\xa0\\xa0\\xa0\\xa0predicted_embeddings\\xa0=\\xa0 [embeddings_dict [word]\\xa0for\\xa0word\\xa0in\\xa0predicted_words ]\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Calculate\\xa0cosine\\xa0similarity\\n\\xa0\\xa0\\xa0\\xa0similarities\\xa0=\\xa0cosine_similarity (embedding ,\\xa0predicted_embeddings )\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Print\\xa0the\\xa0cosine\\xa0similarity\\xa0scores\\n\\xa0\\xa0\\xa0\\xa0for\\xa0word,\\xa0similarity_score\\xa0 in\\xa0zip(predicted_words ,\\xa0similarities [0]):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 print(f\"Cosine\\xa0similarity\\xa0between\\xa0 {wordvec1 }\\xa0and\\xa0{word}:\\xa0{similarity_score :.4f}\")\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nsimilar(\"man\",\\xa0\"woman\")\\nsimilar(\"chair\",\\xa0\"throne\" )\\nsimilar(\"water\",\\xa0\"baby\")\\nCosine similarity between man and woman: 0.8860\\nCosine similarity between chair and throne: 0.2797\\nCosine similarity between water and baby: 0.4081\\n_____ is t o King as W oman is t o Man.\\n_____ is t o Princess as Man is t o Woman.\\n_____ is t o a woman as a child is t o an adult.Step 3: Using these pr ovide analogies for the following: \\ue313\\n#\\xa0Define\\xa0analogy\\xa0examples\\nanalogies\\xa0=\\xa0 [\\n\\xa0\\xa0\\xa0\\xa0(\"king\",\\xa0\"woman\",\\xa0\"man\"),\\n\\xa0\\xa0\\xa0\\xa0(\"princess\" ,\\xa0\"man\",\\xa0\"woman\"),\\n\\xa0\\xa0\\xa0\\xa0(\"woman\",\\xa0\"child\",\\xa0\"adult\")\\n]\\n#\\xa0Calculate\\xa0and\\xa0print\\xa0analogies\\nfor\\xa0analogy_pair\\xa0 in\\xa0analogies :\\n\\xa0\\xa0\\xa0\\xa0analogy_word\\xa0=\\xa0analogy (*analogy_pair )\\n\\xa0\\xa0\\xa0\\xa0print(f\"\\'{analogy_word }\\'\\xa0is\\xa0to\\xa0\\' {analogy_pair [0]}\\'\\xa0as\\xa0\\'{analogy_pair [1]}\\'\\xa0is\\xa0to\\xa0\\' {analogy_pair [2]}\\'\")\\n\\'queen\\' is to \\'king\\' as \\'woman\\' is to \\'man\\'\\n\\'prince\\' is to \\'princess\\' as \\'man\\' is to \\'woman\\'\\n\\'mother\\' is to \\'woman\\' as \\'child\\' is to \\'adult\\'\\nStep 4: Apply Naiv e-Ba yes Classi\\x00er on the Spam-Ham dataset shown in the demo \\ue313\\n#\\xa0Load\\xa0the\\xa0Spam-Ham\\xa0dataset\\ndata\\xa0=\\xa0pd.read_csv (\"spam.csv\" ,\\xa0encoding= \\'latin-1\\' )\\ndata.head ()5/5/24, 1:07 PM Jorge_Gosalvez_255_HW7_NLP.ipynb - Colab\\nhttps://colab.research.google.com/drive/1UsZg3WHJK7kuh4MNL0M1csOhlYXooiox#printMode=true 2/4', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW7_NLP.pdf', 'page': 1}), Document(page_content='v1 v2Unnamed: 2 Unnamed: 3 Unnamed: 4\\n0ham Go until jurong point, crazy .. Available only ... NaN NaN NaN\\n1ham Ok lar ... Joking wif u oni... NaN NaN NaN\\n2spam Free entry in 2 a wkly comp to win F A Cup ﬁna... NaN NaN NaN\\n3ham U dun say so early hor ... U c already then say ... NaN NaN NaN\\n4ham Nah I don\\'t think he goes to usf, he lives aro... NaN NaN NaN\\nv1 = tar get / labels\\nv2 = text t o pre-process and use\\nPreparing and exploring pr eprocessed spam-ham data \\ue313\\n#\\xa0Preprocess\\xa0the\\xa0text\\ndef\\xa0preprocess_text (text):\\n\\xa0\\xa0\\xa0\\xa0text\\xa0=\\xa0re.sub (r\\'\\\\W\\',\\xa0\\'\\xa0\\',\\xa0text)\\xa0\\xa0#\\xa0Remove\\xa0non-word\\xa0characters\\n\\xa0\\xa0\\xa0\\xa0text\\xa0=\\xa0text.lower ()\\xa0\\xa0#\\xa0Convert\\xa0text\\xa0to\\xa0lowercase\\n\\xa0\\xa0\\xa0\\xa0tokens\\xa0=\\xa0nltk.word_tokenize (text)\\xa0\\xa0#\\xa0Tokenize\\xa0the\\xa0text\\n\\xa0\\xa0\\xa0\\xa0tokens\\xa0=\\xa0 [word\\xa0for\\xa0word\\xa0in\\xa0tokens\\xa0 if\\xa0word\\xa0not\\xa0in\\xa0stopwords.words (\\'english\\' )]\\xa0\\xa0#\\xa0Remove\\xa0stopwords\\n\\xa0\\xa0\\xa0\\xa0return\\xa0\\'\\xa0\\'.join(tokens)\\ndata[\\'clean_text\\' ]\\xa0=\\xa0data[\\'v2\\'].apply(preprocess_text )\\nv1 v2Unnamed: 2 Unnamed: 3 Unnamed: 4 clean_text\\n0ham Go until jurong point, crazy .. Available only ... NaN NaN NaN go jurong point crazy available bugis n great ...\\n1ham Ok lar ... Joking wif u oni... NaN NaN NaN ok lar joking wif u oni\\n2spam Free entry in 2 a wkly comp to win F A Cup ﬁna... NaN NaN NaN free entry 2 wkly comp win fa cup ﬁnal tkts 2...\\n3ham U dun say so early hor ... U c already then say ... NaN NaN NaN u dun say early hor u c already say\\n4ham Nah I don\\'t think he goes to usf, he lives aro... NaN NaN NaN nah think goes usf lives around thoughdata.head ()\\nSplit the dataset int o train and test sets 80/20 \\ue313\\nX_train,\\xa0X_test,\\xa0y_train ,\\xa0y_test\\xa0=\\xa0train_test_split (data[\\'clean_text\\' ],\\xa0data[\\'v1\\'],\\xa0test_size= 0.2,\\xa0random_state= 42)\\nVectorize data for model ingestion b y conv ert texting data t o numerical f eatur es using CountV ectorizer \\ue313\\nvectorizer\\xa0=\\xa0CountVectorizer ()\\nX_train_counts\\xa0=\\xa0vectorizer.fit_transform (X_train)\\nX_test_counts\\xa0\\xa0=\\xa0vectorizer.transform (X_test)\\nTrain the Naiv e-Ba yes Classi\\x00er \\ue313\\n▾MultinomialNB\\nMultinomialNB()nb_classifier\\xa0=\\xa0MultinomialNB ()\\nnb_classifier.fit (X_train_counts ,\\xa0y_train )\\nEvaluate the classi\\x00er\\ue313\\ny_pred\\xa0=\\xa0nb_classifier.predict (X_test_counts )\\naccuracy\\xa0=\\xa0accuracy_score (y_test,\\xa0y_pred)\\nprint(f\"Accuracy:\\xa0 {accuracy :.4f}\")5/5/24, 1:07 PM Jorge_Gosalvez_255_HW7_NLP.ipynb - Colab\\nhttps://colab.research.google.com/drive/1UsZg3WHJK7kuh4MNL0M1csOhlYXooiox#printMode=true 3/4', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW7_NLP.pdf', 'page': 2}), Document(page_content='Accuracy: 0.9821\\nconfusion_matrix:  [[959   6]\\n [ 14 136]]\\n#confusion\\xa0matrix\\nconfusion_matrix\\xa0=\\xa0confusion_matrix (y_test,\\xa0y_pred,\\xa0labels= [\"ham\",\"spam\"])\\nskplt.metrics.plot_confusion_matrix (y_test,\\xa0y_pred,\\xa0normalize= False)\\nskplt.metrics.plot_confusion_matrix (y_test,\\xa0y_pred,\\xa0normalize= True)\\nprint(\\'confusion_matrix:\\xa0\\' ,\\xa0confusion_matrix )\\nplt.show ()\\n\\x00\\nStart coding or generate  with AI.5/5/24, 1:07 PM Jorge_Gosalvez_255_HW7_NLP.ipynb - Colab\\nhttps://colab.research.google.com/drive/1UsZg3WHJK7kuh4MNL0M1csOhlYXooiox#printMode=true 4/4', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW7_NLP.pdf', 'page': 3})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6m662-pzyNX",
        "outputId": "c7c0cee6-379a-48d1-d488-533982aab8e5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"Homework 10: BER TSJSU MSDS 255 DL, Spring 2024 - Transformers \\ue313\\nGit: https:/ /github.com/jr gosalv ez/data255_DL\\nSour ce:\\nhttps:/ /rajpurkar .github.io/SQuAD-explor er\\nhttps:/ /pytorch.or g/text/stable/datasets.html#t orchtext.datasets.SQuAD2\\nimport\\xa0torch\\nimport\\xa0json\\nimport\\xa0pandas\\xa0 as\\xa0pd\\nfrom\\xa0transformers\\xa0 import\\xa0BertTokenizer\\n/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: User\\nThe secret `HF_TOKEN` does not exist in your Colab secrets.\\nTo authenticate with the Hugging Face Hub, create a token in your settings tab (\\nYou will be able to reuse this secret in all of your notebooks.\\nPlease note that authentication is recommended but still optional to access publ\\n  warnings.warn(\\ntokenizer_conﬁg.json:\\u2007100% \\u200748.0/48.0\\u2007[00:00<00:00,\\u20073.84kB/s]\\nvocab.txt:\\u2007100% \\u2007232k/232k\\u2007[00:00<00:00,\\u20073.93MB/s]\\ntokenizer .json:\\u2007100% \\u2007466k/466k\\u2007[00:00<00:00,\\u20073.70MB/s]\\nconﬁg.json:\\u2007100% \\u2007570/570\\u2007[00:00<00:00,\\u200750.9kB/s]#\\xa0Load\\xa0pre-trained\\xa0model\\xa0tokenizer\\ntokenizer\\xa0=\\xa0BertTokenizer.from_pretrained ('bert-base-uncased' )\\nDownload the tr aining data \\ue313\\n#\\xa0Define\\xa0the\\xa0path\\xa0to\\xa0your\\xa0SQuAD2\\xa0data\\xa0file\\nsquad_data_path\\xa0=\\xa0 '/content/train-v2.0.json'\\n/content/train-v2.0.json\\n'\\n'squad_data_path\\ndf\\xa0=\\xa0pd.read_json (squad_data_path )\\ndf5/5/24, 12:53 PM Jorge_Gosalvez_HW10_BERT.ipynb - Colab\\nhttps://colab.research.google.com/drive/1K2vYT2LBTWdhmSNVh7uEFu0xars8PcSJ#printMode=true 1/7\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW10_BERT.pdf', 'page': 0}),\n",
              " Document(page_content='version data\\n0 v2.0 {\\'title\\': \\'Beyoncé\\', \\'paragraphs\\': [{\\'qas\\': [{...\\n1 v2.0 {\\'title\\': \\'Frédéric_Chopin\\', \\'paragraphs\\': [{\\'...\\n2 v2.0 {\\'title\\': \\'Sino-T ibetan_relations_during_the_M...\\n3 v2.0 {\\'title\\': \\'IPod\\', \\'paragraphs\\': [{\\'qas\\': [{\\'qu...\\n4 v2.0 {\\'title\\': \\'The_Legend_of_Zelda:_T wilight_Princ...\\n... ... ...\\n437 v2.0 {\\'title\\': \\'Infection\\', \\'paragraphs\\': [{\\'qas\\': ...\\n438 v2.0 {\\'title\\': \\'Hunting\\', \\'paragraphs\\': [{\\'qas\\': [{...\\n439 v2.0 {\\'title\\': \\'Kathmandu\\', \\'paragraphs\\': [{\\'qas\\': ...\\n440 v2.0 {\\'title\\': \\'Myocardial_infarction\\', \\'paragraphs...\\n441 v2.0 {\\'title\\': \\'Matter\\', \\'paragraphs\\': [{\\'qas\\': [{\\'...\\n442 rows × 2 columns\\nOpen and pr eprocess (add special t okens) dataset per BER T format\\nLoad pr eprocess (add special t okens) the SQU AD 2.0  dataset per BER T format. Get a minimum 20 QnA pairs. \\ue313\\n#\\xa0Function\\xa0to\\xa0load\\xa0SQuAD2\\xa0data\\xa0and\\xa0add\\xa0special\\xa0tok ens\\xa0[CLS]\\xa0and\\xa0[SEP]\\ndef\\xa0load_squad_data (file_path ,\\xa0num_samples =20):\\n\\xa0\\xa0\\xa0\\xa0with\\xa0open(file_path ,\\xa0\\'r\\',\\xa0encoding= \\'utf-8\\')\\xa0as\\xa0f:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0squad_data\\xa0=\\xa0json.load (f)\\n\\xa0\\xa0\\xa0\\xa0data\\xa0=\\xa0 []\\n\\xa0\\xa0\\xa0\\xa0for\\xa0i\\xa0in\\xa0range(min(len(squad_data [\\'data\\']),\\xa0num_samples )):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0paragraphs\\xa0=\\xa0squad_data [\\'data\\'][i][\\'paragraphs\\' ]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for\\xa0paragraph\\xa0 in\\xa0paragraphs :\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0context\\xa0=\\xa0paragraph [\\'context\\' ]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0qas\\xa0=\\xa0paragraph [\\'qas\\']\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for\\xa0qa\\xa0in\\xa0qas:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0question\\xa0=\\xa0qa [\\'question\\' ]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0answers\\xa0\\xa0=\\xa0qa [\\'answers\\' ]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\xa0answers :\\xa0\\xa0#\\xa0Check\\xa0if\\xa0answers\\xa0are\\xa0available\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0answer_text\\xa0=\\xa0answers [0][\\'text\\']\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Tokenize\\xa0question\\xa0and\\xa0answer\\xa0text\\xa0.encode\\xa0automa tically\\xa0adds\\xa0[CLS]\\xa0and\\xa0[SEP]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0tokenized_question\\xa0=\\xa0tokenizer .encode(question ,\\xa0add_special_tokens= True)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0tokenized_answer\\xa0=\\xa0tokenizer.e ncode(answer_text ,\\xa0add_special_tokens= True)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0data.append ((tokenized_question ,\\xa0tokenized_answer ))\\n\\xa0\\xa0\\xa0\\xa0return\\xa0data\\nsquad_data\\xa0=\\xa0load_squad_data (squad_data_path ,\\xa0num_samples= 20)\\nDispla y the \\x00rst f ew question-answer pairs \\ue313\\nfor\\xa0i\\xa0in\\xa0range(8):\\n\\xa0\\xa0\\xa0\\xa0print(\"Question:\" ,\\xa0tokenizer.decode (squad_data [i][0]))\\n\\xa0\\xa0\\xa0\\xa0print(\"Answer:\" ,\\xa0tokenizer.decode (squad_data [i][1]))\\n\\xa0\\xa0\\xa0\\xa0print()\\nQuestion: [CLS] when did beyonce start becoming popular? [SEP]\\nAnswer: [CLS] in the late 1990s [SEP]\\nQuestion: [CLS] what areas did beyonce compete in when she was growing up? [SEP]\\nAnswer: [CLS] singing and dancing [SEP]\\nQuestion: [CLS] when did beyonce leave destiny\\'s child and become a solo singer? [SEP]\\nAnswer: [CLS] 2003 [SEP]\\nQuestion: [CLS] in what city and state did beyonce grow up? [SEP]\\nAnswer: [CLS] houston, texas [SEP]\\nQuestion: [CLS] in which decade did beyonce become famous? [SEP]\\nAnswer: [CLS] late 1990s [SEP]5/5/24, 12:53 PM Jorge_Gosalvez_HW10_BERT.ipynb - Colab\\nhttps://colab.research.google.com/drive/1K2vYT2LBTWdhmSNVh7uEFu0xars8PcSJ#printMode=true 2/7', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW10_BERT.pdf', 'page': 1}),\n",
              " Document(page_content='Question: [CLS] in what r & b group was she the lead singer? [SEP]\\nAnswer: [CLS] destiny\\'s child [SEP]\\nQuestion: [CLS] what album made her a worldwide known artist? [SEP]\\nAnswer: [CLS] dangerously in love [SEP]\\nQuestion: [CLS] who managed the destiny\\'s child group? [SEP]\\nAnswer: [CLS] mathew knowles [SEP]\\nDataloader t okenizes text aut omatically \\ue313\\nTrain and e valuate the BER T QnA model \\ue313\\nimport\\xa0torch\\nfrom\\xa0transformers\\xa0 import\\xa0BertTokenizer ,\\xa0BertForQuestionAnswering ,\\xa0AdamW,\\xa0get_linear_schedule_with_warmup\\nfrom\\xa0torch.utils.data\\xa0 import\\xa0DataLoader ,\\xa0RandomSampler ,\\xa0SequentialSampler\\nfrom\\xa0transformers\\xa0 import\\xa0squad_convert_examples_to_features\\nfrom\\xa0transformers.data.processors.squad\\xa0 import\\xa0SquadV2Processor\\nfrom\\xa0tqdm\\xa0import\\xa0tqdm\\nimport\\xa0numpy\\xa0as\\xa0np\\nimport\\xa0random\\n#\\xa0Set\\xa0random\\xa0seeds\\xa0for\\xa0reproducibility\\nseed_val\\xa0=\\xa0 42\\nrandom.seed (seed_val )\\nnp.random.seed (seed_val )\\ntorch.manual_seed (seed_val )\\ntorch.cuda.manual_seed_all (seed_val )\\nSome weights of BertForQuestionAnswering were not initialized from the model che\\nYou should probably TRAIN this model on a down-stream task to be able to use it model.safetensors:\\u2007100% \\u2007440M/440M\\u2007[00:01<00:00,\\u2007329MB/s]#\\xa0Load\\xa0pre-trained\\xa0BERT\\xa0model\\xa0and\\xa0tokenizer\\nmodel_name\\xa0=\\xa0 \\'bert-base-uncased\\'\\ntokenizer\\xa0\\xa0=\\xa0BertTokenizer.from_pretrained (model_name )\\nmodel\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0=\\xa0BertForQuestionAnswering.from_pretrai ned(model_name )\\n#\\xa0Define\\xa0fine-tuning\\xa0parameters\\nbatch_size\\xa0=\\xa0 16\\nepochs\\xa0=\\xa0 3\\nlearning_rate\\xa0=\\xa0 3e-5\\nadam_epsilon\\xa0=\\xa0 1e-8\\n#\\xa0Load\\xa0SQuAD\\xa02.0\\xa0data\\xa0and\\xa0limit\\xa0to\\xa020\\xa0QnA\\xa0pairs\\nprocessor\\xa0=\\xa0SquadV2Processor ()\\ntrain_examples\\xa0=\\xa0processor.get_train_examples (\\'./\\'\\xa0,\\xa0filename= \"train-v2.0.json\" )[:20]\\nval_examples\\xa0\\xa0\\xa0=\\xa0processor.get_train_examples (\\'./\\'\\xa0,\\xa0filename= \"train-v2.0.json\" )[21:29]\\ntest_dataset\\xa0\\xa0\\xa0=\\xa0processor.get_train_examples (\\'./\\'\\xa0,\\xa0filename= \"dev-v2.0.json\" )[:5]\\n100%|██████████| 442/442 [00:42<00:00, 10.32it/s]\\n100%|██████████| 442/442 [00:42<00:00, 10.39it/s]\\n100%|██████████| 35/35 [00:03<00:00,  8.94it/s]5/5/24, 12:53 PM Jorge_Gosalvez_HW10_BERT.ipynb - Colab\\nhttps://colab.research.google.com/drive/1K2vYT2LBTWdhmSNVh7uEFu0xars8PcSJ#printMode=true 3/7', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW10_BERT.pdf', 'page': 2}),\n",
              " Document(page_content='#\\xa0Convert\\xa0examples\\xa0to\\xa0features\\ntrain_features ,\\xa0train_dataset\\xa0=\\xa0squad_convert_examples_to_feature s(\\n\\xa0\\xa0\\xa0\\xa0examples=train_examples ,\\n\\xa0\\xa0\\xa0\\xa0tokenizer=tokenizer ,\\n\\xa0\\xa0\\xa0\\xa0max_seq_length= 384,\\xa0\\xa0#\\xa0BERT\\xa0max\\xa0input\\xa0sequence\\xa0length\\xa0-\\xa02\\xa0for\\xa0[CLS]\\xa0and \\xa0[SEP]\\n\\xa0\\xa0\\xa0\\xa0doc_stride= 128,\\n\\xa0\\xa0\\xa0\\xa0max_query_length= 64,\\n\\xa0\\xa0\\xa0\\xa0is_training= True,\\n\\xa0\\xa0\\xa0\\xa0return_dataset= \"pt\"\\n)\\nval_features ,\\xa0val_dataset\\xa0=\\xa0squad_convert_examples_to_features (\\n\\xa0\\xa0\\xa0\\xa0examples=val_examples ,\\n\\xa0\\xa0\\xa0\\xa0tokenizer=tokenizer ,\\n\\xa0\\xa0\\xa0\\xa0max_seq_length= 384,\\xa0\\xa0#\\xa0BERT\\xa0max\\xa0input\\xa0length\\n\\xa0\\xa0\\xa0\\xa0doc_stride= 128,\\n\\xa0\\xa0\\xa0\\xa0max_query_length= 64,\\n\\xa0\\xa0\\xa0\\xa0is_training= True,\\n\\xa0\\xa0\\xa0\\xa0return_dataset= \"pt\"\\n)\\n/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with m\\n  self.pid = os.fork()\\nconvert squad examples to features: 100%|██████████| 20/20 [00:00<00:00, 123.34it/s]\\nadd example index and unique id: 100%|██████████| 20/20 [00:00<00:00, 150874.24it/s]\\nconvert squad examples to features: 100%|██████████| 8/8 [00:00<00:00, 91.22it/s]\\nadd example index and unique id: 100%|██████████| 8/8 [00:00<00:00, 101989.16it/s]\\n#\\xa0Create\\xa0data\\xa0loader\\xa0[CLS]\\xa0&\\xa0[SEP]\\n#\\xa0NOTE:\\xa0For\\xa0each\\xa0batch\\xa0of\\xa0the\\xa0dataloader;\\xa0input\\xa0ID s,\\xa0attention\\xa0masks,\\xa0and\\xa0token\\xa0type\\xa0IDs\\xa0are\\xa0created ,\\xa0including\\xa0[CLS]\\xa0and\\xa0[SEP]\\n#train_sampler\\xa0\\xa0\\xa0\\xa0=\\xa0RandomSampler(train_dataset)\\n#train_dataloader\\xa0=\\xa0DataLoader(train_dataset,\\xa0samp ler=train_sampler,\\xa0batch_size=batch_size)\\ntrain_dataloader\\xa0=\\xa0DataLoader (train_dataset ,\\xa0sampler=RandomSampler (train_dataset ),\\xa0batch_size=batch_size )\\nval_dataloader\\xa0\\xa0\\xa0=\\xa0DataLoader (val_dataset ,\\xa0sampler=RandomSampler (val_dataset ),\\xa0batch_size=batch_size )\\ntest_dataloader\\xa0\\xa0=\\xa0DataLoader (test_dataset ,\\xa0sampler=RandomSampler (test_dataset ),\\xa0batch_size=batch_size )\\n#\\xa0Prepare\\xa0optimizer\\xa0and\\xa0scheduler\\noptimizer\\xa0\\xa0\\xa0=\\xa0torch.optim.AdamW (model.parameters (),\\xa0lr=learning_rate ,\\xa0eps=adam_epsilon )\\ntotal_steps\\xa0=\\xa0 len(train_dataloader )\\xa0*\\xa0epochs\\nscheduler\\xa0\\xa0\\xa0=\\xa0get_linear_schedule_with_warmup (optimizer ,\\xa0num_warmup_steps= 0,\\xa0num_training_steps=total_steps )5/5/24, 12:53 PM Jorge_Gosalvez_HW10_BERT.ipynb - Colab\\nhttps://colab.research.google.com/drive/1K2vYT2LBTWdhmSNVh7uEFu0xars8PcSJ#printMode=true 4/7', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW10_BERT.pdf', 'page': 3}),\n",
              " Document(page_content='#\\xa0Train\\xa0the\\xa0model\\ndevice\\xa0=\\xa0torch.device (\"cuda\"\\xa0if\\xa0torch.cuda.is_available ()\\xa0else\\xa0\"cpu\")\\nmodel.to (device)\\n\"\"\"print(\"Training...\")\\nfor\\xa0epoch\\xa0in\\xa0range(epochs):\\n\\xa0\\xa0\\xa0\\xa0model.train()\\n\\xa0\\xa0\\xa0\\xa0total_loss\\xa0=\\xa00\\n\\xa0\\xa0\\xa0\\xa0for\\xa0step,\\xa0batch\\xa0in\\xa0enumerate(train_dataloader) :\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0batch\\xa0=\\xa0tuple(t.to(device)\\xa0for\\xa0t\\xa0in\\xa0batch)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0inputs\\xa0=\\xa0{\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\"input_ids\":\\xa0batch[0],\\xa0#\\xa0Input\\xa0token\\xa0I Ds\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\"attention_mask\":\\xa0batch[1],\\xa0#\\xa0Attentio n\\xa0mask\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\"token_type_ids\":\\xa0batch[2],\\xa0#\\xa0Segment\\xa0 token\\xa0IDs\\xa0(for\\xa0sequence\\xa0pairs)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\"start_positions\":\\xa0batch[3],#\\xa0Start\\xa0po sition\\xa0of\\xa0the\\xa0answer\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\"end_positions\":\\xa0batch[4]\\xa0#\\xa0End\\xa0positi on\\xa0of\\xa0the\\xa0answer\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0}\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0optimizer.zero_grad()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0outputs\\xa0=\\xa0model(**inputs)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0loss\\xa0=\\xa0outputs.loss\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0total_loss\\xa0+=\\xa0loss.item()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0loss.backward()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0torch.nn.utils.clip_grad_norm_(model.param eters(),\\xa01.0)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0optimizer.step()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0scheduler.step()\\n\\xa0\\xa0\\xa0\\xa0avg_train_loss\\xa0=\\xa0total_loss\\xa0/\\xa0len(train_datalo ader)\\n\\xa0\\xa0\\xa0\\xa0print(f\\'\\\\nEpoch\\xa0{epoch\\xa0+\\xa01}:\\')\\n\\xa0\\xa0\\xa0\\xa0print(f\\'Average\\xa0training\\xa0loss:\\xa0{avg_train_loss :.4f}\\')\\nprint(\"\\\\nTraining\\xa0complete!\")\"\"\"\\nprint(\"Training...\" )\\nn_epochs\\xa0=\\xa0 0\\ntrain_losses ,\\xa0valid_losses\\xa0=\\xa0 [],\\xa0[]\\nfor\\xa0epoch\\xa0in\\xa0range(epochs):\\n\\xa0\\xa0\\xa0\\xa0model.train ()\\n\\xa0\\xa0\\xa0\\xa0total_train_loss\\xa0=\\xa0 0\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Training\\xa0loop\\n\\xa0\\xa0\\xa0\\xa0for\\xa0step,\\xa0batch\\xa0in\\xa0enumerate (train_dataloader ):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0batch\\xa0=\\xa0 tuple(t.to(device)\\xa0for\\xa0t\\xa0in\\xa0batch)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0inputs\\xa0=\\xa0 {\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \"input_ids\" :\\xa0batch[0],\\xa0#\\xa0Input\\xa0token\\xa0IDs\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \"attention_mask\" :\\xa0batch[1],\\xa0#\\xa0Attention\\xa0mask\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \"token_type_ids\" :\\xa0batch[2],\\xa0#\\xa0Segment\\xa0token\\xa0IDs\\xa0(for\\xa0sequence\\xa0pairs)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \"start_positions\" :\\xa0batch[3],#\\xa0Start\\xa0position\\xa0of\\xa0the\\xa0answer\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \"end_positions\" :\\xa0batch[4]\\xa0#\\xa0End\\xa0position\\xa0of\\xa0the\\xa0answer\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 }\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0optimizer.zero_grad ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0outputs\\xa0=\\xa0model (**inputs )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0loss\\xa0=\\xa0outputs.loss\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0total_train_loss\\xa0+=\\xa0loss.item ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0loss.backward ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0torch.nn.utils.clip_grad_norm_ (model.parameters (),\\xa01.0)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0optimizer.step ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0scheduler.step ()\\n\\xa0\\xa0\\xa0\\xa0avg_train_loss\\xa0=\\xa0total_train_loss\\xa0/\\xa0 len(train_dataloader )\\n\\xa0\\xa0\\xa0\\xa0train_losses.append (avg_train_loss )\\n\\xa0\\xa0\\xa0\\xa0print(f\\'Epoch\\xa0{epoch\\xa0+\\xa0 1}:\\')\\n\\xa0\\xa0\\xa0\\xa0print(f\\'\\xa0\\xa0Average\\xa0training\\xa0loss:\\xa0 {avg_train_loss :.4f}\\')\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Validation\\xa0loop\\n\\xa0\\xa0\\xa0\\xa0model. eval()\\n\\xa0\\xa0\\xa0\\xa0total_val_loss\\xa0=\\xa0 0\\n\\xa0\\xa0\\xa0\\xa0for\\xa0step,\\xa0batch\\xa0in\\xa0enumerate (val_dataloader ):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0batch\\xa0=\\xa0 tuple(t.to(device)\\xa0for\\xa0t\\xa0in\\xa0batch)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0inputs\\xa0=\\xa0 {\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \"input_ids\" :\\xa0batch[0],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \"attention_mask\" :\\xa0batch[1],5/5/24, 12:53 PM Jorge_Gosalvez_HW10_BERT.ipynb - Colab\\nhttps://colab.research.google.com/drive/1K2vYT2LBTWdhmSNVh7uEFu0xars8PcSJ#printMode=true 5/7', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW10_BERT.pdf', 'page': 4}),\n",
              " Document(page_content='\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \"token_type_ids\" :\\xa0batch[2],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \"start_positions\" :\\xa0batch[3],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \"end_positions\" :\\xa0batch[4]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 }\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 with\\xa0torch.no_grad ():\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0outputs\\xa0=\\xa0model (**inputs )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0loss\\xa0=\\xa0outputs.loss\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0total_val_loss\\xa0+=\\xa0loss.item ()\\n\\xa0\\xa0\\xa0\\xa0avg_val_loss\\xa0=\\xa0total_val_loss\\xa0/\\xa0 len(val_dataloader )\\n\\xa0\\xa0\\xa0\\xa0valid_losses.append (avg_val_loss )\\n\\xa0\\xa0\\xa0\\xa0print(f\\'\\xa0\\xa0Average\\xa0validation\\xa0loss:\\xa0 {avg_val_loss :.4f}\\')\\n\\xa0\\xa0\\xa0\\xa0n_epochs\\xa0+=\\xa0 1\\nprint(\"Training\\xa0complete!\" )\\nTraining...\\nEpoch 1:\\n  Average training loss: 5.8237\\n  Average validation loss: 5.5687\\nEpoch 2:\\n  Average training loss: 5.1328\\n  Average validation loss: 5.4107\\nEpoch 3:\\n  Average training loss: 4.9340\\n  Average validation loss: 5.3430\\nTraining complete!\\nimport\\xa0matplotlib.pyplot\\xa0 as\\xa0plt\\nimport\\xa0numpy\\xa0as\\xa0np\\nepoch_ticks\\xa0=\\xa0 range(1,\\xa0n_epochs\\xa0+\\xa0 1)\\xa0\\xa0#\\xa0Assuming\\xa0n_epochs\\xa0is\\xa0defined\\xa0somewhere\\nplt.plot (epoch_ticks ,\\xa0train_losses )\\nplt.plot (epoch_ticks ,\\xa0valid_losses )\\nplt.legend ([\\'Train\\xa0Loss\\' ,\\xa0\\'Valid\\xa0Loss\\' ])\\nplt.xlabel (\\'Epochs\\' )\\nplt.ylabel (\\'Loss\\')\\nplt.title (\\'Training\\xa0and\\xa0Validation\\xa0Losses\\' )\\nplt.show ()\\nContinual tr aining loss implies potential o ver\\x00tting\\nPerform an Inf erence and show the pr edicted vs gr ound truth answers. \\ue3135/5/24, 12:53 PM Jorge_Gosalvez_HW10_BERT.ipynb - Colab\\nhttps://colab.research.google.com/drive/1K2vYT2LBTWdhmSNVh7uEFu0xars8PcSJ#printMode=true 6/7', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW10_BERT.pdf', 'page': 5}),\n",
              " Document(page_content='for\\xa0example\\xa0 in\\xa0test_dataset :\\n\\xa0\\xa0\\xa0\\xa0inputs\\xa0=\\xa0tokenizer.encode_plus (example.question_text ,\\xa0example.context_text ,\\xa0add_special_tokens= True,\\xa0return_tensors= \"pt\")\\n\\xa0\\xa0\\xa0\\xa0input_ids\\xa0=\\xa0inputs [\\'input_ids\\' ].tolist()\\n\\xa0\\xa0\\xa0\\xa0decoded_text\\xa0=\\xa0tokenizer.decode (input_ids [0])\\n\\xa0\\xa0\\xa0\\xa0print(\"DECODED\\xa0TEXT:\" ,\\xa0decoded_text )\\n\\xa0\\xa0\\xa0\\xa0print(\"\\\\nQUESTION:\" ,\\xa0example.question_text )\\n\\xa0\\xa0\\xa0\\xa0print(\"CONTEXT:\" ,\\xa0example.context_text )\\n\\xa0\\xa0\\xa0\\xa0print(\"ANSWER:\" ,\\xa0example.answer_text )\\n\\xa0\\xa0\\xa0\\xa0print()\\n\\xa0\\xa0\\xa0\\xa0print()\\nDECODED TEXT: [CLS] in what country is normandy located? [SEP] the normans ( norman : nourmands ; french : normands ; latin \\nQUESTION: In what country is Normandy located?\\nCONTEXT: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuri\\nANSWER: France\\nDECODED TEXT: [CLS] when were the normans in normandy? [SEP] the normans ( norman : nourmands ; french : normands ; latin : \\nQUESTION: When were the Normans in Normandy?\\nCONTEXT: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuri\\nANSWER: 10th and 11th centuries\\nDECODED TEXT: [CLS] from which countries did the norse originate? [SEP] the normans ( norman : nourmands ; french : normands\\nQUESTION: From which countries did the Norse originate?\\nCONTEXT: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuri\\nANSWER: Denmark, Iceland and Norway\\nDECODED TEXT: [CLS] who was the norse leader? [SEP] the normans ( norman : nourmands ; french : normands ; latin : normanni \\nQUESTION: Who was the Norse leader?\\nCONTEXT: TheNormans (Norman: Nourmands; French: Normands; Latin:Normanni) werethepeoplewhointhe10thand11thcenturi5/5/24, 12:53 PM Jorge_Gosalvez_HW10_BERT.ipynb - Colab\\nhttps://colab.research.google.com/drive/1K2vYT2LBTWdhmSNVh7uEFu0xars8PcSJ#printMode=true 7/7', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW10_BERT.pdf', 'page': 6}),\n",
              " Document(page_content='Homework 05: GANSJSU MSDS 255 DL, Spring 2024\\ue313\\nGit: https:/ /github.com/jr gosalv ez/data255_DL\\nPart 1 - GAN\\ue313\\nStep 1. Load F ashion MNIST  & Replace Dataset in Demo \\ue313\\n#\\xa0prerequisites\\nimport\\xa0torch\\nimport\\xa0torch.nn\\xa0 as\\xa0nn\\nimport\\xa0torch.nn.functional\\xa0 as\\xa0F\\nimport\\xa0torch.optim\\xa0 as\\xa0optim\\nfrom\\xa0torchvision\\xa0 import\\xa0datasets ,\\xa0transforms\\nfrom\\xa0torch.autograd\\xa0 import\\xa0Variable\\nfrom\\xa0torchvision.utils\\xa0 import\\xa0save_image\\nimport\\xa0warnings\\nwarnings.filterwarnings (\"ignore\" )\\n#\\xa0Device\\xa0configuration\\ndevice\\xa0=\\xa0torch.device (\\'cuda\\'\\xa0if\\xa0torch.cuda.is_available ()\\xa0else\\xa0\\'cpu\\')\\n#\\xa0Fashion\\xa0MNIST\\xa0Dataset\\ntransform\\xa0=\\xa0transforms.Compose ([transforms.ToTensor ()])\\n#transform\\xa0=\\xa0transforms.Compose([transforms.ToTens or(),\\n#\\xa0\\xa0transforms.Normalize((0.5,),\\xa0(0.5,))\\n#])\\ntrain_dataset\\xa0=\\xa0datasets.FashionMNIST (root=\\'./mnist_data/\\' ,\\xa0train=True,\\xa0transform=tr\\ntest_dataset\\xa0\\xa0=\\xa0datasets.FashionMNIST (root=\\'./mnist_data/\\' ,\\xa0train=False,\\xa0transform=t\\nimg,\\xa0label\\xa0=\\xa0train_dataset [0]\\nprint(\\'Label:\\xa0\\' ,\\xa0label)\\nprint(img[:,10:15,10:15])\\ntorch.min(img),\\xa0torch.max(img)\\nLabel:  9\\ntensor([[[0.0000, 0.0000, 0.0000, 0.7569, 0.8941],5/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 1/25', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 0}),\n",
              " Document(page_content=\"         [0.0118, 0.0000, 0.0471, 0.8588, 0.8627],\\n         [0.0235, 0.0000, 0.3882, 0.9569, 0.8706],\\n         [0.0000, 0.0000, 0.2157, 0.9255, 0.8941],\\n         [0.0000, 0.0000, 0.9294, 0.8863, 0.8510]]])\\n(tensor(0.), tensor(1.))\\ndef\\xa0denorm(x):\\n\\xa0\\xa0\\xa0\\xa0out\\xa0=\\xa0 (x+1)\\xa0/\\xa02\\n\\xa0\\xa0\\xa0\\xa0return\\xa0out.clamp (0,1)\\nLabel: 9\\nimport\\xa0matplotlib.pyplot\\xa0 as\\xa0plt\\n%matplotlib\\xa0 inline\\nimg_norm\\xa0=\\xa0denorm (img)\\nplt.imshow (img_norm [0],\\xa0cmap='gray')\\nprint('Label:' ,\\xa0label)\\nDownload in batches\\ue313\\nbatch_size\\xa0=\\xa0 100\\n#\\xa0Data\\xa0Loader\\xa0(Input\\xa0Pipeline)\\ntrain_loader\\xa0=\\xa0torch.utils.data.DataLoader (dataset=train_dataset ,\\xa0batch_size=batch_s\\ntest_loader\\xa0\\xa0=\\xa0torch.utils.data.DataLoader (dataset=test_dataset ,\\xa0batch_size=batch_si5/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 2/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 1}),\n",
              " Document(page_content=\"first batch\\ntorch.Size([100, 1, 28, 28])\\ntensor([4, 6, 6, 1, 4, 9, 2, 8, 0, 3, 7, 8, 4, 2, 7, 1, 0, 9, 9, 5, 8, 9, 4, 0,\\n        2, 3, 4, 7, 7, 3, 8, 1, 1, 9, 3, 7, 9, 8, 5, 4, 3, 7, 4, 3, 7, 8, 3, 0,\\n        4, 3, 1, 7, 9, 4, 0, 2, 6, 9, 4, 9, 3, 7, 7, 9, 8, 4, 8, 4, 5, 8, 2, 1,\\n        7, 8, 5, 4, 5, 9, 9, 5, 1, 9, 2, 9, 7, 6, 3, 9, 5, 9, 2, 7, 5, 6, 3, 1,\\n        3, 8, 1, 9])\\nfor\\xa0img_batch ,\\xa0label_batch\\xa0 in\\xa0train_loader :\\n\\xa0\\xa0\\xa0\\xa0print('first\\xa0batch' )\\n\\xa0\\xa0\\xa0\\xa0print(img_batch.shape )\\n\\xa0\\xa0\\xa0\\xa0plt.imshow (img_batch [0][0],\\xa0cmap='gray')\\n\\xa0\\xa0\\xa0\\xa0print(label_batch )\\n\\xa0\\xa0\\xa0\\xa0break\\nStep 2. Train GAN t o produce images \\ue313\\nimage_size\\xa0\\xa0=\\xa0 784\\nhidden_size\\xa0=\\xa0 256\\nlatent_size\\xa0=\\xa0 645/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 3/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 2}),\n",
              " Document(page_content=\"D\\xa0=\\xa0nn.Sequential (\\n\\xa0\\xa0\\xa0\\xa0nn.Linear (image_size ,\\xa0hidden_size ),\\n\\xa0\\xa0\\xa0\\xa0nn.LeakyReLU (0.2),\\n\\xa0\\xa0\\xa0\\xa0nn.Linear (hidden_size ,\\xa0hidden_size ),\\n\\xa0\\xa0\\xa0\\xa0nn.LeakyReLU (0.2),\\n\\xa0\\xa0\\xa0\\xa0nn.Linear (hidden_size ,\\xa01),\\n\\xa0\\xa0\\xa0\\xa0nn.Sigmoid ())\\nG\\xa0=\\xa0nn.Sequential (\\n\\xa0\\xa0\\xa0\\xa0nn.Linear (latent_size ,\\xa0hidden_size ),\\n\\xa0\\xa0\\xa0\\xa0nn.ReLU (),\\n\\xa0\\xa0\\xa0\\xa0nn.Linear (hidden_size ,\\xa0hidden_size ),\\n\\xa0\\xa0\\xa0\\xa0nn.ReLU (),\\n\\xa0\\xa0\\xa0\\xa0nn.Linear (hidden_size ,\\xa0image_size ),\\n\\xa0\\xa0\\xa0\\xa0nn.Tanh ())\\nInstantiate the gener ator and gener age r andom fak e images \\ue313\\ny\\xa0=\\xa0G(torch.randn (2,\\xa0latent_size ))\\ngen_imgs\\xa0=\\xa0denorm (y.reshape ((-1,\\xa028,28)).detach())\\nplt.imshow (gen_imgs [0],\\xa0cmap='gray');5/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 4/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 3}),\n",
              " Document(page_content='Build the network and mo ve gener ator and discriminat or to the de vice \\ue313\\nD.to(device);\\nG.to(device);\\nG\\nSequential(\\n  (0): Linear(in_features=64, out_features=256, bias=True)\\n  (1): ReLU()\\n  (2): Linear(in_features=256, out_features=256, bias=True)\\n  (3): ReLU()\\n  (4): Linear(in_features=256, out_features=784, bias=True)\\n  (5): Tanh()\\n)\\nD\\nSequential(\\n  (0): Linear(in_features=784, out_features=256, bias=True)\\n  (1): LeakyReLU(negative_slope=0.2)\\n  (2): Linear(in_features=256, out_features=256, bias=True)\\n  (3): LeakyReLU(negative_slope=0.2)\\n  (4): Linear(in_features=256, out_features=1, bias=True)\\n  (5): Sigmoid()\\n)\\n#\\xa0loss\\ncriterion\\xa0=\\xa0nn.BCELoss ()\\n#\\xa0optimizer\\nlr\\xa0=\\xa00.0002\\nG_optimizer\\xa0=\\xa0optim.Adam (G.parameters (),\\xa0lr\\xa0=\\xa0lr )\\nD_optimizer\\xa0=\\xa0optim.Adam (D.parameters (),\\xa0lr\\xa0=\\xa0lr )5/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 5/25', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 4}),\n",
              " Document(page_content='def\\xa0reset_grad ():\\n\\xa0\\xa0\\xa0\\xa0D_optimizer.zero_grad ()\\n\\xa0\\xa0\\xa0\\xa0G_optimizer.zero_grad ()\\ndef\\xa0D_train(images):\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Create\\xa0the\\xa0labels\\xa0which\\xa0are\\xa0later\\xa0used\\xa0as\\xa0input\\xa0 for\\xa0the\\xa0BCE\\xa0loss\\n\\xa0\\xa0\\xa0\\xa0real_labels\\xa0=\\xa0torch.ones (batch_size ,\\xa01).to(device)\\n\\xa0\\xa0\\xa0\\xa0fake_labels\\xa0=\\xa0torch.zeros (batch_size ,\\xa01).to(device)\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Loss\\xa0for\\xa0real\\xa0images\\n\\xa0\\xa0\\xa0\\xa0outputs\\xa0=\\xa0D (images)\\n\\xa0\\xa0\\xa0\\xa0d_loss_real\\xa0=\\xa0criterion (outputs,\\xa0real_labels )\\n\\xa0\\xa0\\xa0\\xa0real_score\\xa0=\\xa0outputs\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Loss\\xa0for\\xa0fake\\xa0images\\n\\xa0\\xa0\\xa0\\xa0z\\xa0=\\xa0torch.randn (batch_size ,\\xa0latent_size ).to(device)\\n\\xa0\\xa0\\xa0\\xa0fake_images\\xa0=\\xa0G (z)\\n\\xa0\\xa0\\xa0\\xa0outputs\\xa0=\\xa0D (fake_images )\\n\\xa0\\xa0\\xa0\\xa0d_loss_fake\\xa0=\\xa0criterion (outputs,\\xa0fake_labels )\\n\\xa0\\xa0\\xa0\\xa0fake_score\\xa0=\\xa0outputs\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Combine\\xa0losses\\n\\xa0\\xa0\\xa0\\xa0d_loss\\xa0=\\xa0d_loss_real\\xa0+\\xa0d_loss_fake\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Reset\\xa0gradients\\n\\xa0\\xa0\\xa0\\xa0reset_grad ()\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Compute\\xa0gradients\\n\\xa0\\xa0\\xa0\\xa0d_loss.backward ()\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Adjust\\xa0the\\xa0parameters\\xa0using\\xa0backprop\\n\\xa0\\xa0\\xa0\\xa0D_optimizer.step ()\\n\\xa0\\xa0\\xa0\\xa0return\\xa0d_loss,\\xa0real_score ,\\xa0fake_score\\ndef\\xa0G_train():\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Generate\\xa0fake\\xa0images\\xa0and\\xa0calculate\\xa0loss\\n\\xa0\\xa0\\xa0\\xa0z\\xa0=\\xa0torch.randn (batch_size ,\\xa0latent_size ).to(device)\\n\\xa0\\xa0\\xa0\\xa0fake_images\\xa0=\\xa0G (z)\\n\\xa0\\xa0\\xa0\\xa0labels\\xa0=\\xa0torch.ones (batch_size ,\\xa01).to(device)\\n\\xa0\\xa0\\xa0\\xa0g_loss\\xa0=\\xa0criterion (D(fake_images ),\\xa0labels)\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Backprop\\xa0and\\xa0optimize\\n\\xa0\\xa0\\xa0\\xa0reset_grad ()\\n\\xa0\\xa0\\xa0\\xa0g_loss.backward ()\\n\\xa0\\xa0\\xa0\\xa0G_optimizer.step ()\\n\\xa0\\xa0\\xa0\\xa0return\\xa0g_loss,\\xa0fake_images\\nTrain the model\\ue3135/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 6/25', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 5}),\n",
              " Document(page_content=\"Create and sa ve intermediate outputs fr om the gener ator for visual inspection later \\ue313\\nimport\\xa0os\\nsample_dir\\xa0=\\xa0 'sample_data'\\nif\\xa0not\\xa0os.path.exists (sample_dir ):\\n\\xa0\\xa0\\xa0\\xa0os.makedirs (sample_dir )\\nBatch of r eal tr aining images \\ue313\\nfrom\\xa0IPython.display\\xa0 import\\xa0Image\\n#\\xa0Save\\xa0some\\xa0real\\xa0images\\nfor\\xa0images,\\xa0_\\xa0in\\xa0train_loader :\\n\\xa0\\xa0\\xa0\\xa0images\\xa0=\\xa0images.reshape (images.size (0),\\xa01,\\xa028,\\xa028)\\n\\xa0\\xa0\\xa0\\xa0save_image (denorm(images),\\xa0os.path.join (sample_dir ,\\xa0'real_images.png' ),\\xa0nrow=10)\\n\\xa0\\xa0\\xa0\\xa0break\\nImage(os.path.join (sample_dir ,\\xa0'real_images.png' ))\\nHelper function t o sa ve batch of gener ated images at end of ea epoch t o see\\nevolution o ver time\\ue3135/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 7/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 6}),\n",
              " Document(page_content=\"Saving fake_images-0000.png\\nsample_vectors\\xa0=\\xa0torch.randn (batch_size ,\\xa0latent_size ).to(device)\\ndef\\xa0save_fake_images (index):\\n\\xa0\\xa0\\xa0\\xa0fake_images\\xa0=\\xa0G (sample_vectors )\\n\\xa0\\xa0\\xa0\\xa0fake_images\\xa0=\\xa0fake_images.reshape (fake_images.size (0),\\xa01,\\xa028,\\xa028)\\n\\xa0\\xa0\\xa0\\xa0fake_fname\\xa0=\\xa0 'fake_images-{0:0=4d}.png' .format(index)\\n\\xa0\\xa0\\xa0\\xa0print('Saving' ,\\xa0fake_fname )\\n\\xa0\\xa0\\xa0\\xa0save_image (denorm(fake_images ),\\xa0os.path.join (sample_dir ,\\xa0fake_fname ),\\xa0nrow=10)\\n#\\xa0Before\\xa0training\\nsave_fake_images (0)\\nImage(os.path.join (sample_dir ,\\xa0'fake_images-0000.png' ))\\nTrain Model\\ue3135/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 8/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 7}),\n",
              " Document(page_content=\"%%time\\nnum_epochs\\xa0=\\xa0 300\\ntotal_step\\xa0=\\xa0 len(train_loader )\\nd_losses ,\\xa0g_losses ,\\xa0real_scores ,\\xa0fake_scores\\xa0=\\xa0 [],\\xa0[],\\xa0[],\\xa0[]\\nfor\\xa0epoch\\xa0in\\xa0range(num_epochs ):\\n\\xa0\\xa0\\xa0\\xa0for\\xa0i,\\xa0(images,\\xa0_)\\xa0in\\xa0enumerate (train_loader ):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Load\\xa0a\\xa0batch\\xa0&\\xa0transform\\xa0to\\xa0vectors\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0images\\xa0=\\xa0images.reshape (batch_size ,\\xa0-1).to(device)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Train\\xa0the\\xa0discriminator\\xa0and\\xa0generator\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0d_loss ,\\xa0real_score ,\\xa0fake_score\\xa0=\\xa0D_train (images)\\xa0#\\xa0discriminator\\xa0ingests\\xa0rea\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0g_loss ,\\xa0fake_images\\xa0=\\xa0G_train ()\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0generator\\xa0creates\\xa0images\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Inspect\\xa0the\\xa0losses\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\xa0(i+1)\\xa0%\\xa0200\\xa0==\\xa00:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0d_losses.append (d_loss.item ())\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0g_losses.append (g_loss.item ())\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0real_scores.append (real_score.mean ().item())\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0fake_scores.append (fake_score.mean ().item())\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 print('Epoch\\xa0[{}/{}],\\xa0Step\\xa0[{}/{}],\\xa0d_loss:\\xa0{:.4f},\\xa0g_lo ss:\\xa0{:.4f},\\xa0D(x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0. format(epoch,\\xa0num_epochs ,\\xa0i+1,\\xa0total_step ,\\xa0d_loss.item (),\\xa0g_loss.\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0real_score.mean ().item(),\\xa0fake_score.mean ().item()))\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Sample\\xa0and\\xa0save\\xa0images\\n\\xa0\\xa0\\xa0\\xa0save_fake_images (epoch+1)\\nEpoch [0/300], Step [200/600], d_loss: 0.0899, g_loss: 4.3959, D(x): 0.96, D(G(z\\nEpoch [0/300], Step [400/600], d_loss: 0.2149, g_loss: 5.8951, D(x): 0.90, D(G(z\\nEpoch [0/300], Step [600/600], d_loss: 0.1693, g_loss: 4.2356, D(x): 0.92, D(G(z\\nSaving fake_images-0001.png\\nEpoch [1/300], Step [200/600], d_loss: 0.2010, g_loss: 3.4154, D(x): 0.92, D(G(z\\nEpoch [1/300], Step [400/600], d_loss: 0.4308, g_loss: 2.9578, D(x): 0.85, D(G(z\\nEpoch [1/300], Step [600/600], d_loss: 0.3155, g_loss: 3.7022, D(x): 0.89, D(G(z\\nSaving fake_images-0002.png\\nEpoch [2/300], Step [200/600], d_loss: 0.8614, g_loss: 1.9833, D(x): 0.73, D(G(z\\nEpoch [2/300], Step [400/600], d_loss: 0.8980, g_loss: 3.6903, D(x): 0.67, D(G(z\\nEpoch [2/300], Step [600/600], d_loss: 0.1770, g_loss: 3.2625, D(x): 0.93, D(G(z\\nSaving fake_images-0003.png\\nEpoch [3/300], Step [200/600], d_loss: 0.4630, g_loss: 3.2015, D(x): 0.87, D(G(z\\nEpoch [3/300], Step [400/600], d_loss: 0.3005, g_loss: 3.7343, D(x): 0.94, D(G(z\\nEpoch [3/300], Step [600/600], d_loss: 0.2058, g_loss: 3.7432, D(x): 0.92, D(G(z\\nSaving fake_images-0004.png\\nEpoch [4/300], Step [200/600], d_loss: 0.7285, g_loss: 3.8536, D(x): 0.72, D(G(z\\nEpoch [4/300], Step [400/600], d_loss: 0.5269, g_loss: 3.9735, D(x): 0.84, D(G(z\\nEpoch [4/300], Step [600/600], d_loss: 0.5230, g_loss: 3.1785, D(x): 0.79, D(G(z\\nSaving fake_images-0005.png\\nEpoch [5/300], Step [200/600], d_loss: 0.4228, g_loss: 3.7234, D(x): 0.84, D(G(z\\nEpoch [5/300], Step [400/600], d_loss: 0.3359, g_loss: 3.7561, D(x): 0.88, D(G(z\\nEpoch [5/300], Step [600/600], d_loss: 0.3470, g_loss: 2.8234, D(x): 0.88, D(G(z\\nSaving fake_images-0006.png\\nEpoch [6/300], Step [200/600], d_loss: 0.1924, g_loss: 3.9460, D(x): 0.92, D(G(z5/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 9/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 8}),\n",
              " Document(page_content=\"Epoch [6/300], Step [400/600], d_loss: 0.3272, g_loss: 4.2030, D(x): 0.87, D(G(z\\nEpoch [6/300], Step [600/600], d_loss: 1.0456, g_loss: 3.3530, D(x): 0.71, D(G(z\\nSaving fake_images-0007.png\\nEpoch [7/300], Step [200/600], d_loss: 0.2176, g_loss: 3.3354, D(x): 0.94, D(G(z\\nEpoch [7/300], Step [400/600], d_loss: 0.4283, g_loss: 3.0464, D(x): 0.83, D(G(z\\nEpoch [7/300], Step [600/600], d_loss: 0.5261, g_loss: 3.3285, D(x): 0.87, D(G(z\\nSaving fake_images-0008.png\\nEpoch [8/300], Step [200/600], d_loss: 0.3052, g_loss: 4.3178, D(x): 0.90, D(G(z\\nEpoch [8/300], Step [400/600], d_loss: 0.4412, g_loss: 4.2961, D(x): 0.85, D(G(z\\nEpoch [8/300], Step [600/600], d_loss: 0.3429, g_loss: 3.0841, D(x): 0.90, D(G(z\\nSaving fake_images-0009.png\\nEpoch [9/300], Step [200/600], d_loss: 0.3318, g_loss: 3.2190, D(x): 0.91, D(G(z\\nEpoch [9/300], Step [400/600], d_loss: 0.3514, g_loss: 3.9454, D(x): 0.90, D(G(z\\nEpoch [9/300], Step [600/600], d_loss: 0.4781, g_loss: 4.5425, D(x): 0.79, D(G(z\\nSaving fake_images-0010.png\\nEpoch [10/300], Step [200/600], d_loss: 0.6771, g_loss: 4.1380, D(x): 0.77, D(G(\\nEpoch [10/300], Step [400/600], d_loss: 0.2611, g_loss: 3.6003, D(x): 0.91, D(G(\\nEpoch [10/300], Step [600/600], d_loss: 0.3264, g_loss: 2.7118, D(x): 0.90, D(G(\\nSaving fake_images-0011.png\\nEpoch [11/300], Step [200/600], d_loss: 0.3563, g_loss: 3.4108, D(x): 0.90, D(G(\\nEpoch [11/300], Step [400/600], d_loss: 0.4287, g_loss: 3.2734, D(x): 0.90, D(G(\\nEpoch [11/300], Step [600/600], d_loss: 0.5221, g_loss: 5.1330, D(x): 0.78, D(G(\\nSaving fake_images-0012.png\\nEpoch [12/300], Step [200/600], d_loss: 0.7478, g_loss: 3.0103, D(x): 0.84, D(G(\\nEpoch [12/300], Step [400/600], d_loss: 0.3199, g_loss: 3.5907, D(x): 0.91, D(G(\\nEpoch [12/300], Step [600/600], d_loss: 0.5902, g_loss: 4.5121, D(x): 0.77, D(G(\\nSaving fake_images-0013.png\\nEpoch [13/300], Step [200/600], d_loss: 0.6364, g_loss: 2.8478, D(x): 0.77, D(G(\\nEpoch [13/300], Step [400/600], d_loss: 0.5108, g_loss: 2.3321, D(x): 0.86, D(G(\\nEpoch [13/300], Step [600/600], d_loss: 1.1140, g_loss: 3.8358, D(x): 0.65, D(G(\\nSaving fake_images-0014.png\\nEpoch [14/300], Step [200/600], d_loss: 0.5655, g_loss: 2.7899, D(x): 0.78, D(G(\\nEh[14/300] S[400/600] dl04094 l30086D()086D(G(\\nimport\\xa0matplotlib.pyplot\\xa0 as\\xa0plt\\nplt.plot (d_losses ,\\xa0'-')\\nplt.plot (g_losses ,\\xa0'-')\\nplt.xlabel ('epoch')\\nplt.ylabel ('loss')\\nplt.legend (['Discriminator' ,\\xa0'Generator' ])\\nplt.title ('Losses' );5/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 10/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 9}),\n",
              " Document(page_content=\"plt.plot (real_scores ,\\xa0'-')\\nplt.plot (fake_scores ,\\xa0'-')\\nplt.xlabel ('epoch')\\nplt.ylabel ('score')\\nplt.legend (['Real\\xa0Score' ,\\xa0'Fake\\xa0score' ])\\nplt.title ('Scores' );5/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 11/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 10}),\n",
              " Document(page_content='Evaluate P erformance of GAN model (\\x00rst 300 epochs) \\ue3135/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 12/25', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 11}),\n",
              " Document(page_content=\"def\\xa0evaluate_discriminator (discriminator ,\\xa0test_loader ):\\n\\xa0\\xa0\\xa0\\xa0discriminator. eval()\\xa0\\xa0#\\xa0Set\\xa0the\\xa0model\\xa0to\\xa0evaluation\\xa0mode\\n\\xa0\\xa0\\xa0\\xa0correct_real\\xa0=\\xa0 0\\n\\xa0\\xa0\\xa0\\xa0correct_fake\\xa0=\\xa0 0\\n\\xa0\\xa0\\xa0\\xa0total\\xa0=\\xa0 0\\n\\xa0\\xa0\\xa0\\xa0with\\xa0torch.no_grad ():\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for\\xa0images,\\xa0_\\xa0in\\xa0test_loader :\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0images\\xa0=\\xa0images.reshape (-1,\\xa0image_size ).to(device)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0outputs\\xa0=\\xa0discriminator (images)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0predicted_labels\\xa0=\\xa0 (outputs\\xa0>=\\xa0 0.5).float()\\xa0\\xa0#\\xa0Threshold\\xa0at\\xa00.5\\xa0for\\xa0bina\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Compute\\xa0accuracy\\xa0for\\xa0real\\xa0and\\xa0fake\\xa0images\\xa0separa tely\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0correct_real\\xa0+=\\xa0 (predicted_labels [:len(images)//2]\\xa0==\\xa01).sum().item()\\xa0\\xa0#\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0correct_fake\\xa0+=\\xa0 (predicted_labels [len(images)//2:]\\xa0==\\xa00).sum().item()\\xa0\\xa0#\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0total\\xa0+=\\xa0 len(images)\\n\\xa0\\xa0\\xa0\\xa0accuracy_real\\xa0=\\xa0 100\\xa0*\\xa0correct_real\\xa0/\\xa0 (total\\xa0//\\xa0 2)\\n\\xa0\\xa0\\xa0\\xa0accuracy_fake\\xa0=\\xa0 100\\xa0*\\xa0correct_fake\\xa0/\\xa0 (total\\xa0//\\xa0 2)\\n\\xa0\\xa0\\xa0\\xa0accuracy_total\\xa0=\\xa0 100\\xa0*\\xa0(correct_real\\xa0+\\xa0correct_fake )\\xa0/\\xa0total\\n\\xa0\\xa0\\xa0\\xa0print('Accuracy\\xa0on\\xa0real\\xa0images:\\xa0{:.2f}%' .format(accuracy_real ))\\n\\xa0\\xa0\\xa0\\xa0print('Accuracy\\xa0on\\xa0fake\\xa0images:\\xa0{:.2f}%' .format(accuracy_fake ))\\n\\xa0\\xa0\\xa0\\xa0print('Overall\\xa0accuracy:\\xa0{:.2f}%' .format(accuracy_total ))\\n\\xa0\\xa0\\xa0\\xa0return\\xa0accuracy_real ,\\xa0accuracy_fake ,\\xa0accuracy_total\\n#\\xa0Usage\\nGAN_real ,\\xa0GAN_fake ,\\xa0GAN_overall\\xa0=\\xa0evaluate_discriminator (D,\\xa0test_loader )\\nAccuracy on real images: 65.14%\\nAccuracy on fake images: 36.76%\\nOverall accuracy: 50.95%\\nStep 3. Show 3+ samples cr eated b y GAN. Shar e lessons learned. \\ue313\\nShow fak e images after 10th, 50th, 100th and 300th epochs of tr aining \\ue3135/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 13/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 12}),\n",
              " Document(page_content=\"import\\xa0matplotlib.image\\xa0 as\\xa0mpimg\\n#\\xa0List\\xa0of\\xa0image\\xa0paths\\nimage_paths\\xa0=\\xa0 [\\n\\xa0\\xa0\\xa0\\xa0'./sample_data/fake_images-0010.png' ,\\n\\xa0\\xa0\\xa0\\xa0'./sample_data/fake_images-0050.png' ,\\n\\xa0\\xa0\\xa0\\xa0'./sample_data/fake_images-0100.png' ,\\n\\xa0\\xa0\\xa0\\xa0'./sample_data/fake_images-0300.png'\\n]\\n#\\xa0Plot\\xa0images\\xa0side\\xa0by\\xa0side\\nplt.figure (figsize= (28,\\xa028))\\nfor\\xa0i,\\xa0path\\xa0in\\xa0enumerate (image_paths ):\\n\\xa0\\xa0\\xa0\\xa0plt.subplot (1,\\xa0len(image_paths ),\\xa0i\\xa0+\\xa01)\\n\\xa0\\xa0\\xa0\\xa0img\\xa0=\\xa0mpimg.imread (path)\\n\\xa0\\xa0\\xa0\\xa0plt.imshow (img)\\n\\xa0\\xa0\\xa0\\xa0plt.axis ('off')\\nplt.show ()\\nGener ation of image samples called individually yields usable r esults.\\nGener ator impr oves with pr actice.\\nGets close t o real images.\\nGener ate a time series video. NO TE: Images must be r esized to match the\\ndimensions of the video fr ames.\\ue3135/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 14/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 13}),\n",
              " Document(page_content=\"gans_training.mp4import\\xa0cv2\\nimport\\xa0os\\nfrom\\xa0IPython.display\\xa0 import\\xa0FileLink\\nvid_fname\\xa0\\xa0=\\xa0 'gans_training.mp4'\\nfiles\\xa0=\\xa0 [os.path.join (sample_dir ,\\xa0f)\\xa0for\\xa0f\\xa0in\\xa0os.listdir (sample_dir )\\xa0if\\xa0'fake_images\\nfiles.sort ()\\nout\\xa0=\\xa0cv2.VideoWriter (vid_fname ,cv2.VideoWriter_fourcc (*'MP4V'),\\xa08,\\xa0(302,302))\\n[out.write (cv2.imread (fname))\\xa0for\\xa0fname\\xa0in\\xa0files]\\nout.release ()\\nFileLink ('gans_training.mp4' )\\nStep 4. Sa ve model weights with checkpoints \\ue313\\ntorch.save (G.state_dict (),\\xa0'generator_weights_checkpoints.ckpt' )\\ntorch.save (D.state_dict (),\\xa0'discriminator_weights_checkpoints.ckpt' )\\nStep 5. Load model\\ue313\\n#\\xa0Load\\xa0checkpoint\\xa0models\\xa0for\\xa0generator\\xa0and\\xa0discrim inator\\ngenerator_checkpoint_path\\xa0\\xa0\\xa0\\xa0\\xa0=\\xa0 'generator_weights_checkpoints.ckpt'\\ndiscriminator_checkpoint_path\\xa0=\\xa0 'discriminator_weights_checkpoints.ckpt'\\n#\\xa0Load\\xa0weights\\xa0into\\xa0the\\xa0models\\nG.load_state_dict (torch.load (generator_checkpoint_path ))\\nD.load_state_dict (torch.load (discriminator_checkpoint_path ))\\n<All keys matched successfully>\\n#\\xa0Set\\xa0models\\xa0to\\xa0evaluation\\xa0mode\\nG.eval()\\nSequential(\\n  (0): Linear(in_features=64, out_features=256, bias=True)\\n  (1): ReLU()\\n  (2): Linear(in_features=256, out_features=256, bias=True)\\n  (3): ReLU()\\n  (4): Linear(in_features=256, out_features=784, bias=True)\\n  (5): Tanh()\\n)\\nD.eval()5/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 15/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 14}),\n",
              " Document(page_content=\"Sequential(\\n  (0): Linear(in_features=784, out_features=256, bias=True)\\n  (1): LeakyReLU(negative_slope=0.2)\\n  (2): Linear(in_features=256, out_features=256, bias=True)\\n  (3): LeakyReLU(negative_slope=0.2)\\n  (4): Linear(in_features=256, out_features=1, bias=True)\\n  (5): Sigmoid()\\n)\\nStep 6. Re-tr ain GAN (100 epochs) \\ue313\\nSaving transf_fake_images-0000.png\\ndef\\xa0save_transf_fake_images (index):\\n\\xa0\\xa0\\xa0\\xa0fake_images\\xa0=\\xa0G (sample_vectors )\\n\\xa0\\xa0\\xa0\\xa0fake_images\\xa0=\\xa0fake_images.reshape (fake_images.size (0),\\xa01,\\xa028,\\xa028)\\n\\xa0\\xa0\\xa0\\xa0fake_fname\\xa0=\\xa0 'transf_fake_images-{0:0=4d}.png' .format(index)\\n\\xa0\\xa0\\xa0\\xa0print('Saving' ,\\xa0fake_fname )\\n\\xa0\\xa0\\xa0\\xa0save_image (denorm(fake_images ),\\xa0os.path.join (sample_dir ,\\xa0fake_fname ),\\xa0nrow=10)\\n#\\xa0Before\\xa0training\\nsave_transf_fake_images (0)\\nImage(os.path.join (sample_dir ,\\xa0'transf_fake_images-0000.png' ))5/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 16/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 15}),\n",
              " Document(page_content=\"%%time\\nnum_epochs\\xa0=\\xa0 100\\ntotal_step\\xa0=\\xa0 len(train_loader )\\nd_losses ,\\xa0g_losses ,\\xa0real_scores ,\\xa0fake_scores\\xa0=\\xa0 [],\\xa0[],\\xa0[],\\xa0[]\\nfor\\xa0epoch\\xa0in\\xa0range(num_epochs ):\\n\\xa0\\xa0\\xa0\\xa0for\\xa0i,\\xa0(images,\\xa0_)\\xa0in\\xa0enumerate (train_loader ):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Load\\xa0a\\xa0batch\\xa0&\\xa0transform\\xa0to\\xa0vectors\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0images\\xa0=\\xa0images.reshape (batch_size ,\\xa0-1).to(device)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Train\\xa0the\\xa0discriminator\\xa0and\\xa0generator\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0d_loss ,\\xa0real_score ,\\xa0fake_score\\xa0=\\xa0D_train (images)\\xa0#\\xa0discriminator\\xa0ingests\\xa0rea\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0g_loss ,\\xa0fake_images\\xa0=\\xa0G_train ()\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0generator\\xa0creates\\xa0images\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Inspect\\xa0the\\xa0losses\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\xa0(i+1)\\xa0%\\xa0200\\xa0==\\xa00:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0d_losses.append (d_loss.item ())\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0g_losses.append (g_loss.item ())\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0real_scores.append (real_score.mean ().item())\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0fake_scores.append (fake_score.mean ().item())\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 print('Epoch\\xa0[{}/{}],\\xa0Step\\xa0[{}/{}],\\xa0d_loss:\\xa0{:.4f},\\xa0g_lo ss:\\xa0{:.4f},\\xa0D(x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0. format(epoch,\\xa0num_epochs ,\\xa0i+1,\\xa0total_step ,\\xa0d_loss.item (),\\xa0g_loss.\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0real_score.mean ().item(),\\xa0fake_score.mean ().item()))\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Sample\\xa0and\\xa0save\\xa0images\\n\\xa0\\xa0\\xa0\\xa0save_transf_fake_images (epoch+1)\\nEpoch [0/100], Step [200/600], d_loss: 0.8590, g_loss: 1.7160, D(x): 0.68, D(G(z\\nEpoch [0/100], Step [400/600], d_loss: 0.8768, g_loss: 1.4377, D(x): 0.77, D(G(z\\nEpoch [0/100], Step [600/600], d_loss: 0.9805, g_loss: 1.4882, D(x): 0.64, D(G(z\\nSaving transf_fake_images-0001.png\\nEpoch [1/100], Step [200/600], d_loss: 0.9452, g_loss: 1.2370, D(x): 0.73, D(G(z\\nEpoch [1/100], Step [400/600], d_loss: 0.9002, g_loss: 1.7187, D(x): 0.63, D(G(z\\nEpoch [1/100], Step [600/600], d_loss: 0.7542, g_loss: 1.4563, D(x): 0.79, D(G(z\\nSaving transf_fake_images-0002.png\\nEpoch [2/100], Step [200/600], d_loss: 1.0868, g_loss: 1.3415, D(x): 0.72, D(G(z\\nEpoch [2/100], Step [400/600], d_loss: 0.9292, g_loss: 1.7201, D(x): 0.69, D(G(z\\nEpoch [2/100], Step [600/600], d_loss: 0.8240, g_loss: 1.6917, D(x): 0.73, D(G(z\\nSaving transf_fake_images-0003.png\\nEpoch [3/100], Step [200/600], d_loss: 0.9090, g_loss: 1.5493, D(x): 0.69, D(G(z\\nEpoch [3/100], Step [400/600], d_loss: 1.0896, g_loss: 1.7720, D(x): 0.64, D(G(z\\nEpoch [3/100], Step [600/600], d_loss: 0.7860, g_loss: 1.6753, D(x): 0.73, D(G(z\\nSaving transf_fake_images-0004.png\\nEpoch [4/100], Step [200/600], d_loss: 0.8495, g_loss: 1.4874, D(x): 0.74, D(G(z\\nEpoch [4/100], Step [400/600], d_loss: 1.1451, g_loss: 1.6324, D(x): 0.71, D(G(z\\nEpoch [4/100], Step [600/600], d_loss: 1.0033, g_loss: 1.4655, D(x): 0.69, D(G(z\\nSaving transf_fake_images-0005.png\\nEpoch [5/100], Step [200/600], d_loss: 0.9414, g_loss: 1.7263, D(x): 0.63, D(G(z\\nEpoch [5/100], Step [400/600], d_loss: 0.9534, g_loss: 1.6562, D(x): 0.66, D(G(z\\nEpoch [5/100], Step [600/600], d_loss: 0.7930, g_loss: 1.6494, D(x): 0.80, D(G(z\\nSaving transf_fake_images-0006.png\\nEpoch [6/100], Step [200/600], d_loss: 0.8373, g_loss: 1.4738, D(x): 0.68, D(G(z5/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 17/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 16}),\n",
              " Document(page_content=\"Epoch [6/100], Step [400/600], d_loss: 0.9604, g_loss: 1.4123, D(x): 0.69, D(G(z\\nEpoch [6/100], Step [600/600], d_loss: 0.7964, g_loss: 2.2006, D(x): 0.70, D(G(z\\nSaving transf_fake_images-0007.png\\nEpoch [7/100], Step [200/600], d_loss: 0.8545, g_loss: 1.6371, D(x): 0.75, D(G(z\\nEpoch [7/100], Step [400/600], d_loss: 1.1041, g_loss: 1.2763, D(x): 0.66, D(G(z\\nEpoch [7/100], Step [600/600], d_loss: 1.0368, g_loss: 1.4407, D(x): 0.70, D(G(z\\nSaving transf_fake_images-0008.png\\nEpoch [8/100], Step [200/600], d_loss: 0.9560, g_loss: 1.3123, D(x): 0.71, D(G(z\\nEpoch [8/100], Step [400/600], d_loss: 0.8008, g_loss: 2.0138, D(x): 0.68, D(G(z\\nEpoch [8/100], Step [600/600], d_loss: 1.0397, g_loss: 1.1578, D(x): 0.66, D(G(z\\nSaving transf_fake_images-0009.png\\nEpoch [9/100], Step [200/600], d_loss: 0.8883, g_loss: 1.8156, D(x): 0.69, D(G(z\\nEpoch [9/100], Step [400/600], d_loss: 0.9205, g_loss: 1.4928, D(x): 0.70, D(G(z\\nEpoch [9/100], Step [600/600], d_loss: 0.8847, g_loss: 1.5528, D(x): 0.66, D(G(z\\nSaving transf_fake_images-0010.png\\nEpoch [10/100], Step [200/600], d_loss: 1.0144, g_loss: 1.5923, D(x): 0.66, D(G(\\nEpoch [10/100], Step [400/600], d_loss: 0.9205, g_loss: 1.4008, D(x): 0.73, D(G(\\nEpoch [10/100], Step [600/600], d_loss: 0.8550, g_loss: 1.5597, D(x): 0.76, D(G(\\nSaving transf_fake_images-0011.png\\nEpoch [11/100], Step [200/600], d_loss: 1.0416, g_loss: 1.6035, D(x): 0.62, D(G(\\nEpoch [11/100], Step [400/600], d_loss: 1.0777, g_loss: 1.4967, D(x): 0.64, D(G(\\nEpoch [11/100], Step [600/600], d_loss: 0.9896, g_loss: 1.6013, D(x): 0.73, D(G(\\nSaving transf_fake_images-0012.png\\nEpoch [12/100], Step [200/600], d_loss: 0.8026, g_loss: 1.3385, D(x): 0.76, D(G(\\nEpoch [12/100], Step [400/600], d_loss: 0.7611, g_loss: 1.9218, D(x): 0.78, D(G(\\nEpoch [12/100], Step [600/600], d_loss: 1.0435, g_loss: 1.4299, D(x): 0.67, D(G(\\nSaving transf_fake_images-0013.png\\nEpoch [13/100], Step [200/600], d_loss: 0.8468, g_loss: 1.7631, D(x): 0.73, D(G(\\nEpoch [13/100], Step [400/600], d_loss: 0.8582, g_loss: 1.5410, D(x): 0.67, D(G(\\nEpoch [13/100], Step [600/600], d_loss: 0.9419, g_loss: 1.8377, D(x): 0.75, D(G(\\nSaving transf_fake_images-0014.png\\nEpoch [14/100], Step [200/600], d_loss: 0.9698, g_loss: 1.5276, D(x): 0.70, D(G(\\nEh[14/100] S[400/600] dl10664 l17683D()070D(G(\\nimport\\xa0matplotlib.pyplot\\xa0 as\\xa0plt\\nplt.plot (d_losses ,\\xa0'-')\\nplt.plot (g_losses ,\\xa0'-')\\nplt.xlabel ('epoch')\\nplt.ylabel ('loss')\\nplt.legend (['Discriminator' ,\\xa0'Generator' ])\\nplt.title ('Losses' );5/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 18/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 17}),\n",
              " Document(page_content=\"plt.plot (real_scores ,\\xa0'-')\\nplt.plot (fake_scores ,\\xa0'-')\\nplt.xlabel ('epoch')\\nplt.ylabel ('score')\\nplt.legend (['Real\\xa0Score' ,\\xa0'Fake\\xa0score' ])\\nplt.title ('Scores' );5/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 19/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 18}),\n",
              " Document(page_content=\"import\\xa0matplotlib.image\\xa0 as\\xa0mpimg\\n#\\xa0List\\xa0of\\xa0image\\xa0paths\\nimage_paths\\xa0=\\xa0 [\\n\\xa0\\xa0\\xa0\\xa0'./sample_data/transf_fake_images-0010.png' ,\\n\\xa0\\xa0\\xa0\\xa0'./sample_data/transf_fake_images-0050.png' ,\\n\\xa0\\xa0\\xa0\\xa0'./sample_data/transf_fake_images-0100.png'\\n]\\n#\\xa0Plot\\xa0images\\xa0side\\xa0by\\xa0side\\nplt.figure (figsize= (28,\\xa028))\\nfor\\xa0i,\\xa0path\\xa0in\\xa0enumerate (image_paths ):\\n\\xa0\\xa0\\xa0\\xa0plt.subplot (1,\\xa0len(image_paths ),\\xa0i\\xa0+\\xa01)\\n\\xa0\\xa0\\xa0\\xa0img\\xa0=\\xa0mpimg.imread (path)\\n\\xa0\\xa0\\xa0\\xa0plt.imshow (img)\\n\\xa0\\xa0\\xa0\\xa0plt.axis ('off')\\nplt.show ()5/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 20/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 19}),\n",
              " Document(page_content='Evaluate P erformance of tr anfer learned model \\ue3135/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 21/25', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 20}),\n",
              " Document(page_content=\"def\\xa0evaluate_discriminator (discriminator ,\\xa0test_loader ):\\n\\xa0\\xa0\\xa0\\xa0discriminator. eval()\\xa0\\xa0#\\xa0Set\\xa0the\\xa0model\\xa0to\\xa0evaluation\\xa0mode\\n\\xa0\\xa0\\xa0\\xa0correct_real\\xa0=\\xa0 0\\n\\xa0\\xa0\\xa0\\xa0correct_fake\\xa0=\\xa0 0\\n\\xa0\\xa0\\xa0\\xa0total\\xa0=\\xa0 0\\n\\xa0\\xa0\\xa0\\xa0with\\xa0torch.no_grad ():\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for\\xa0images,\\xa0_\\xa0in\\xa0test_loader :\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0images\\xa0=\\xa0images.reshape (-1,\\xa0image_size ).to(device)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0outputs\\xa0=\\xa0discriminator (images)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0predicted_labels\\xa0=\\xa0 (outputs\\xa0>=\\xa0 0.5).float()\\xa0\\xa0#\\xa0Threshold\\xa0at\\xa00.5\\xa0for\\xa0bina\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Compute\\xa0accuracy\\xa0for\\xa0real\\xa0and\\xa0fake\\xa0images\\xa0separa tely\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0correct_real\\xa0+=\\xa0 (predicted_labels [:len(images)//2]\\xa0==\\xa01).sum().item()\\xa0\\xa0#\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0correct_fake\\xa0+=\\xa0 (predicted_labels [len(images)//2:]\\xa0==\\xa00).sum().item()\\xa0\\xa0#\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0total\\xa0+=\\xa0 len(images)\\n\\xa0\\xa0\\xa0\\xa0accuracy_real\\xa0=\\xa0 100\\xa0*\\xa0correct_real\\xa0/\\xa0 (total\\xa0//\\xa0 2)\\n\\xa0\\xa0\\xa0\\xa0accuracy_fake\\xa0=\\xa0 100\\xa0*\\xa0correct_fake\\xa0/\\xa0 (total\\xa0//\\xa0 2)\\n\\xa0\\xa0\\xa0\\xa0accuracy_total\\xa0=\\xa0 100\\xa0*\\xa0(correct_real\\xa0+\\xa0correct_fake )\\xa0/\\xa0total\\n\\xa0\\xa0\\xa0\\xa0print('Accuracy\\xa0on\\xa0real\\xa0images:\\xa0{:.2f}%' .format(accuracy_real ))\\n\\xa0\\xa0\\xa0\\xa0print('Accuracy\\xa0on\\xa0fake\\xa0images:\\xa0{:.2f}%' .format(accuracy_fake ))\\n\\xa0\\xa0\\xa0\\xa0print('Overall\\xa0accuracy:\\xa0{:.2f}%' .format(accuracy_total ))\\n\\xa0\\xa0\\xa0\\xa0return\\xa0accuracy_real ,\\xa0accuracy_fake ,\\xa0accuracy_total\\n#\\xa0Usage\\ntrans_real ,\\xa0trans_fake ,\\xa0trans_overall\\xa0=\\xa0evaluate_discriminator (D,\\xa0test_loader )\\nAccuracy on real images: 69.58%\\nAccuracy on fake images: 29.14%\\nOverall accuracy: 49.36%\\nStep 7. Sa ve w/o checkpoints \\ue313\\ntorch.save (G.state_dict (),\\xa0'generator_weights_wo_checkpoints.pth' )\\ntorch.save (D.state_dict (),\\xa0'discriminator_weights_wo_checkpoints.pth' )\\nStep 8. Load w/o checkpoints\\ue3135/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 22/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 21}),\n",
              " Document(page_content=\"#\\xa0Load\\xa0checkpoint\\xa0models\\xa0for\\xa0generator\\xa0and\\xa0discrim inator\\ngenerator_checkpoint_path\\xa0\\xa0\\xa0\\xa0\\xa0=\\xa0 'generator_weights_wo_checkpoints.pth'\\ndiscriminator_checkpoint_path\\xa0=\\xa0 'discriminator_weights_wo_checkpoints.pth'\\n#\\xa0Load\\xa0weights\\xa0into\\xa0the\\xa0models\\nG.load_state_dict (torch.load (generator_checkpoint_path ))\\nD.load_state_dict (torch.load (discriminator_checkpoint_path ))\\n<All keys matched successfully>\\nG\\nSequential(\\n  (0): Linear(in_features=64, out_features=256, bias=True)\\n  (1): ReLU()\\n  (2): Linear(in_features=256, out_features=256, bias=True)\\n  (3): ReLU()\\n  (4): Linear(in_features=256, out_features=784, bias=True)\\n  (5): Tanh()\\n)\\nD\\nSequential(\\n  (0): Linear(in_features=784, out_features=256, bias=True)\\n  (1): LeakyReLU(negative_slope=0.2)\\n  (2): Linear(in_features=256, out_features=256, bias=True)\\n  (3): LeakyReLU(negative_slope=0.2)\\n  (4): Linear(in_features=256, out_features=1, bias=True)\\n  (5): Sigmoid()\\n)\\nPart 2 - LSGAN\\ue313\\nRepeat the steps 1-6 with Least Squar e GAN and compar e it with GAN r esults\\nUnsuppor ted Cell Type. Double-Click t o inspect/edit the content.\\nReinitializ e the model \\ue313\\nG.to(device);\\nD.to(device);5/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 23/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 22}),\n",
              " Document(page_content=\"#\\xa0For\\xa0LSGAN\\ncriterion\\xa0=\\xa0nn.MSELoss ()\\nSaving fake_LSGAN_images-0000.png\\ndef\\xa0save_fake_LSGAN_images (index):\\n\\xa0\\xa0\\xa0\\xa0fake_images\\xa0=\\xa0G (sample_vectors )\\n\\xa0\\xa0\\xa0\\xa0fake_images\\xa0=\\xa0fake_images.reshape (fake_images.size (0),\\xa01,\\xa028,\\xa028)\\n\\xa0\\xa0\\xa0\\xa0fake_fname\\xa0=\\xa0 'fake_LSGAN_images-{0:0=4d}.png' .format(index)\\n\\xa0\\xa0\\xa0\\xa0print('Saving' ,\\xa0fake_fname )\\n\\xa0\\xa0\\xa0\\xa0save_image (denorm(fake_images ),\\xa0os.path.join (sample_dir ,\\xa0fake_fname ),\\xa0nrow=10)\\n#\\xa0Before\\xa0training\\nsave_fake_LSGAN_images (0)\\nImage(os.path.join (sample_dir ,\\xa0'fake_LSGAN_images-0000.png' ))5/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 24/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 23}),\n",
              " Document(page_content=\"%%time\\nnum_epochs\\xa0=\\xa0 300\\ntotal_step\\xa0=\\xa0 len(train_loader )\\nd_losses ,\\xa0g_losses ,\\xa0real_scores ,\\xa0fake_scores\\xa0=\\xa0 [],\\xa0[],\\xa0[],\\xa0[]\\nfor\\xa0epoch\\xa0in\\xa0range(num_epochs ):\\n\\xa0\\xa0\\xa0\\xa0for\\xa0i,\\xa0(images,\\xa0_)\\xa0in\\xa0enumerate (train_loader ):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Load\\xa0a\\xa0batch\\xa0&\\xa0transform\\xa0to\\xa0vectors\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0images\\xa0=\\xa0images.reshape (batch_size ,\\xa0-1).to(device)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Train\\xa0the\\xa0discriminator\\xa0and\\xa0generator\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0d_loss ,\\xa0real_score ,\\xa0fake_score\\xa0=\\xa0D_train (images)\\xa0#\\xa0discriminator\\xa0ingests\\xa0rea\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0g_loss ,\\xa0fake_images\\xa0=\\xa0G_train ()\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0generator\\xa0creates\\xa0images\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Inspect\\xa0the\\xa0losses\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\xa0(i+1)\\xa0%\\xa0200\\xa0==\\xa00:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0d_losses.append (d_loss.item ())\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0g_losses.append (g_loss.item ())\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0real_scores.append (real_score.mean ().item())\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0fake_scores.append (fake_score.mean ().item())\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 print('Epoch\\xa0[{}/{}],\\xa0Step\\xa0[{}/{}],\\xa0d_loss:\\xa0{:.4f},\\xa0g_lo ss:\\xa0{:.4f},\\xa0D(x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0. format(epoch,\\xa0num_epochs ,\\xa0i+1,\\xa0total_step ,\\xa0d_loss.item (),\\xa0g_loss.\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0real_score.mean ().item(),\\xa0fake_score.mean ().item()))\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Sample\\xa0and\\xa0save\\xa0images\\n\\xa0\\xa0\\xa0\\xa0save_fake_LSGAN_images (epoch+1)\\nEpoch [0/300], Step [200/600], d_loss: 0.2916, g_loss: 0.4707, D(x): 0.67, D(G(z\\nEpoch [0/300], Step [400/600], d_loss: 0.3390, g_loss: 0.4438, D(x): 0.67, D(G(z\\nEpoch [0/300], Step [600/600], d_loss: 0.3594, g_loss: 0.4308, D(x): 0.68, D(G(z\\nSaving fake_LSGAN_images-0001.png\\nEpoch [1/300], Step [200/600], d_loss: 0.3522, g_loss: 0.4785, D(x): 0.60, D(G(z\\nEpoch [1/300], Step [400/600], d_loss: 0.3391, g_loss: 0.4873, D(x): 0.63, D(G(z\\nEpoch[1/300] Step[600/600] dloss:03226gloss:04723D(x):068D(G(z5/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 25/25\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW5_GAN.pdf', 'page': 24}),\n",
              " Document(page_content='Homework 09: Transformer for German t o English Translation P ytorch TutorialSJSU MSDS 255 DL, Spring 2024 - Transformers \\ue313\\nGit: https:/ /github.com/jr gosalv ez/data255_DL\\n%matplotlib\\xa0 inline\\nimport\\xa0warnings\\nwarnings.filterwarnings (\"ignore\" )\\nTrain a tr anslation model fr om scr atch using Transformer .\\nUse t orchtext libr ary to access Multi30k  dataset t o train a German t o Englishtr anslation model.\\nBased on Tensor\\x00ow/K eras Tutorial: https:/ /www .tensor\\x00ow .org/text/tut orials/tr ansformerLanguage Translation with nn.Transformer and t orchtext\\nData Sour cing and Pr ocessing\\ntorchtext libr ary has utilities for cr eating datasets that can be easily iter ated thr ough for the purposes of cr eating a language tr anslation model.\\nIn this example, we show how t o use t orchtext\\' s inbuilt datasets, t okenize a r aw text sentence, build v ocabular y, and numericaliz e tokens int o\\ntensor . We will use Multi30k dataset fr om t orchtext libr ary that yields a pair of sour ce-tar get r aw sentences.\\nTo access t orchtext datasets, please install t orchdata following instructions at https:/ /github.com/p ytorch/data .Data Sour cing and Pr ocessing \\ue313\\nfrom\\xa0torchtext.data.utils\\xa0 import\\xa0get_tokenizer\\nfrom\\xa0torchtext.vocab\\xa0 import\\xa0build_vocab_from_iterator\\nfrom\\xa0torchtext.datasets\\xa0 import\\xa0multi30k ,\\xa0Multi30k\\nfrom\\xa0typing\\xa0 import\\xa0Iterable ,\\xa0List\\n#\\xa0We\\xa0need\\xa0to\\xa0modify\\xa0the\\xa0URLs\\xa0for\\xa0the\\xa0dataset\\xa0since \\xa0the\\xa0links\\xa0to\\xa0the\\xa0original\\xa0dataset\\xa0are\\xa0broken\\n#\\xa0Refer\\xa0to\\xa0https://github.com/pytorch/text/issues/ 1756#issuecomment-1163664163\\xa0for\\xa0more\\xa0info\\nmulti30k.URL [\"train\"]\\xa0=\\xa0\"https://raw.githubusercontent.com/neychev/small_D L_repo/master/datasets/Multi30k/training.tar.gz\"\\nmulti30k.URL [\"valid\"]\\xa0=\\xa0\"https://raw.githubusercontent.com/neychev/small_D L_repo/master/datasets/Multi30k/validation.tar.gz\"\\nSRC_LANGUAGE\\xa0=\\xa0 \\'de\\'\\nTGT_LANGUAGE\\xa0=\\xa0 \\'en\\'\\n#\\xa0Place-holders\\ntoken_transform\\xa0=\\xa0 {}\\nvocab_transform\\xa0=\\xa0 {}\\nInstall t orchdata and spacy , then reset k ernel  (Jup yter) or runtime (Colab) and skip this step  in the next run.\\n!pip\\xa0install\\xa0-U\\xa0torchdata\\n!pip\\xa0install\\xa0-U\\xa0spacy\\n!python\\xa0-m\\xa0spacy\\xa0download\\xa0en_core_web_sm\\n!python\\xa0-m\\xa0spacy\\xa0download\\xa0de_core_news_sm\\nRequirement already satisfied: torchdata in /usr/local/lib/python3.10/dist-packages (0.7.1)\\nRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.0.7)\\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.31.0)\\nRequirement already satisfied: torch>=2 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.2.1+cu121)\\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.13.4)\\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata\\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (1.12)\\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.3)\\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.1.3)\\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2023.6.0)\\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2->torchdata)\\n  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2->torchdata)5/5/24, 12:58 PM Jorge_Gosalvez_255_HW9_trans_de_en_Pytorch.ipynb - Colab\\nhttps://colab.research.google.com/drive/1RIjNTsxbQRFg1ofAdsZ4UrwOC0oyKZHy#printMode=true 1/8', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW9_trans_de_en.pdf', 'page': 0}),\n",
              " Document(page_content='  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2->torchdata)\\n  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\\nCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2->torchdata)\\n  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2->torchdata)\\n  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2->torchdata)\\n  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=2->torchdata)\\n  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2->torchdata)\\n  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2->torchdata)\\n  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\\nCollecting nvidia-nccl-cu12==2.19.3 (from torch>=2->torchdata)\\n  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=2->torchdata)\\n  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\\nRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2.2.0)\\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2->torchdata)\\n  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata\\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.6)\\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (202\\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2->torchdata)\\nRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2->torchdata) (1.\\nInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-c\\nSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cu\\nRequirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.4)\\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\\nRequirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.4)\\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.2)\\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) \\nSkip back t o this install of por tallock er step after r eimpor ting e verything but the t orchdata and spacy\\n!pip\\xa0install\\xa0portalocker>= 2.0.0\\nRestar t the k ernel (Jup yter) or runtime (Colab)\\nimport\\xa0os\\nos.kill(os.getpid (),\\xa09)5/5/24, 12:58 PM Jorge_Gosalvez_255_HW9_trans_de_en_Pytorch.ipynb - Colab\\nhttps://colab.research.google.com/drive/1RIjNTsxbQRFg1ofAdsZ4UrwOC0oyKZHy#printMode=true 2/8', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW9_trans_de_en.pdf', 'page': 1}),\n",
              " Document(page_content='token_transform [SRC_LANGUAGE ]\\xa0=\\xa0get_tokenizer (\\'spacy\\',\\xa0language= \\'de_core_news_sm\\' )\\ntoken_transform [TGT_LANGUAGE ]\\xa0=\\xa0get_tokenizer (\\'spacy\\',\\xa0language= \\'en_core_web_sm\\' )\\n#\\xa0helper\\xa0function\\xa0to\\xa0yield\\xa0list\\xa0of\\xa0tokens\\ndef\\xa0yield_tokens (data_iter :\\xa0Iterable ,\\xa0language :\\xa0str)\\xa0->\\xa0List [str]:\\n\\xa0\\xa0\\xa0\\xa0language_index\\xa0=\\xa0 {SRC_LANGUAGE :\\xa00,\\xa0TGT_LANGUAGE :\\xa01}\\n\\xa0\\xa0\\xa0\\xa0for\\xa0data_sample\\xa0 in\\xa0data_iter :\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 yield\\xa0token_transform [language ](data_sample [language_index [language ]])\\n#\\xa0Define\\xa0special\\xa0symbols\\xa0and\\xa0indices\\nUNK_IDX,\\xa0PAD_IDX ,\\xa0BOS_IDX ,\\xa0EOS_IDX\\xa0=\\xa0 0,\\xa01,\\xa02,\\xa03\\n#\\xa0Make\\xa0sure\\xa0the\\xa0tokens\\xa0are\\xa0in\\xa0order\\xa0of\\xa0their\\xa0indic es\\xa0to\\xa0properly\\xa0insert\\xa0them\\xa0in\\xa0vocab\\nspecial_symbols\\xa0=\\xa0 [\\'<unk>\\',\\xa0\\'<pad>\\',\\xa0\\'<bos>\\',\\xa0\\'<eos>\\']\\n\"\"\"\\nPurpose:\\xa0Defines\\xa0special\\xa0tokens\\xa0used\\xa0in\\xa0the\\xa0vocabu lary\\xa0for\\xa0machine\\xa0learning\\xa0tasks\\xa0with\\xa0text\\xa0data.\\nSpecial\\xa0Tokens:\\n<unk>:\\xa0\"Unknown\"\\xa0token\\xa0(represents\\xa0words\\xa0not\\xa0in\\xa0th e\\xa0vocabulary)\\n<pad>:\\xa0Padding\\xa0token\\xa0(to\\xa0make\\xa0sequences\\xa0the\\xa0same\\xa0l ength)\\n<bos>:\\xa0\"Beginning\\xa0of\\xa0Sequence\"\\n<eos>:\\xa0\"End\\xa0of\\xa0Sequence\"\\n\"\"\"\\nfor\\xa0ln\\xa0in\\xa0[SRC_LANGUAGE ,\\xa0TGT_LANGUAGE ]:\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Training\\xa0data\\xa0Iterator\\n\\xa0\\xa0\\xa0\\xa0train_iter\\xa0=\\xa0Multi30k (split=\\'train\\',\\xa0language_pair= (SRC_LANGUAGE ,\\xa0TGT_LANGUAGE ))\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Create\\xa0torchtext\\'s\\xa0Vocab\\xa0object\\n\\xa0\\xa0\\xa0\\xa0vocab_transform [ln]\\xa0=\\xa0build_vocab_from_iterator (yield_tokens (train_iter ,\\xa0ln),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0min_freq= 1,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0specials=special_symbols ,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0special_first= True)\\n#\\xa0Set\\xa0``UNK_IDX``\\xa0as\\xa0the\\xa0default\\xa0index.\\xa0This\\xa0index \\xa0is\\xa0returned\\xa0when\\xa0the\\xa0token\\xa0is\\xa0not\\xa0found.\\n#\\xa0If\\xa0not\\xa0set,\\xa0it\\xa0throws\\xa0``RuntimeError``\\xa0when\\xa0the\\xa0 queried\\xa0token\\xa0is\\xa0not\\xa0found\\xa0in\\xa0the\\xa0Vocabulary.\\nfor\\xa0ln\\xa0in\\xa0[SRC_LANGUAGE ,\\xa0TGT_LANGUAGE ]:\\n\\xa0\\xa0vocab_transform [ln].set_default_index (UNK_IDX)\\nThe original Transformer diagr am A representation of a 4-la yer Transformer\\nEach of the components in these two diagr ams will be explained as y ou pr ogress thr ough the tut orial.5/5/24, 12:58 PM Jorge_Gosalvez_255_HW9_trans_de_en_Pytorch.ipynb - Colab\\nhttps://colab.research.google.com/drive/1RIjNTsxbQRFg1ofAdsZ4UrwOC0oyKZHy#printMode=true 3/8', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW9_trans_de_en.pdf', 'page': 2}),\n",
              " Document(page_content='Transformer is a Seq2Seq model intr oduced in \"Attention is all y ou need\"  paper for solving machine tr anslation tasks.\\nCreate a Seq2Seq network that uses Transformer . The network consists of thr ee par ts. First par t is the embedding la yer which conv erts tensor\\nof input indices int o corr esponding tensor of input embeddings. These embedding ar e fur ther augmented with positional encodings t o provide\\nposition information of input t okens t o the model. The second par t is the actual Transformer  model. Finally , the output of the Transformer\\nmodel is passed thr ough linear la yer that giv es unnormaliz ed pr obabilities for each t oken in the tar get language.Seq2Seq Network using Transformer \\ue3135/5/24, 12:58 PM Jorge_Gosalvez_255_HW9_trans_de_en_Pytorch.ipynb - Colab\\nhttps://colab.research.google.com/drive/1RIjNTsxbQRFg1ofAdsZ4UrwOC0oyKZHy#printMode=true 4/8', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW9_trans_de_en.pdf', 'page': 3}),\n",
              " Document(page_content=\"from\\xa0torch\\xa0import\\xa0Tensor\\nimport\\xa0torch\\nimport\\xa0torch.nn\\xa0 as\\xa0nn\\nfrom\\xa0torch.nn\\xa0 import\\xa0Transformer\\nimport\\xa0math\\nDEVICE\\xa0=\\xa0torch.device ('cuda'\\xa0if\\xa0torch.cuda.is_available ()\\xa0else\\xa0'cpu')\\n#\\xa0helper\\xa0Module\\xa0that\\xa0adds\\xa0positional\\xa0encoding\\xa0to\\xa0t he\\xa0token\\xa0embedding\\xa0to\\xa0introduce\\xa0a\\xa0notion\\xa0of\\xa0word\\xa0o rder.\\nclass\\xa0PositionalEncoding (nn.Module):\\n\\xa0\\xa0\\xa0\\xa0def\\xa0__init__ (self,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 emb_size :\\xa0int,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 dropout:\\xa0float,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 maxlen:\\xa0int\\xa0=\\xa05000):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0super (PositionalEncoding ,\\xa0self).__init__ ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0den\\xa0=\\xa0torch.exp (-\\xa0torch.arange (0,\\xa0emb_size ,\\xa02)*\\xa0math.log (10000)\\xa0/\\xa0emb_size )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0pos\\xa0=\\xa0torch.arange (0,\\xa0maxlen).reshape (maxlen,\\xa01)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0pos_embedding\\xa0=\\xa0torch.zeros ((maxlen,\\xa0emb_size ))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0pos_embedding [:,\\xa00::2]\\xa0=\\xa0torch.sin (pos\\xa0*\\xa0den )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0pos_embedding [:,\\xa01::2]\\xa0=\\xa0torch.cos (pos\\xa0*\\xa0den )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0pos_embedding\\xa0=\\xa0pos_embedding.unsqueeze (-2)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.dropout\\xa0=\\xa0nn.Dropout (dropout)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.register_buffer ('pos_embedding' ,\\xa0pos_embedding )\\n\\xa0\\xa0\\xa0\\xa0def\\xa0forward(self,\\xa0token_embedding :\\xa0Tensor):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return\\xa0self.dropout (token_embedding\\xa0+\\xa0 self.pos_embedding [:token_embedding.size (0),\\xa0:])\\n#\\xa0helper\\xa0Module\\xa0to\\xa0convert\\xa0tensor\\xa0of\\xa0input\\xa0indices \\xa0into\\xa0corresponding\\xa0tensor\\xa0of\\xa0token\\xa0embeddings\\nclass\\xa0TokenEmbedding (nn.Module):\\n\\xa0\\xa0\\xa0\\xa0def\\xa0__init__ (self,\\xa0vocab_size :\\xa0int,\\xa0emb_size ):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0super (TokenEmbedding ,\\xa0self).__init__ ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.embedding\\xa0=\\xa0nn.Embedding (vocab_size ,\\xa0emb_size )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.emb_size\\xa0=\\xa0emb_size\\n\\xa0\\xa0\\xa0\\xa0def\\xa0forward(self,\\xa0tokens:\\xa0Tensor):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return\\xa0self.embedding (tokens.long ())\\xa0*\\xa0math.sqrt (self.emb_size )\\n#\\xa0Seq2Seq\\xa0Network\\nclass\\xa0Seq2SeqTransformer (nn.Module):\\n\\xa0\\xa0\\xa0\\xa0def\\xa0__init__ (self,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 num_encoder_layers :\\xa0int,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 num_decoder_layers :\\xa0int,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 emb_size :\\xa0int,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 nhead:\\xa0int,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 src_vocab_size :\\xa0int,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 tgt_vocab_size :\\xa0int,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 dim_feedforward :\\xa0int\\xa0=\\xa0512,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 dropout:\\xa0float\\xa0=\\xa00.1):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0super (Seq2SeqTransformer ,\\xa0self).__init__ ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.transformer\\xa0=\\xa0Transformer (d_model=emb_size ,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0nhead=nhead ,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0num_encoder _layers=num_encoder_layers ,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0num_decoder _layers=num_decoder_layers ,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0dim_feedfor ward=dim_feedforward ,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0dropout=dro pout)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.generator\\xa0=\\xa0nn.Linear (emb_size ,\\xa0tgt_vocab_size )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.src_tok_emb\\xa0=\\xa0TokenEmbedding (src_vocab_size ,\\xa0emb_size )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.tgt_tok_emb\\xa0=\\xa0TokenEmbedding (tgt_vocab_size ,\\xa0emb_size )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.positional_encoding\\xa0=\\xa0PositionalEncoding (\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0emb_size ,\\xa0dropout=dropout )\\n\\xa0\\xa0\\xa0\\xa0def\\xa0forward(self,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 src:\\xa0Tensor,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 trg:\\xa0Tensor,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 src_mask :\\xa0Tensor,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 tgt_mask :\\xa0Tensor,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 src_padding_mask :\\xa0Tensor,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 tgt_padding_mask :\\xa0Tensor,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 memory_key_padding_mask :\\xa0Tensor):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0src_emb\\xa0=\\xa0 self.positional_encoding (self.src_tok_emb (src))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0tgt_emb\\xa0=\\xa0 self.positional_encoding (self.tgt_tok_emb (trg))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0outs\\xa0=\\xa0 self.transformer (src_emb,\\xa0tgt_emb ,\\xa0src_mask ,\\xa0tgt_mask ,\\xa0None,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0src_padding_mask ,\\xa0tgt_padding_mask ,\\xa0memory_key_padding_mask )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return\\xa0self.generator (outs)\\n\\xa0\\xa0\\xa0\\xa0def\\xa0encode(self,\\xa0src:\\xa0Tensor,\\xa0src_mask :\\xa0Tensor):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return\\xa0self.transformer.encoder (self.positional_encoding (5/5/24, 12:58 PM Jorge_Gosalvez_255_HW9_trans_de_en_Pytorch.ipynb - Colab\\nhttps://colab.research.google.com/drive/1RIjNTsxbQRFg1ofAdsZ4UrwOC0oyKZHy#printMode=true 5/8\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW9_trans_de_en.pdf', 'page': 4}),\n",
              " Document(page_content=\"\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.src_tok_emb (src)),\\xa0src_mask )\\n\\xa0\\xa0\\xa0\\xa0def\\xa0decode(self,\\xa0tgt:\\xa0Tensor,\\xa0memory:\\xa0Tensor,\\xa0tgt_mask :\\xa0Tensor):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return\\xa0self.transformer.decoder (self.positional_encoding (\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.tgt_tok_emb (tgt)),\\xa0memory,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0tgt_mask )\\nDuring tr aining, use a subsequent wor d mask that will pr event the model fr om looking int o the futur e wor ds when making pr edictions. Use\\nmasks t o hide sour ce and tar get padding t okens.\\ndef\\xa0generate_square_subsequent_mask (sz):\\n\\xa0\\xa0\\xa0\\xa0mask\\xa0=\\xa0 (torch.triu (torch.ones ((sz,\\xa0sz),\\xa0device=DEVICE ))\\xa0==\\xa01).transpose (0,\\xa01)\\n\\xa0\\xa0\\xa0\\xa0mask\\xa0=\\xa0mask. float().masked_fill (mask\\xa0==\\xa0 0,\\xa0float('-inf')).masked_fill (mask\\xa0==\\xa0 1,\\xa0float(0.0))\\n\\xa0\\xa0\\xa0\\xa0return\\xa0mask\\ndef\\xa0create_mask (src,\\xa0tgt):\\n\\xa0\\xa0\\xa0\\xa0src_seq_len\\xa0=\\xa0src.shape [0]\\n\\xa0\\xa0\\xa0\\xa0tgt_seq_len\\xa0=\\xa0tgt.shape [0]\\n\\xa0\\xa0\\xa0\\xa0tgt_mask\\xa0=\\xa0generate_square_subsequent_mask (tgt_seq_len )\\n\\xa0\\xa0\\xa0\\xa0src_mask\\xa0=\\xa0torch.zeros ((src_seq_len ,\\xa0src_seq_len ),device=DEVICE ).type(torch.bool)\\n\\xa0\\xa0\\xa0\\xa0src_padding_mask\\xa0=\\xa0 (src\\xa0==\\xa0PAD_IDX ).transpose (0,\\xa01)\\n\\xa0\\xa0\\xa0\\xa0tgt_padding_mask\\xa0=\\xa0 (tgt\\xa0==\\xa0PAD_IDX ).transpose (0,\\xa01)\\n\\xa0\\xa0\\xa0\\xa0return\\xa0src_mask ,\\xa0tgt_mask ,\\xa0src_padding_mask ,\\xa0tgt_padding_mask\\nDe\\x00ne the par ameters of the model and instantiate the same.\\nDe\\x00ne the loss function as cr oss-entr opy loss and the Adam optimiz er used for tr aining.\\ntorch.manual_seed (0)\\nSRC_VOCAB_SIZE\\xa0=\\xa0 len(vocab_transform [SRC_LANGUAGE ])\\nTGT_VOCAB_SIZE\\xa0=\\xa0 len(vocab_transform [TGT_LANGUAGE ])\\nEMB_SIZE\\xa0=\\xa0 512\\nNHEAD\\xa0=\\xa0 8\\nFFN_HID_DIM\\xa0=\\xa0 512\\nBATCH_SIZE\\xa0=\\xa0 128\\nNUM_ENCODER_LAYERS\\xa0=\\xa0 3\\nNUM_DECODER_LAYERS\\xa0=\\xa0 3\\ntransformer\\xa0=\\xa0Seq2SeqTransformer (NUM_ENCODER_LAYERS ,\\xa0NUM_DECODER_LAYERS ,\\xa0EMB_SIZE ,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0NHEAD ,\\xa0SRC_VOCAB_SIZE ,\\xa0TGT_VOCAB_SIZE ,\\xa0FFN_HID_DIM )\\nfor\\xa0p\\xa0in\\xa0transformer.parameters ():\\n\\xa0\\xa0\\xa0\\xa0if\\xa0p.dim()\\xa0>\\xa01:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0nn.init.xavier_uniform_ (p)\\ntransformer\\xa0=\\xa0transformer.to (DEVICE)\\nloss_fn\\xa0=\\xa0torch.nn.CrossEntropyLoss (ignore_index=PAD_IDX )\\noptimizer\\xa0=\\xa0torch.optim.Adam (transformer.parameters (),\\xa0lr=0.0001,\\xa0betas=(0.9,\\xa00.98),\\xa0eps=1e-9)\\nAs seen in the Data Sourcing and Processing section, the data iter ator yields a pair of r aw strings. Conv ert string pairs int o the batched\\ntensors that can be pr ocessed b y our Seq2Seq network de\\x00ned pr eviously . De\\x00ne our collate function that conv erts a batch of r aw strings int o\\nbatch tensors t o feed dir ectly int o the model.Collation\\ue3135/5/24, 12:58 PM Jorge_Gosalvez_255_HW9_trans_de_en_Pytorch.ipynb - Colab\\nhttps://colab.research.google.com/drive/1RIjNTsxbQRFg1ofAdsZ4UrwOC0oyKZHy#printMode=true 6/8\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW9_trans_de_en.pdf', 'page': 5}),\n",
              " Document(page_content='from\\xa0torch.nn.utils.rnn\\xa0 import\\xa0pad_sequence\\n#\\xa0helper\\xa0function\\xa0to\\xa0club\\xa0together\\xa0sequential\\xa0oper ations\\ndef\\xa0sequential_transforms (*transforms ):\\n\\xa0\\xa0\\xa0\\xa0def\\xa0func(txt_input ):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for\\xa0transform\\xa0 in\\xa0transforms :\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0txt_input\\xa0=\\xa0transform (txt_input )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return\\xa0txt_input\\n\\xa0\\xa0\\xa0\\xa0return\\xa0func\\n#\\xa0function\\xa0to\\xa0add\\xa0BOS/EOS\\xa0and\\xa0create\\xa0tensor\\xa0for\\xa0in put\\xa0sequence\\xa0indices\\ndef\\xa0tensor_transform (token_ids :\\xa0List[int]):\\n\\xa0\\xa0\\xa0\\xa0return\\xa0torch.cat ((torch.tensor ([BOS_IDX]),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0torch.tensor (token_ids ),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0torch.tensor ([EOS_IDX])))\\n#\\xa0``src``\\xa0and\\xa0``tgt``\\xa0language\\xa0text\\xa0transforms\\xa0to\\xa0 convert\\xa0raw\\xa0strings\\xa0into\\xa0tensors\\xa0indices\\ntext_transform\\xa0=\\xa0 {}\\nfor\\xa0ln\\xa0in\\xa0[SRC_LANGUAGE ,\\xa0TGT_LANGUAGE ]:\\n\\xa0\\xa0\\xa0\\xa0text_transform [ln]\\xa0=\\xa0sequential_transforms (token_transform [ln],\\xa0#Tokenization\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0voc ab_transform [ln],\\xa0#Numericalization\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0ten sor_transform )\\xa0#\\xa0Add\\xa0BOS/EOS\\xa0and\\xa0create\\xa0tensor\\n#\\xa0function\\xa0to\\xa0collate\\xa0data\\xa0samples\\xa0into\\xa0batch\\xa0tens ors\\ndef\\xa0collate_fn (batch):\\n\\xa0\\xa0\\xa0\\xa0src_batch ,\\xa0tgt_batch\\xa0=\\xa0 [],\\xa0[]\\n\\xa0\\xa0\\xa0\\xa0for\\xa0src_sample ,\\xa0tgt_sample\\xa0 in\\xa0batch:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0src_batch.append (text_transform [SRC_LANGUAGE ](src_sample.rstrip (\"\\\\n\")))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0tgt_batch.append (text_transform [TGT_LANGUAGE ](tgt_sample.rstrip (\"\\\\n\")))\\n\\xa0\\xa0\\xa0\\xa0src_batch\\xa0=\\xa0pad_sequence (src_batch ,\\xa0padding_value=PAD_IDX )\\n\\xa0\\xa0\\xa0\\xa0tgt_batch\\xa0=\\xa0pad_sequence (tgt_batch ,\\xa0padding_value=PAD_IDX )\\n\\xa0\\xa0\\xa0\\xa0return\\xa0src_batch ,\\xa0tgt_batch\\nDe\\x00ne tr aining and e valuation loop that will be called for each epoch.5/5/24, 12:58 PM Jorge_Gosalvez_255_HW9_trans_de_en_Pytorch.ipynb - Colab\\nhttps://colab.research.google.com/drive/1RIjNTsxbQRFg1ofAdsZ4UrwOC0oyKZHy#printMode=true 7/8', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW9_trans_de_en.pdf', 'page': 6}),\n",
              " Document(page_content='from\\xa0torch.utils.data\\xa0 import\\xa0DataLoader\\ndef\\xa0train_epoch (model,\\xa0optimizer ):\\n\\xa0\\xa0\\xa0\\xa0model.train ()\\n\\xa0\\xa0\\xa0\\xa0losses\\xa0=\\xa0 0\\n\\xa0\\xa0\\xa0\\xa0train_iter\\xa0=\\xa0Multi30k (split=\\'train\\',\\xa0language_pair= (SRC_LANGUAGE ,\\xa0TGT_LANGUAGE ))\\n\\xa0\\xa0\\xa0\\xa0train_dataloader\\xa0=\\xa0DataLoader (train_iter ,\\xa0batch_size=BATCH_SIZE ,\\xa0collate_fn=collate_fn )\\n\\xa0\\xa0\\xa0\\xa0for\\xa0src,\\xa0tgt\\xa0in\\xa0train_dataloader :\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0src\\xa0=\\xa0src.to (DEVICE)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0tgt\\xa0=\\xa0tgt.to (DEVICE)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0tgt_input\\xa0=\\xa0tgt [:-1,\\xa0:]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0src_mask ,\\xa0tgt_mask ,\\xa0src_padding_mask ,\\xa0tgt_padding_mask\\xa0=\\xa0create_mask (src,\\xa0tgt_input )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0logits\\xa0=\\xa0model (src,\\xa0tgt_input ,\\xa0src_mask ,\\xa0tgt_mask ,src_padding_mask ,\\xa0tgt_padding_mask ,\\xa0src_padding_mask )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0optimizer.zero_grad ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0tgt_out\\xa0=\\xa0tgt [1:,\\xa0:]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0loss\\xa0=\\xa0loss_fn (logits.reshape (-1,\\xa0logits.shape [-1]),\\xa0tgt_out.reshape (-1))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0loss.backward ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0optimizer.step ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0losses\\xa0+=\\xa0loss.item ()\\n\\xa0\\xa0\\xa0\\xa0return\\xa0losses\\xa0/\\xa0 len(list(train_dataloader ))\\ndef\\xa0evaluate (model):\\n\\xa0\\xa0\\xa0\\xa0model. eval()\\n\\xa0\\xa0\\xa0\\xa0losses\\xa0=\\xa0 0\\n\\xa0\\xa0\\xa0\\xa0val_iter\\xa0=\\xa0Multi30k (split=\\'valid\\',\\xa0language_pair= (SRC_LANGUAGE ,\\xa0TGT_LANGUAGE ))\\n\\xa0\\xa0\\xa0\\xa0val_dataloader\\xa0=\\xa0DataLoader (val_iter ,\\xa0batch_size=BATCH_SIZE ,\\xa0collate_fn=collate_fn )\\n\\xa0\\xa0\\xa0\\xa0for\\xa0src,\\xa0tgt\\xa0in\\xa0val_dataloader :\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0src\\xa0=\\xa0src.to (DEVICE)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0tgt\\xa0=\\xa0tgt.to (DEVICE)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0tgt_input\\xa0=\\xa0tgt [:-1,\\xa0:]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0src_mask ,\\xa0tgt_mask ,\\xa0src_padding_mask ,\\xa0tgt_padding_mask\\xa0=\\xa0create_mask (src,\\xa0tgt_input )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0logits\\xa0=\\xa0model (src,\\xa0tgt_input ,\\xa0src_mask ,\\xa0tgt_mask ,src_padding_mask ,\\xa0tgt_padding_mask ,\\xa0src_padding_mask )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0tgt_out\\xa0=\\xa0tgt [1:,\\xa0:]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0loss\\xa0=\\xa0loss_fn (logits.reshape (-1,\\xa0logits.shape [-1]),\\xa0tgt_out.reshape (-1))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0losses\\xa0+=\\xa0loss.item ()\\n\\xa0\\xa0\\xa0\\xa0return\\xa0losses\\xa0/\\xa0 len(list(val_dataloader ))Train the model.\\nfrom\\xa0timeit\\xa0 import\\xa0default_timer\\xa0 as\\xa0timer\\nNUM_EPOCHS\\xa0=\\xa0 25\\nfor\\xa0epoch\\xa0in\\xa0range(1,\\xa0NUM_EPOCHS+ 1):\\n\\xa0\\xa0\\xa0\\xa0start_time\\xa0=\\xa0timer ()\\n\\xa0\\xa0\\xa0\\xa0train_loss\\xa0=\\xa0train_epoch (transformer ,\\xa0optimizer )\\n\\xa0\\xa0\\xa0\\xa0end_time\\xa0=\\xa0timer ()\\n\\xa0\\xa0\\xa0\\xa0val_loss\\xa0=\\xa0evaluate (transformer )\\n\\xa0\\xa0\\xa0\\xa0print((f\"Epoch:\\xa0 {epoch},\\xa0Train\\xa0loss:\\xa0 {train_loss :.3f},\\xa0Val\\xa0loss:\\xa0 {val_loss :.3f},\\xa0\"f\"Epoch\\xa0time\\xa0=\\xa0 {(end_time\\xa0-\\xa0start_time ):.3\\n#\\xa0function\\xa0to\\xa0generate\\xa0output\\xa0sequence\\xa0using\\xa0greed y\\xa0algorithm\\ndef\\xa0greedy_decode (model,\\xa0src,\\xa0src_mask ,\\xa0max_len,\\xa0start_symbol ):\\n\\xa0\\xa0\\xa0\\xa0src\\xa0=\\xa0src.to (DEVICE)\\n\\xa0\\xa0\\xa0\\xa0src_mask\\xa0=\\xa0src_mask.to (DEVICE)\\n\\xa0\\xa0\\xa0\\xa0memory\\xa0=\\xa0model.encode (src,\\xa0src_mask )\\n\\xa0\\xa0\\xa0\\xa0ys\\xa0=\\xa0torch.ones (1,\\xa01).fill_(start_symbol ).type(torch.long).to(DEVICE)\\n\\xa0\\xa0\\xa0\\xa0for\\xa0i\\xa0in\\xa0range(max_len-1):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0memory\\xa0=\\xa0memory.to (DEVICE)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0tgt_mask\\xa0=\\xa0 (generate_square_subsequent_mask (ys.size(0))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0. type(torch.bool)).to(DEVICE)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0out\\xa0=\\xa0model.decode (ys,\\xa0memory,\\xa0tgt_mask )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0out\\xa0=\\xa0out.transpose (0,\\xa01)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0prob\\xa0=\\xa0model.generator (out[:,\\xa0-1])\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0_ ,\\xa0next_word\\xa0=\\xa0torch. max(prob,\\xa0dim=1)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0next_word\\xa0=\\xa0next_word.item ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0ys\\xa0=\\xa0torch.cat ([ys,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0torch.ones (1,\\xa01).type_as (src.data ).fill_(next_word )],\\xa0dim=0)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\xa0next_word\\xa0==\\xa0EOS_IDX :\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 break\\n\\xa0\\xa0\\xa0\\xa0return\\xa0ys\\n#\\xa0actual\\xa0function\\xa0to\\xa0translate\\xa0input\\xa0sentence\\xa0into \\xa0target\\xa0language\\ndef\\xa0translate (model:\\xa0torch.nn.Module,\\xa0src_sentence :\\xa0str):\\n\\xa0\\xa0\\xa0\\xa0model. eval()\\n\\xa0\\xa0\\xa0\\xa0src\\xa0=\\xa0text_transform [SRC_LANGUAGE ](src_sentence ).view(-1,\\xa01)\\n\\xa0\\xa0\\xa0\\xa0num_tokens\\xa0=\\xa0src.shape [0]\\n\\xa0\\xa0\\xa0\\xa0src_mask\\xa0=\\xa0 (torch.zeros (num_tokens ,\\xa0num_tokens )).type(torch.bool)\\n\\xa0\\xa0\\xa0\\xa0tgt_tokens\\xa0=\\xa0greedy_decode (\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0model ,\\xa0\\xa0src,\\xa0src_mask ,\\xa0max_len=num_tokens\\xa0+\\xa0 5,\\xa0start_symbol=BOS_IDX ).flatten ()\\n\\xa0\\xa0\\xa0\\xa0return\\xa0\"\\xa0\".join(vocab_transform [TGT_LANGUAGE ].lookup_tokens (list(tgt_tokens.cpu ().numpy()))).replace (\"<bos>\",\\xa0\"\").replace (\"<\\nEpoch: 1, Train loss: 5.337, Val loss: 4.102, Epoch time = 23.990s\\nEpoch: 2, Train loss: 3.760, Val loss: 3.312, Epoch time = 21.813s\\nEpoch: 3, Train loss: 3.159, Val loss: 2.882, Epoch time = 21.874s\\nEpoch: 4, Train loss: 2.768, Val loss: 2.631, Epoch time = 21.996s\\nEpoch: 5, Train loss: 2.479, Val loss: 2.439, Epoch time = 22.149s\\nEpoch: 6, Train loss: 2.251, Val loss: 2.317, Epoch time = 22.230s\\nEpoch: 7, Train loss: 2.059, Val loss: 2.212, Epoch time = 22.057s\\nEpoch: 8, Train loss: 1.897, Val loss: 2.119, Epoch time = 21.400s\\nEpoch: 9, Train loss: 1.757, Val loss: 2.052, Epoch time = 21.840s5/5/24, 12:58 PM Jorge_Gosalvez_255_HW9_trans_de_en_Pytorch.ipynb - Colab\\nhttps://colab.research.google.com/drive/1RIjNTsxbQRFg1ofAdsZ4UrwOC0oyKZHy#printMode=true 8/8', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW9_trans_de_en.pdf', 'page': 7}),\n",
              " Document(page_content=\"Homework 03: Cust om CNNsSJSU MSDS 255 DL, Spring 2024\\ue313\\nGit: https:/ /github.com/jr gosalv ez/data255_DL\\n#\\xa0Setup\\xa0experiment\\xa0and\\xa0import\\xa0CIFAR\\xa0data\\xa0from\\xa0Pyto rch\\n#\\xa0https://pytorch.org/vision/stable/datasets.html\\nimport\\xa0numpy\\xa0as\\xa0np\\nimport\\xa0time\\nimport\\xa0torch\\nimport\\xa0torch.nn\\xa0 as\\xa0nn\\nimport\\xa0torch.optim\\xa0 as\\xa0optim\\nimport\\xa0torch.nn.functional\\xa0 as\\xa0F\\nimport\\xa0torchvision\\nimport\\xa0torchvision.transforms\\xa0 as\\xa0transforms\\nfrom\\xa0torchvision.datasets\\xa0 import\\xa0CIFAR10\\nfrom\\xa0torch.utils.data\\xa0 import\\xa0DataLoader ,\\xa0Subset\\nfrom\\xa0torchvision\\xa0 import\\xa0datasets ,\\xa0models\\nimport\\xa0os\\nimport\\xa0matplotlib.pyplot\\xa0 as\\xa0plt\\nWithout a pr etrained model \\ue313\\n1. Cr eate cust om dataset of 3 categories fr om CIF AR10 with at least 100 images each \\ue313\\nMSD A 255, F all demo_03-classi\\x00cation examples r eferenced\\nUnsuppor ted Cell Type. Double-Click t o inspect/edit the content.\\ndef\\xa0show_data (img):\\n\\xa0\\xa0\\xa0\\xa0try:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0plt.imshow (img[0])\\n\\xa0\\xa0\\xa0\\xa0except\\xa0Exception\\xa0 as\\xa0e:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 print(e)\\n\\xa0\\xa0\\xa0\\xa0print(img[0].shape,\\xa0img[0].permute (1,2,0).shape)\\n\\xa0\\xa0\\xa0\\xa0plt.imshow (img[0].permute (1,2,0))\\n\\xa0\\xa0\\xa0\\xa0plt.title ('y\\xa0=\\xa0'+\\xa0str(img[1]))\\n\\xa0\\xa0\\xa0\\xa0plt.show ()\\n#\\xa0Define\\xa0transform\\xa0to\\xa0preprocess\\xa0the\\xa0data.\\xa0NOTE:\\xa0c an\\xa0define\\xa0transformations\\xa0for\\xa0train\\xa0and\\xa0test\\xa0(vali dation)\\xa0sets.\\xa0\\n#\\xa0Will\\xa0keep\\xa0simple\\xa0for\\xa0this\\xa0model\\ntransform\\xa0=\\xa0transforms.Compose ([\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.RandomResizedCrop (224),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.RandomHorizontalFlip (),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.ToTensor (),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.Normalize ([0.485,\\xa00.456,\\xa00.406],\\xa0[0.229,\\xa00.224,\\xa00.225])\\n\\xa0\\xa0\\xa0\\xa0])\\n#\\xa0Load\\xa0CIFAR10\\xa0dataset\\ntrainset\\xa0=\\xa0datasets.CIFAR10 (root='./data' ,\\xa0train=True,\\xa0download= True,\\xa0transform=transform )\\ntestset\\xa0\\xa0=\\xa0datasets.CIFAR10 (root='./data' ,\\xa0train=False,\\xa0download= True,\\xa0transform=transform )\\nFiles already downloaded and verified\\nFiles already downloaded and verified\\ntrainset\\nDataset CIFAR10\\n    Number of datapoints: 50000\\n    Root location: ./data\\n    Split: Train5/5/24, 1:00 PM Jorge-Gosalvez_255_HW3 (1).ipynb - Colab\\nhttps://colab.research.google.com/drive/1P3xSSReWugbWYgunQEEK6ktGeMY3734S#printMode=true 1/14\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW3_CNN.pdf', 'page': 0}),\n",
              " Document(page_content=\"    StandardTransform\\nTransform: Compose(\\n               RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear,  \\nantialias=True)\\n               RandomHorizontalFlip(p=0.5)\\n               ToTensor()\\n               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\\n           )\\ntestset\\nDataset CIFAR10\\n    Number of datapoints: 10000\\n    Root location: ./data\\n    Split: Test\\n    StandardTransform\\nTransform: Compose(\\n               RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear,  \\nantialias=True)\\n               RandomHorizontalFlip(p=0.5)\\n               ToTensor()\\n               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\\n           )\\nclass_names\\xa0=\\xa0trainset.classes\\nprint(class_names )\\n['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\\nnum_classes\\xa0=\\xa0 len(class_names )\\nprint(num_classes )\\n10\\n10 classes as par t of CIF AR10 dataset\\n#\\xa0select\\xa0classes\\xa0you\\xa0want\\xa0to\\xa0include\\xa0in\\xa0your\\xa0subse t\\n#\\xa0https://www.cs.toronto.edu/~kriz/cifar.html\\xa0\\n#\\xa010\\xa0classes\\xa0total,\\xa0picked\\xa0three;\\xa00:\\xa0airplane,\\xa02:\\xa0 bird,\\xa06:\\xa0frog\\nclasses\\xa0=\\xa0torch.tensor ([0,\\xa02,\\xa06])\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n#\\xa0get\\xa0indices\\xa0that\\xa0correspond\\xa0to\\xa0one\\xa0of\\xa0the\\xa0select ed\\xa0classes\\nindices\\xa0=\\xa0 (torch.tensor (trainset.targets )[...,\\xa0None]\\xa0==\\xa0classes ).any(-1).nonzero (as_tuple= True)[0]\\ntest_indices\\xa0=\\xa0 (torch.tensor (testset.targets )[...,\\xa0None]\\xa0==\\xa0classes ).any(-1).nonzero (as_tuple= True)[0]\\n#\\xa0subset\\xa0the\\xa0dataset\\xa0and\\xa0reduce\\xa0to\\xa01/4th\\xa0the\\xa0size\\xa0 and\\xa0randamize\\ndata\\xa0=\\xa0Subset (trainset ,\\xa0indices )\\n#\\xa01/7th\\xa0the\\xa0datapoints\\xa0of\\xa0each\\xa0of\\xa0the\\xa0three\\xa0classe s\\xa0from\\xa0the\\xa0trainset\\ntotal\\xa0=\\xa0 len(data)\\ntrain_data\\xa0=\\xa0torch.utils.data.random_split (data,\\xa0[total//7,\\xa0total-total// 7])[0]\\nt_data\\xa0=\\xa0torch.utils.data.random_split (data,\\xa0[total//7,\\xa0total-total// 7])[0]\\nprint(len(train_data ))\\nprint(len(t_data))\\n2142\\n2142\\nshow_data (train_data [8])5/5/24, 1:00 PM Jorge-Gosalvez_255_HW3 (1).ipynb - Colab\\nhttps://colab.research.google.com/drive/1P3xSSReWugbWYgunQEEK6ktGeMY3734S#printMode=true 2/14\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW3_CNN.pdf', 'page': 1}),\n",
              " Document(page_content='Clipping input data to the valid range for imshow with RGB data ([0..1] for floa\\nInvalid shape (3, 224, 224) for image data\\ntorch.Size([3, 224, 224]) torch.Size([224, 224, 3])\\n2. Split cust om data 80% tr aining and 20% v alidation set for tr aining and testing \\ue313\\n#\\xa0Split\\xa0the\\xa0dataset\\xa0into\\xa0training\\xa0and\\xa0validation\\xa0s ets\\xa0(80-20\\xa0split)\\ntrain_size\\xa0=\\xa0 int(0.8\\xa0*\\xa0len(train_data ))\\nval_size\\xa0\\xa0\\xa0=\\xa0 len(train_data )\\xa0-\\xa0train_size\\ntrain_data ,\\xa0valset\\xa0=\\xa0torch.utils.data.random_split (train_data ,\\xa0[train_size ,\\xa0val_size ])\\n#\\xa0Define\\xa0data\\xa0loaders\\ntrain_dataloader\\xa0=\\xa0DataLoader (train_data ,\\xa0batch_size= 64,\\xa0shuffle= True,\\xa0num_workers= 2)\\nval_dataloader\\xa0\\xa0\\xa0=\\xa0DataLoader (valset,\\xa0batch_size= 64,\\xa0shuffle= False,\\xa0num_workers= 2)\\ntest_dataloader\\xa0\\xa0=\\xa0DataLoader (t_data,\\xa0batch_size= 64,\\xa0shuffle= False,\\xa0num_workers= 2)\\n#\\xa0Check\\xa0the\\xa0size\\xa0of\\xa0each\\xa0dataset\\nprint(\"Training\\xa0set\\xa0size:\" ,\\xa0len(train_data ))\\nprint(\"Validation\\xa0set\\xa0size:\" ,\\xa0len(valset))\\nprint(\"Test\\xa0set\\xa0size:\" ,\\xa0len(t_data))\\nprint(f\"Train\\xa0+\\xa0Val\\xa0 {len(train_data )\\xa0+\\xa0len(valset)}\\xa0|\\xa0Test\\xa0Set\\xa0 {len(t_data)}\")\\nTraining set size: 1713\\nValidation set size: 429\\nTest set size: 2142\\nTrain + Val 2142 | Test Set 2142\\n3. Pr eprocess data \\ue313\\n#\\xa0train_dataloader\\xa0is\\xa0PyTorch\\xa0DataLoader\\ndata_iter\\xa0=\\xa0 iter(train_dataloader )\\n#\\xa0Fetch\\xa0the\\xa0first\\xa0batch\\xa0of\\xa0data\\nimages,\\xa0labels\\xa0=\\xa0 next(data_iter )\\n#\\xa0Plot\\xa0the\\xa0first\\xa012\\xa0images\\nplt.figure (figsize= (8,\\xa08))\\nfor\\xa0i\\xa0in\\xa0range(12):\\n\\xa0\\xa0\\xa0\\xa0image\\xa0=\\xa0images [i].permute (1,\\xa02,\\xa00).numpy()\\xa0\\xa0#\\xa0Convert\\xa0tensor\\xa0to\\xa0numpy\\xa0array\\n\\xa0\\xa0\\xa0\\xa0plt.subplot (3,\\xa04,\\xa0i\\xa0+\\xa01)\\n\\xa0\\xa0\\xa0\\xa0plt.imshow (image)\\n\\xa0\\xa0\\xa0\\xa0plt.title (f\"Label:\\xa0 {class_names [labels[i].item()]}\")\\n\\xa0\\xa0\\xa0\\xa0plt.axis (\"off\")\\nplt.show ()5/5/24, 1:00 PM Jorge-Gosalvez_255_HW3 (1).ipynb - Colab\\nhttps://colab.research.google.com/drive/1P3xSSReWugbWYgunQEEK6ktGeMY3734S#printMode=true 3/14', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW3_CNN.pdf', 'page': 2}),\n",
              " Document(page_content=\"Clipping input data to the valid range for imshow with RGB data ([0..1] for floa\\nClipping input data to the valid range for imshow with RGB data ([0..1] for floa\\nClipping input data to the valid range for imshow with RGB data ([0..1] for floa\\nClipping input data to the valid range for imshow with RGB data ([0..1] for floa\\nClipping input data to the valid range for imshow with RGB data ([0..1] for floa\\nClipping input data to the valid range for imshow with RGB data ([0..1] for floa\\nClipping input data to the valid range for imshow with RGB data ([0..1] for floa\\nClipping input data to the valid range for imshow with RGB data ([0..1] for floa\\nClipping input data to the valid range for imshow with RGB data ([0..1] for floa\\nClipping input data to the valid range for imshow with RGB data ([0..1] for floa\\nClipping input data to the valid range for imshow with RGB data ([0..1] for floa\\nClipping input data to the valid range for imshow with RGB data ([0..1] for floa\\nprint(f'Length\\xa0of\\xa0train\\xa0dataloader:\\xa0 {len(train_dataloader.dataset )}')\\nfor\\xa0image_batch ,\\xa0labels_batch\\xa0 in\\xa0train_dataloader :\\n\\xa0\\xa0print(image_batch.shape )\\n\\xa0\\xa0print(labels_batch.shape )\\n\\xa0\\xa0break\\nLength of train dataloader: 1713\\ntorch.Size([64, 3, 224, 224])\\ntorch.Size([64])\\ntorch.siz e (batch, channel, image siz e X, image siz e Y), wher e the image batch is tensor of shape ([64, 3, 224, 224]) with a batch of 64 images.\\nThe shape of these images is 244x244x3 based on center out tr ansformation. Images ar e blur y b/c the y are str etched, nativ e images ar e\\n32x32. 3 indicates the channel (e.g. RBG, not gr ay or other).\\nprint(train_dataloader.dataset [0][0].shape)\\nprint(train_dataloader.dataset [1][0].shape)\\nprint(images.shape )\\nprint(labels.shape )\\ntorch.Size([3, 224, 224])\\ntorch.Size([3, 224, 224])\\ntorch.Size([64, 3, 224, 224])\\ntorch.Size([64])\\n4. Cr eate CNN t o learn tr aining set \\ue3135/5/24, 1:00 PM Jorge-Gosalvez_255_HW3 (1).ipynb - Colab\\nhttps://colab.research.google.com/drive/1P3xSSReWugbWYgunQEEK6ktGeMY3734S#printMode=true 4/14\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW3_CNN.pdf', 'page': 3}),\n",
              " Document(page_content='class\\xa0CustomModel (nn.Module):\\n\\xa0\\xa0\\xa0\\xa0def\\xa0__init__ (self,\\xa0num_classes ):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0super (CustomModel ,\\xa0self).__init__ ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.rescale\\xa0=\\xa0nn.Sequential (\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0nn.Conv2d (3,\\xa03,\\xa0kernel_size= 1),\\xa0\\xa0#\\xa0Rescaling\\xa0layer,\\xa01x1\\xa0conv\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0nn.BatchNorm2d (3),\\xa0\\xa0#\\xa0Batch\\xa0normalization\\xa0to\\xa0maintain\\xa0scale\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0nn.ReLU (inplace= True)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.conv1\\xa0=\\xa0nn.Conv2d (3,\\xa016,\\xa0kernel_size= 3)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.pool1\\xa0=\\xa0nn.MaxPool2d (kernel_size= 2,\\xa0stride= 2)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.conv2\\xa0=\\xa0nn.Conv2d (16,\\xa032,\\xa0kernel_size= 3)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.pool2\\xa0=\\xa0nn.MaxPool2d (kernel_size= 2,\\xa0stride= 2)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.conv3\\xa0=\\xa0nn.Conv2d (32,\\xa064,\\xa0kernel_size= 3)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.pool3\\xa0=\\xa0nn.MaxPool2d (kernel_size= 2,\\xa0stride= 2)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.flatten\\xa0=\\xa0nn.Flatten ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.fc1\\xa0=\\xa0nn.Linear (224\\xa0*\\xa0224\\xa0*\\xa032,\\xa0128)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.fc2\\xa0=\\xa0nn.Linear (128,\\xa0num_classes )\\n\\xa0\\xa0\\xa0\\xa0def\\xa0forward(self,\\xa0x):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0 self.rescale (x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0 self.conv1(x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0nn.ReLU (inplace= True)(x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0 self.pool1(x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0 self.conv2(x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0nn.ReLU (inplace= True)(x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0 self.pool2(x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0 self.conv3(x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0nn.ReLU (inplace= True)(x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0 self.pool3(x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0 self.flatten (x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0 self.fc1(x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0nn.ReLU (inplace= True)(x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0 self.fc2(x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return\\xa0x\\n#\\xa0Instantiate\\xa0the\\xa0model\\n#\\xa0num_classes\\xa0=\\xa010\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0#\\xa010\\xa0categories\\xa0 in\\xa0CIFAR\\xa0dataset\\nmodel\\xa0=\\xa0CustomModel (num_classes )\\n#\\xa0Print\\xa0the\\xa0model\\xa0architecture\\nprint(model)\\nCustomModel(\\n  (rescale): Sequential(\\n    (0): Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1))\\n    (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\\n    (2): ReLU(inplace=True)\\n  )\\n  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\\n  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\\n  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\\n  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\\n  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\\n  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\\n  (flatten): Flatten(start_dim=1, end_dim=-1)\\n  (fc1): Linear(in_features=1605632, out_features=128, bias=True)\\n  (fc2): Linear(in_features=128, out_features=10, bias=True)\\n)5/5/24, 1:00 PM Jorge-Gosalvez_255_HW3 (1).ipynb - Colab\\nhttps://colab.research.google.com/drive/1P3xSSReWugbWYgunQEEK6ktGeMY3734S#printMode=true 5/14', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW3_CNN.pdf', 'page': 4}),\n",
              " Document(page_content='class\\xa0CNNModel (nn.Module):\\n\\xa0\\xa0\\xa0\\xa0def\\xa0__init__ (self,\\xa0num_classes ,\\xa0img_height ,\\xa0img_width ):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0super (CNNModel ,\\xa0self).__init__ ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.rescaling\\xa0=\\xa0nn.Sequential (\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0nn.Conv2d (3,\\xa03,\\xa0kernel_size= 1),\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0nn.BatchNorm2d (3),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0nn.ReLU (inplace= True)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.conv1\\xa0=\\xa0nn.Conv2d (3,\\xa016,\\xa0kernel_size= 3,\\xa0padding= 1)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.pool\\xa0=\\xa0nn.MaxPool2d (2,\\xa02)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.conv2\\xa0=\\xa0nn.Conv2d (16,\\xa032,\\xa0kernel_size= 3,\\xa0padding= 1)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.conv3\\xa0=\\xa0nn.Conv2d (32,\\xa064,\\xa0kernel_size= 3,\\xa0padding= 1)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.flatten\\xa0=\\xa0nn.Flatten ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.fc1\\xa0=\\xa0nn.Linear (64\\xa0*\\xa0(img_height\\xa0//\\xa0 8)\\xa0*\\xa0(img_width\\xa0//\\xa0 8),\\xa0128)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.fc2\\xa0=\\xa0nn.Linear (128,\\xa0num_classes )\\n\\xa0\\xa0\\xa0\\xa0def\\xa0forward(self,\\xa0x):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0 self.rescaling (x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0 self.pool(nn.functional.relu (self.conv1(x)))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0 self.pool(nn.functional.relu (self.conv2(x)))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0 self.pool(nn.functional.relu (self.conv3(x)))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0 self.flatten (x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0nn.functional.relu (self.fc1(x))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0 self.fc2(x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return\\xa0x\\n#\\xa0Define\\xa0input\\xa0dimensions\\nimg_height ,\\xa0img_width\\xa0=\\xa0 224,\\xa0224\\n#\\xa0Define\\xa0the\\xa0number\\xa0of\\xa0classes\\n#\\xa0num_classes\\xa0=\\xa010\\n#\\xa0Instantiate\\xa0the\\xa0model\\xa0woTransfer\\nmodel\\xa0=\\xa0CNNModel (num_classes ,\\xa0img_height ,\\xa0img_width )\\n#\\xa0optimizer\\noptimizer\\xa0=\\xa0optim.Adam (model.parameters ())\\n#\\xa0loss\\xa0function\\ncriterion\\xa0=\\xa0nn.CrossEntropyLoss ()\\n#\\xa0Define\\xa0the\\xa0metrics\\xa0(accuracy)\\ndef\\xa0accuracy (outputs,\\xa0labels):\\n\\xa0\\xa0\\xa0\\xa0_,\\xa0preds\\xa0=\\xa0torch. max(outputs,\\xa0dim=1)\\n\\xa0\\xa0\\xa0\\xa0return\\xa0torch.tensor (torch.sum(preds\\xa0==\\xa0labels ).item()\\xa0/\\xa0len(preds))\\n#\\xa0Define\\xa0the\\xa0training\\xa0loop\\ndef\\xa0train_model (model,\\xa0train_dl ,\\xa0criterion ,\\xa0optimizer ,\\xa0num_epochs =10):\\n\\xa0\\xa0\\xa0\\xa0for\\xa0epoch\\xa0in\\xa0range(num_epochs ):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0model.train ()\\xa0\\xa0#\\xa0Set\\xa0the\\xa0model\\xa0to\\xa0train\\xa0mode\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0train_loss\\xa0=\\xa0 0.0\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0train_acc\\xa0\\xa0=\\xa0 0.0\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for\\xa0images,\\xa0labels\\xa0 in\\xa0train_dl :\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0optimizer.zero_grad ()\\xa0\\xa0#\\xa0Zero\\xa0the\\xa0parameter\\xa0gradients\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0outputs\\xa0=\\xa0model (images)\\xa0\\xa0#\\xa0Forward\\xa0pass\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0loss\\xa0=\\xa0criterion (outputs,\\xa0labels)\\xa0\\xa0#\\xa0Calculate\\xa0the\\xa0loss\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0loss.backward ()\\xa0\\xa0#\\xa0Backward\\xa0pass\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0optimizer.step ()\\xa0\\xa0#\\xa0Optimize\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Compute\\xa0training\\xa0accuracy\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0acc\\xa0=\\xa0accuracy (outputs,\\xa0labels)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0train_acc\\xa0+=\\xa0acc.item ()\\xa0*\\xa0images.size (0)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Track\\xa0training\\xa0loss\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0train_loss\\xa0+=\\xa0loss.item ()\\xa0*\\xa0images.size (0)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Print\\xa0training\\xa0statistics\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0epoch_loss\\xa0=\\xa0train_loss\\xa0/\\xa0 len(train_dl.dataset )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0epoch_acc\\xa0=\\xa0train_acc\\xa0/\\xa0 len(train_dl.dataset )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 print(f\"Epoch\\xa0{epoch\\xa0+\\xa0 1}/{num_epochs },\\xa0Loss:\\xa0 {epoch_loss :.4f},\\xa0Accuracy:\\xa0 {epoch_acc :.4f}\")5/5/24, 1:00 PM Jorge-Gosalvez_255_HW3 (1).ipynb - Colab\\nhttps://colab.research.google.com/drive/1P3xSSReWugbWYgunQEEK6ktGeMY3734S#printMode=true 6/14', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW3_CNN.pdf', 'page': 5}),\n",
              " Document(page_content='%%time\\n#\\xa0Train\\xa0the\\xa0model\\nnum_epochs\\xa0=\\xa0 3\\xa0\\xa0#\\xa0Choose\\xa0the\\xa0number\\xa0of\\xa0epochs\\ntrain_model (model,\\xa0train_dataloader ,\\xa0criterion ,\\xa0optimizer ,\\xa0num_epochs )\\nEpoch 1/3, Loss: 1.3408, Accuracy: 0.3497\\nEpoch 2/3, Loss: 1.0268, Accuracy: 0.4781\\nEpoch 3/3, Loss: 0.9585, Accuracy: 0.5406\\nCPU times: user 5min 24s, sys: 1min 29s, total: 6min 53s\\nWall time: 4min 15s\\nRetrain with data augmentation\\ue313\\n%%time\\n#\\xa0Define\\xa0the\\xa0training\\xa0loop\\ndef\\xa0train_model (model,\\xa0train_dl ,\\xa0criterion ,\\xa0optimizer ,\\xa0num_epochs =10):\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0for\\xa0epoch\\xa0in\\xa0range(num_epochs ):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0model.train ()\\xa0\\xa0#\\xa0Set\\xa0the\\xa0model\\xa0to\\xa0train\\xa0mode\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0train_loss\\xa0=\\xa0 0.0\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0train_acc\\xa0=\\xa0 0.0\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for\\xa0images,\\xa0labels\\xa0 in\\xa0train_dl :\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0data_augmented_inputs\\xa0=\\xa0torch.stack ([transforms.RandomHorizontalFlip ()(img)\\xa0for\\xa0img\\xa0in\\xa0images])\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0data_augmented_inputs\\xa0=\\xa0torch.stack ([transforms.RandomRotation (degrees= 10)(img)\\xa0for\\xa0img\\xa0in\\xa0data_augmented_inputs ])\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0optimizer.zero_grad ()\\xa0\\xa0#\\xa0Zero\\xa0the\\xa0parameter\\xa0gradients\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0outputs\\xa0=\\xa0model (images)\\xa0\\xa0#\\xa0Forward\\xa0pass\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0loss\\xa0=\\xa0criterion (outputs,\\xa0labels)\\xa0\\xa0#\\xa0Calculate\\xa0the\\xa0loss\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0loss.backward ()\\xa0\\xa0#\\xa0Backward\\xa0pass\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0optimizer.step ()\\xa0\\xa0#\\xa0Optimize\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Compute\\xa0training\\xa0accuracy\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0acc\\xa0=\\xa0accuracy (outputs,\\xa0labels)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0train_acc\\xa0+=\\xa0acc.item ()\\xa0*\\xa0images.size (0)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Track\\xa0training\\xa0loss\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0train_loss\\xa0+=\\xa0loss.item ()\\xa0*\\xa0images.size (0)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Print\\xa0training\\xa0statistics\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0epoch_loss\\xa0=\\xa0train_loss\\xa0/\\xa0 len(train_dl.dataset )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0epoch_acc\\xa0\\xa0=\\xa0train_acc\\xa0/\\xa0 len(train_dl.dataset )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 print(f\"Epoch\\xa0{epoch\\xa0+\\xa0 1}/{num_epochs },\\xa0Loss:\\xa0 {epoch_loss :.4f},\\xa0Accuracy:\\xa0 {epoch_acc :.4f}\")\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0return\\xa0epoch_loss ,\\xa0epoch_acc\\xa0\\xa0\\xa0\\xa0\\n#\\xa0Train\\xa0the\\xa0model\\nnum_epochs\\xa0=\\xa0 3\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Choose\\xa0the\\xa0number\\xa0of\\xa0epochs\\nm1_epoch_loss ,\\xa0m1_epoch_acc\\xa0=\\xa0train_model (model,\\xa0train_dataloader ,\\xa0criterion ,\\xa0optimizer ,\\xa0num_epochs )\\nprint(f\\'Model\\xa0Final:\\xa0loss\\xa0 {m1_epoch_loss :.4f}\\xa0|\\xa0accuracy\\xa0 {m1_epoch_acc :.4f}\\')\\nEpoch 1/3, Loss: 0.8994, Accuracy: 0.5867\\nEpoch 2/3, Loss: 0.8749, Accuracy: 0.6013\\nEpoch 3/3, Loss: 0.8522, Accuracy: 0.6025\\nModel Final: loss 0.8522 | accuracy 0.6025\\nCPU times: user 5min 44s, sys: 1min 59s, total: 7min 43s\\nWall time: 4min 39s\\n5. Mak e predictions on test dataset and compar e accur acy t o expected categories \\ue313\\nfrom\\xa0PIL\\xa0import\\xa0Image\\nimport\\xa0requests\\nfrom\\xa0io\\xa0import\\xa0BytesIO\\nfrom\\xa0torch\\xa0import\\xa0argmax5/5/24, 1:00 PM Jorge-Gosalvez_255_HW3 (1).ipynb - Colab\\nhttps://colab.research.google.com/drive/1P3xSSReWugbWYgunQEEK6ktGeMY3734S#printMode=true 7/14', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW3_CNN.pdf', 'page': 6}),\n",
              " Document(page_content='img_height ,\\xa0img_width\\xa0=\\xa0 224,\\xa0224\\n#\\xa0Define\\xa0the\\xa0location\\xa0of\\xa0the\\xa0image\\n#\\xa0https://cdn.britannica.com/q:60/69/155469-131-14 083F59/airplane-flight.jpg\\n#\\xa0https://external-content.duckduckgo.com/iu/?u=ht tp%3A%2F%2Fwallpaperping.com%2Fwp-content%2Fupload s%2F2018%2F12%2FBluebirdTote\\n#\\xa0another\\xa0frog:\\xa0https://www.welcomewildlife.com/wp -content/uploads/2015/01/Frog-compare.jpg\\nfrog_url\\xa0=\\xa0 \"https://zoo.bca.ac.uk/wp-content/uploads/sites/2/ 2020/02/BCA-Small-Size-101.jpg\"\\n#\\xa0Download\\xa0the\\xa0image\\xa0and\\xa0convert\\xa0it\\xa0to\\xa0a\\xa0PyTorch\\xa0t ensor\\nresponse\\xa0=\\xa0requests.get (frog_url )\\nimg\\xa0=\\xa0Image. open(BytesIO(response.content )).convert (\\'RGB\\')\\nimg\\xa0=\\xa0img.resize ((img_width ,\\xa0img_height ))\\xa0\\xa0#\\xa0Resize\\xa0the\\xa0image\\xa0to\\xa0match\\xa0the\\xa0model\\'s\\xa0input\\xa0size\\nimg_tensor\\xa0=\\xa0transforms.ToTensor ()(img)\\nimg_tensor\\xa0=\\xa0img_tensor.unsqueeze (0)\\xa0\\xa0#\\xa0Add\\xa0batch\\xa0dimension\\n#\\xa0Move\\xa0the\\xa0tensor\\xa0to\\xa0the\\xa0appropriate\\xa0device\\xa0(GPU\\xa0i f\\xa0available)\\ndevice\\xa0=\\xa0torch.device (\"cuda\"\\xa0if\\xa0torch.cuda.is_available ()\\xa0else\\xa0\"cpu\")\\nimg_tensor\\xa0=\\xa0img_tensor.to (device)\\n#\\xa0Make\\xa0predictions\\nmodel.eval()\\xa0\\xa0#\\xa0Set\\xa0the\\xa0model\\xa0to\\xa0evaluation\\xa0mode\\nwith\\xa0torch.no_grad ():\\n\\xa0\\xa0\\xa0\\xa0outputs\\xa0=\\xa0model (img_tensor )\\n\\xa0\\xa0\\xa0\\xa0probabilities\\xa0=\\xa0torch.softmax (outputs,\\xa0dim=1)\\n#\\xa0Convert\\xa0tensor\\xa0to\\xa0numpy\\xa0array\\npredicted_class\\xa0=\\xa0class_names [argmax(probabilities )]\\n#\\xa0Print\\xa0the\\xa0predicted\\xa0class\\xa0and\\xa0confidence\\npredicted_class\\xa0=\\xa0class_names [torch.argmax (probabilities )]\\nconfidence\\xa0=\\xa0torch. max(probabilities ).item()\\nprint(f\"This\\xa0image\\xa0most\\xa0likely\\xa0belongs\\xa0to\\xa0 {predicted_class }\\xa0with\\xa0a\\xa0 {confidence* 100:.2f}%\\xa0confidence.\" )\\nThis image most likely belongs to frog with a 57.76% confidence.\\nTest pr ediction on single image \\ue313\\n#\\xa0Load\\xa0and\\xa0preprocess\\xa0the\\xa0unseen\\xa0image\\nimage_path\\xa0=\\xa0 \\'frog.jpg\\' \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Replace\\xa0with\\xa0the\\xa0path\\xa0to\\xa0your\\xa0image\\nimage\\xa0=\\xa0Image. open(image_path )\\npreprocess\\xa0=\\xa0transforms.Compose ([\\n\\xa0\\xa0\\xa0\\xa0transforms.Resize (256),\\n\\xa0\\xa0\\xa0\\xa0transforms.CenterCrop (224),\\n\\xa0\\xa0\\xa0\\xa0transforms.ToTensor (),\\n\\xa0\\xa0\\xa0\\xa0transforms.Normalize ([0.485,\\xa00.456,\\xa00.406],\\xa0[0.229,\\xa00.224,\\xa00.225])\\n])\\ninput_tensor\\xa0=\\xa0preprocess (image)\\ninput_batch\\xa0=\\xa0input_tensor.unsqueeze (0)\\xa0\\xa0#\\xa0Add\\xa0a\\xa0batch\\xa0dimension\\n#\\xa0Perform\\xa0inference\\nwith\\xa0torch.no_grad ():\\n\\xa0\\xa0\\xa0\\xa0output\\xa0=\\xa0model (input_batch )\\n#\\xa0Get\\xa0the\\xa0predicted\\xa0class\\n_,\\xa0predicted_class\\xa0=\\xa0output. max(1)\\n#\\xa0Map\\xa0the\\xa0predicted\\xa0class\\xa0to\\xa0the\\xa0class\\xa0name\\npredicted_class_name\\xa0=\\xa0class_names [predicted_class.item ()]\\nprobabilities\\xa0=\\xa0torch.softmax (outputs,\\xa0dim=1)\\nconfidence\\xa0=\\xa0torch. max(probabilities ).item()\\nprint(f\"This\\xa0image\\xa0most\\xa0likely\\xa0belongs\\xa0to\\xa0 {predicted_class_name }\\xa0with\\xa0a\\xa0 {confidence* 100:.2f}%\\xa0confidence.\" )\\nThis image most likely belongs to frog with a 57.76% confidence.\\n#\\xa0Display\\xa0the\\xa0image\\xa0with\\xa0the\\xa0predicted\\xa0class\\xa0name\\nimage\\xa0=\\xa0np.array (image)\\nplt.imshow (image)\\nplt.axis (\\'off\\')\\nplt.text (10,\\xa010,\\xa0f\\'Predicted:\\xa0 {predicted_class_name }\\',\\xa0fontsize= 12,\\xa0color=\\'white\\',\\xa0backgroundcolor= \\'red\\')\\nplt.show ()5/5/24, 1:00 PM Jorge-Gosalvez_255_HW3 (1).ipynb - Colab\\nhttps://colab.research.google.com/drive/1P3xSSReWugbWYgunQEEK6ktGeMY3734S#printMode=true 8/14', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW3_CNN.pdf', 'page': 7}),\n",
              " Document(page_content='With input batch cr eated, cr eate function t o apply t o multiple images \\ue313\\ndef\\xa0check(image,\\xa0input_batch ):\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Load\\xa0and\\xa0preprocess\\xa0the\\xa0unseen\\xa0image\\n\\xa0\\xa0\\xa0\\xa0image_path\\xa0=\\xa0image\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Replace\\xa0with\\xa0the\\xa0path\\xa0to\\xa0your\\xa0image\\n\\xa0\\xa0\\xa0\\xa0image\\xa0=\\xa0Image. open(image_path )\\n\\xa0\\xa0\\xa0\\xa0preprocess\\xa0=\\xa0transforms.Compose ([\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.Resize (256),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.CenterCrop (224),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.ToTensor (),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.Normalize ([0.485,\\xa00.456,\\xa00.406],\\xa0[0.229,\\xa00.224,\\xa00.225])\\n\\xa0\\xa0\\xa0\\xa0])\\n\\xa0\\xa0\\xa0\\xa0input_tensor\\xa0=\\xa0preprocess (image)\\n\\xa0\\xa0\\xa0\\xa0input_batch\\xa0=\\xa0input_tensor.unsqueeze (0)\\xa0\\xa0#\\xa0Add\\xa0a\\xa0batch\\xa0dimension\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Perform\\xa0inference\\n\\xa0\\xa0\\xa0\\xa0with\\xa0torch.no_grad ():\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0output\\xa0=\\xa0model (input_batch )\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Get\\xa0the\\xa0predicted\\xa0class\\n\\xa0\\xa0\\xa0\\xa0_,\\xa0predicted_class\\xa0=\\xa0output. max(1)\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Map\\xa0the\\xa0predicted\\xa0class\\xa0to\\xa0the\\xa0class\\xa0name\\n\\xa0\\xa0\\xa0\\xa0predicted_class_name\\xa0=\\xa0class_names [predicted_class.item ()]\\n\\xa0\\xa0\\xa0\\xa0probabilities\\xa0=\\xa0torch.softmax (outputs,\\xa0dim=1)\\n\\xa0\\xa0\\xa0\\xa0confidence\\xa0=\\xa0torch. max(probabilities ).item()\\n\\xa0\\xa0\\xa0\\xa0print(f\"This\\xa0image\\xa0most\\xa0likely\\xa0belongs\\xa0to\\xa0 {predicted_class_name }\\xa0with\\xa0a\\xa0 {confidence* 100:.2f}%\\xa0confidence.\" )\\n\\xa0\\xa0\\xa0\\xa0print()\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Display\\xa0the\\xa0image\\xa0with\\xa0the\\xa0predicted\\xa0class\\xa0name\\n\\xa0\\xa0\\xa0\\xa0image\\xa0=\\xa0np.array (image)\\n\\xa0\\xa0\\xa0\\xa0plt.imshow (image)\\n\\xa0\\xa0\\xa0\\xa0plt.axis (\\'off\\')\\n\\xa0\\xa0\\xa0\\xa0plt.text (10,\\xa010,\\xa0f\\'Predicted:\\xa0 {predicted_class_name }\\',\\xa0fontsize= 12,\\xa0color=\\'white\\',\\xa0backgroundcolor= \\'red\\')\\n\\xa0\\xa0\\xa0\\xa0plt.show ()\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0return\\xa0input_batch\\nnew_images\\xa0=\\xa0 [\\'frog.jpg\\' ,\\xa0\\'airplane.jpg\\' ,\\xa0\\'bird.jpg\\' ,\\xa0\\'frog2.jpg\\' ]\\nfor\\xa0i\\xa0in\\xa0new_images :\\n\\xa0\\xa0\\xa0\\xa0check (i,\\xa0input_batch )5/5/24, 1:00 PM Jorge-Gosalvez_255_HW3 (1).ipynb - Colab\\nhttps://colab.research.google.com/drive/1P3xSSReWugbWYgunQEEK6ktGeMY3734S#printMode=true 9/14', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW3_CNN.pdf', 'page': 8}),\n",
              " Document(page_content='This image most likely belongs to frog with a 57.76% confidence.\\nThis image most likely belongs to airplane with a 57.76% confidence.\\nThis image most likely belongs to bird with a 57.76% confidence.\\nThis image most likely belongs to airplane with a 57.76% confidence.\\n\\x005/5/24, 1:00 PM Jorge-Gosalvez_255_HW3 (1).ipynb - Colab\\nhttps://colab.research.google.com/drive/1P3xSSReWugbWYgunQEEK6ktGeMY3734S#printMode=true 10/14', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW3_CNN.pdf', 'page': 9}),\n",
              " Document(page_content=\"Event with augmenting data t o incr ease accur acy and con\\x00dence.\\nImages must be same siz e (e.g. uniform or pr ocessed t o be uniform. F or example, tr anspar ent images can cause challenges)\\nNow with a pr etrained model...\\n6. Use GoogleNet (InceptionNet) and add a LinearLa yer \\ue313\\nUnsuppor ted Cell Type. Double-Click t o inspect/edit the content.\\nExperiment b y creating a local cop y of data \\ue313\\n#\\xa0Define\\xa0the\\xa0data\\xa0directory\\xa0\\xa0#\\xa0train_dataloader\\ndata_dir\\xa0=\\xa0 './data'\\n#\\xa0Create\\xa0a\\xa0directory\\xa0to\\xa0save\\xa0the\\xa0images\\xa0if\\xa0it\\xa0does n't\\xa0exist\\nsave_dir\\xa0=\\xa0os.path.join (data_dir ,\\xa0'train')\\nos.makedirs (save_dir ,\\xa0exist_ok= True)\\nfor\\xa0i,\\xa0(image,\\xa0label)\\xa0in\\xa0enumerate (train_data ):\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Remove\\xa0the\\xa0batch\\xa0dimension\\xa0if\\xa0it\\xa0exists\\n\\xa0\\xa0\\xa0\\xa0image\\xa0=\\xa0image.squeeze (0)\\xa0if\\xa0len(image.size ())\\xa0==\\xa04\\xa0else\\xa0image\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Convert\\xa0the\\xa0tensor\\xa0image\\xa0to\\xa0PIL\\xa0image\\n\\xa0\\xa0\\xa0\\xa0pil_image\\xa0=\\xa0transforms.ToPILImage ()(image)\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Define\\xa0the\\xa0subdirectory\\xa0based\\xa0on\\xa0the\\xa0label\\n\\xa0\\xa0\\xa0\\xa0label_dir\\xa0=\\xa0os.path.join (save_dir ,\\xa0str(label))\\n\\xa0\\xa0\\xa0\\xa0os.makedirs (label_dir ,\\xa0exist_ok= True)\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Define\\xa0the\\xa0filename\\xa0for\\xa0saving\\n\\xa0\\xa0\\xa0\\xa0filename\\xa0=\\xa0os.path.join (label_dir ,\\xa0f'image_{i}.jpg')\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Save\\xa0the\\xa0PIL\\xa0image\\xa0as\\xa0a\\xa0JPEG\\xa0file\\n\\xa0\\xa0\\xa0\\xa0pil_image.save (filename )5/5/24, 1:00 PM Jorge-Gosalvez_255_HW3 (1).ipynb - Colab\\nhttps://colab.research.google.com/drive/1P3xSSReWugbWYgunQEEK6ktGeMY3734S#printMode=true 11/14\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW3_CNN.pdf', 'page': 10}),\n",
              " Document(page_content=\"#\\xa0Create\\xa0a\\xa0val\\xa0directory\\xa0with\\xa0sub\\xa0folders\\xa0for\\xa03\\xa0cl asses\\xa0to\\xa0save\\xa0the\\xa0images\\xa0if\\xa0it\\xa0doesn't\\xa0exist\\nsave_dir\\xa0=\\xa0os.path.join (data_dir ,\\xa0'val')\\nos.makedirs (save_dir ,\\xa0exist_ok= True)\\nfor\\xa0i,\\xa0(image,\\xa0label)\\xa0in\\xa0enumerate (valset):\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Remove\\xa0the\\xa0batch\\xa0dimension\\xa0if\\xa0it\\xa0exists\\n\\xa0\\xa0\\xa0\\xa0image\\xa0=\\xa0image.squeeze (0)\\xa0if\\xa0len(image.size ())\\xa0==\\xa04\\xa0else\\xa0image\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0#for\\xa0i\\xa0in\\xa0range(3):\\n\\xa0\\xa0\\xa0\\xa0#\\xa0\\xa0\\xa0\\xa0print(label)\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Convert\\xa0the\\xa0tensor\\xa0image\\xa0to\\xa0PIL\\xa0image\\n\\xa0\\xa0\\xa0\\xa0pil_image\\xa0=\\xa0transforms.ToPILImage ()(image)\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Define\\xa0the\\xa0subdirectory\\xa0based\\xa0on\\xa0the\\xa0label\\n\\xa0\\xa0\\xa0\\xa0label_dir\\xa0=\\xa0os.path.join (save_dir ,\\xa0str(label))\\n\\xa0\\xa0\\xa0\\xa0os.makedirs (label_dir ,\\xa0exist_ok= True)\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Define\\xa0the\\xa0filename\\xa0for\\xa0saving\\n\\xa0\\xa0\\xa0\\xa0filename\\xa0=\\xa0os.path.join (label_dir ,\\xa0f'image_{i}.jpg')\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Save\\xa0the\\xa0PIL\\xa0image\\xa0as\\xa0a\\xa0JPEG\\xa0file\\n\\xa0\\xa0\\xa0\\xa0pil_image.save (filename )\\n#\\xa0Define\\xa0data\\xa0transformations\\xa0for\\xa0data\\xa0augmentatio n\\xa0and\\xa0normalization\\ndata_transforms\\xa0=\\xa0 {\\n\\xa0\\xa0\\xa0\\xa0'train':\\xa0transforms.Compose ([\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.RandomResizedCrop (299),\\xa0\\xa0\\xa0\\xa0#\\xa0resize\\xa0for\\xa0inception\\xa0v3\\xa0<<<<\\xa0required!\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.RandomHorizontalFlip (),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.ToTensor (),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.Normalize ([0.485,\\xa00.456,\\xa00.406],\\xa0[0.229,\\xa00.224,\\xa00.225])\\n\\xa0\\xa0\\xa0\\xa0]),\\n\\xa0\\xa0\\xa0\\xa0'val':\\xa0transforms.Compose ([\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.Resize (299),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.CenterCrop (299),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.ToTensor (),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.Normalize ([0.485,\\xa00.456,\\xa00.406],\\xa0[0.229,\\xa00.224,\\xa00.225])\\n\\xa0\\xa0\\xa0\\xa0]),\\n}\\n#\\xa0Create\\xa0data\\xa0loaders\\nimage_datasets\\xa0=\\xa0 {x:\\xa0datasets.ImageFolder (os.path.join (data_dir ,\\xa0x),\\xa0data_transforms [x])\\xa0for\\xa0x\\xa0in\\xa0['train',\\xa0'val']}\\nimage_datasets\\n{'train': Dataset ImageFolder\\n     Number of datapoints: 4968\\n     Root location: ./data/train\\n     StandardTransform\\n Transform: Compose(\\n                RandomResizedCrop(size=(299, 299), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear,  \\nantialias=True)\\n                RandomHorizontalFlip(p=0.5)\\n                ToTensor()\\n                Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\\n            ),\\n 'val': Dataset ImageFolder\\n     Number of datapoints: 1232\\n     Root location: ./data/val\\n     StandardTransform\\n Transform: Compose(\\n                Resize(size=299, interpolation=bilinear, max_size=None, antialias=True)\\n                CenterCrop(size=(299, 299))\\n                ToTensor()\\n                Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\\n            )}\\ndataloaders\\xa0\\xa0\\xa0=\\xa0 {x:\\xa0DataLoader (image_datasets [x],\\xa0batch_size= 4,\\xa0shuffle= True,\\xa0num_workers= 4)\\xa0for\\xa0x\\xa0in\\xa0['train',\\xa0'val']}\\ndataset_sizes\\xa0=\\xa0 {x:\\xa0len(image_datasets [x])\\xa0for\\xa0x\\xa0in\\xa0['train',\\xa0'val']}\\nprint(dataset_sizes )\\nclass_names\\xa0=\\xa0image_datasets ['train'].classes\\nclass_names\\n{'train': 4968, 'val': 1232}\\n['0', '2', '6']5/5/24, 1:00 PM Jorge-Gosalvez_255_HW3 (1).ipynb - Colab\\nhttps://colab.research.google.com/drive/1P3xSSReWugbWYgunQEEK6ktGeMY3734S#printMode=true 12/14\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW3_CNN.pdf', 'page': 11}),\n",
              " Document(page_content='#\\xa0Load\\xa0the\\xa0pre-trained\\xa0ResNet-18\\xa0model\\xa0-\\xa0works\\xa0b/c \\xa0kernel\\xa0and\\xa0input\\xa0sizes\\xa0fit\\xa03x3\\n#\\xa0model\\xa0=\\xa0models.resnet18(pretrained=True)\\n#\\xa0Load\\xa0the\\xa0pre-trained\\xa0inception_v3\\xa0model\\xa0-\\xa0does\\xa0n ot\\xa0work\\xa0b/c\\xa0kernel\\xa0is\\xa05x5\\xa0versus\\xa03x3\\xa0inputs,\\xa0need\\xa0 to\\xa0set\\xa0final\\xa0\\nmodel\\xa0=\\xa0models.inception_v3 (pretrained= True)\\xa0\\xa0\\xa0\\nmodel.aux_logits\\xa0=\\xa0 False\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0inception\\xa0v3\\xa0\\n#\\xa0print(model)\\n/opt/anaconda3/envs/python311/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter \\'pre\\n  warnings.warn(\\n/opt/anaconda3/envs/python311/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other th\\n  warnings.warn(msg)\\n#\\xa0Freeze\\xa0all\\xa0layers\\xa0except\\xa0the\\xa0final\\xa0classificatio n\\xa0layer\\nfor\\xa0name,\\xa0param\\xa0in\\xa0model.named_parameters ():\\n\\xa0\\xa0\\xa0\\xa0if\\xa0\"fc\"\\xa0in\\xa0name:\\xa0\\xa0#\\xa0Unfreeze\\xa0the\\xa0final\\xa0classification\\xa0layer\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0param.requires_grad\\xa0=\\xa0 True\\n\\xa0\\xa0\\xa0\\xa0else:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0param.requires_grad\\xa0=\\xa0 False\\n#\\xa0Define\\xa0the\\xa0loss\\xa0function\\xa0and\\xa0optimizer\\ncriterion\\xa0=\\xa0nn.CrossEntropyLoss ()\\noptimizer\\xa0=\\xa0optim.SGD (model.parameters (),\\xa0lr=0.001,\\xa0momentum= 0.9)\\xa0\\xa0#\\xa0Use\\xa0all\\xa0parameters\\n#\\xa0Move\\xa0the\\xa0model\\xa0to\\xa0the\\xa0GPU\\xa0if\\xa0available\\ndevice\\xa0=\\xa0torch.device (\"cuda:0\" \\xa0if\\xa0torch.cuda.is_available ()\\xa0else\\xa0\"cpu\")\\nmodel\\xa0=\\xa0model.to (device)\\n%%time\\n#\\xa0Training\\xa0loop\\nnew_acc\\xa0\\xa0\\xa0\\xa0=\\xa0 0.0\\nnew_acc\\xa0\\xa0\\xa0\\xa0=\\xa0 0.0\\nnum_epochs\\xa0=\\xa0 3\\nfor\\xa0epoch\\xa0in\\xa0range(num_epochs ):\\n\\xa0\\xa0\\xa0\\xa0for\\xa0phase\\xa0in\\xa0[\\'train\\',\\xa0\\'val\\']:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\xa0phase\\xa0==\\xa0 \\'train\\':\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0model.train ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 else:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0model. eval()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0running_loss\\xa0=\\xa0 0.0\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0running_corrects\\xa0=\\xa0 0\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for\\xa0inputs,\\xa0labels\\xa0 in\\xa0dataloaders [phase]:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0inputs\\xa0=\\xa0inputs.to (device)\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0inception\\xa0v3\\xa0requires\\xa0images\\xa0of\\xa0size\\xa0299x299,\\xa0wh ich\\xa0is\\xa0done\\xa0in\\xa0the\\xa0encoder/d\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0labels\\xa0=\\xa0labels.to (device)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #print(inputs.size())\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0optimizer.zero_grad ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 with\\xa0torch.set_grad_enabled (phase\\xa0==\\xa0 \\'train\\'):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0outputs\\xa0=\\xa0model (inputs)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0_ ,\\xa0preds\\xa0=\\xa0torch. max(outputs,\\xa01)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0loss\\xa0=\\xa0criterion (outputs,\\xa0labels)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\xa0phase\\xa0==\\xa0 \\'train\\':\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0loss.backward ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0optimizer.step ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0running_loss\\xa0+=\\xa0loss.item ()\\xa0*\\xa0inputs.size (0)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0running_corrects\\xa0+=\\xa0torch. sum(preds\\xa0==\\xa0labels.data )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0epoch_loss\\xa0=\\xa0running_loss\\xa0/\\xa0dataset_sizes [phase]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0epoch_acc\\xa0\\xa0=\\xa0running_corrects.double ()\\xa0/\\xa0dataset_sizes [phase]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 print(f\\'{phase}\\xa0Loss:\\xa0{epoch_loss :.4f}\\xa0Acc:\\xa0{epoch_acc :.4f}\\')\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0new_acc\\xa0\\xa0=\\xa0epoch_acc\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0new_loss\\xa0=\\xa0epoch_loss\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nprint(f\\'Transfer\\xa0Learning\\xa0Model\\xa0Final:\\xa0loss\\xa0 {new_loss :.4f}\\xa0|\\xa0accuracy\\xa0 {new_acc:.4f}\\')\\nprint(\"Training\\xa0complete!\" )\\ntrain Loss: 1.1920 Acc: 0.4605\\nval Loss: 0.9323 Acc: 0.5771\\ntrain Loss: 1.1299 Acc: 0.50525/5/24, 1:00 PM Jorge-Gosalvez_255_HW3 (1).ipynb - Colab\\nhttps://colab.research.google.com/drive/1P3xSSReWugbWYgunQEEK6ktGeMY3734S#printMode=true 13/14', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW3_CNN.pdf', 'page': 12}),\n",
              " Document(page_content='val Loss: 0.8110 Acc: 0.6356\\ntrain Loss: 1.1087 Acc: 0.5139\\nval Loss: 0.9193 Acc: 0.6144\\nTransfer Learning Model Final: loss 0.9193 | accuracy 0.6144\\nTraining complete!\\nCPU times: user 2h 7min 39s, sys: 9min 45s, total: 2h 17min 24s\\nWall time: 1h 12min 57s\\n#\\xa0Save\\xa0the\\xa0model\\ntorch.save (model.state_dict (),\\xa0\\'3_class_CIFAR_classification_model.pth\\' )\\n#\\xa0Load\\xa0the\\xa0saved\\xa0modelmodel\\xa0=\\xa0models.inception_v3( pretrained=True)\\nmodel.fc\\xa0=\\xa0nn.Linear (model.fc.in_features ,\\xa01000)\\xa0\\xa0#\\xa0Adjust\\xa0to\\xa0match\\xa0the\\xa0original\\xa0model\\'s\\xa0output\\xa0unit s\\nmodel.load_state_dict (torch.load (\\'3_class_CIFAR_classification_model.pth\\' ))\\nmodel.eval()\\n#\\xa0Create\\xa0a\\xa0new\\xa0model\\xa0with\\xa0the\\xa0correct\\xa0final\\xa0layer\\n#new_model\\xa0=\\xa0models.resnet18(pretrained=True)\\nnew_model\\xa0=\\xa0models.inception_v3 (pretrained= True)\\nnew_model.fc\\xa0=\\xa0nn.Linear (new_model.fc.in_features ,\\xa02)\\xa0\\xa0#\\xa0Adjust\\xa0to\\xa0match\\xa0the\\xa0desired\\xa0output\\xa0units\\n#\\xa0Copy\\xa0the\\xa0weights\\xa0and\\xa0biases\\xa0from\\xa0the\\xa0loaded\\xa0mode l\\xa0to\\xa0the\\xa0new\\xa0model\\nnew_model.fc.weight.data\\xa0=\\xa0model.fc.weight.data [0:2]\\xa0\\xa0#\\xa0Copy\\xa0only\\xa0the\\xa0first\\xa02\\xa0output\\xa0units\\nnew_model.fc.bias.data\\xa0\\xa0\\xa0=\\xa0model.fc.bias.data [0:2]\\ndef\\xa0check_new (image,\\xa0input_batch ):\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Load\\xa0and\\xa0preprocess\\xa0the\\xa0unseen\\xa0image\\n\\xa0\\xa0\\xa0\\xa0image_path\\xa0=\\xa0image\\xa0\\xa0 #\\xa0Replace\\xa0with\\xa0the\\xa0path\\xa0to\\xa0your\\xa0image\\n\\xa0\\xa0\\xa0\\xa0image\\xa0=\\xa0Image. open(image_path )\\n\\xa0\\xa0\\xa0\\xa0preprocess\\xa0=\\xa0transforms.Compose ([\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.Resize (299),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.CenterCrop (299),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.ToTensor (),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0transforms.Normalize ([0.485,\\xa00.456,\\xa00.406],\\xa0[0.229,\\xa00.224,\\xa00.225])\\n\\xa0\\xa0\\xa0\\xa0])\\n\\xa0\\xa0\\xa0\\xa0input_tensor\\xa0=\\xa0preprocess (image)\\n\\xa0\\xa0\\xa0\\xa0input_batch\\xa0=\\xa0input_tensor.unsqueeze (0)\\xa0\\xa0#\\xa0Add\\xa0a\\xa0batch\\xa0dimension\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Perform\\xa0inference\\n\\xa0\\xa0\\xa0\\xa0with\\xa0torch.no_grad ():\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0output\\xa0=\\xa0new_model (input_batch )\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Get\\xa0the\\xa0predicted\\xa0class\\n\\xa0\\xa0\\xa0\\xa0_,\\xa0predicted_class\\xa0=\\xa0output. max(1)\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Map\\xa0the\\xa0predicted\\xa0class\\xa0to\\xa0the\\xa0class\\xa0name\\n\\xa0\\xa0\\xa0\\xa0class_names\\xa0=\\xa0 [\\'0\\',\\xa0\\'2\\',\\xa0\\'6\\']\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0#\\xa0Make\\xa0sure\\xa0these\\xa0class\\xa0names\\xa0match\\xa0your\\xa0training\\xa0 data\\n\\xa0\\xa0\\xa0\\xa0predicted_class_name\\xa0=\\xa0class_names [predicted_class.item ()]\\n\\xa0\\xa0\\xa0\\xa0probabilities\\xa0=\\xa0torch.softmax (outputs,\\xa0dim=1)\\n\\xa0\\xa0\\xa0\\xa0confidence\\xa0=\\xa0torch. max(probabilities ).item()\\n\\xa0\\xa0\\xa0\\xa0print(f\"This\\xa0image\\xa0most\\xa0likely\\xa0belongs\\xa0to\\xa0 {predicted_class_name }\\xa0with\\xa0a\\xa0 {confidence* 100:.2f}%\\xa0confidence.\" )\\n\\xa0\\xa0\\xa0\\xa0print()\\n\\xa0\\xa0\\xa0\\xa0for\\xa0i\\xa0in\\xa0predicted_class_name :\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\xa0i\\xa0==\\xa0\\'0\\':\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0predicted_class_name_text\\xa0=\\xa0 \\'airplane\\'\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\xa0i\\xa0==\\xa0\\'2\\':\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0predicted_class_name_text\\xa0=\\xa0 \\'bird\\'\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\xa0i\\xa0==\\xa0\\'6\\':\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0predicted_class_name_text\\xa0=\\xa0 \\'frog\\'\\n\\xa0\\xa0\\xa0\\xa0print(f\\'The\\xa0predicted\\xa0class\\xa0is:\\xa0 {predicted_class_name }\\')\\n\\xa0\\xa0\\xa0\\xa0print(f\\'The\\xa0predicted\\xa0class\\xa0is:\\xa0 {predicted_class_name_text }\\')\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Display\\xa0the\\xa0image\\xa0with\\xa0the\\xa0predicted\\xa0class\\xa0name\\n\\xa0\\xa0\\xa0\\xa0image\\xa0=\\xa0np.array (image)\\n\\xa0\\xa0\\xa0\\xa0plt.imshow (image)\\n\\xa0\\xa0\\xa0\\xa0plt.axis (\\'off\\')\\n\\xa0\\xa0\\xa0\\xa0plt.text (10,\\xa010,\\xa0f\\'Predicted:\\xa0 {predicted_class_name },\\xa0which\\xa0is\\xa0 {predicted_class_name_text }\\',\\xa0fontsize= 12,\\xa0color=\\'white\\',\\xa0bac\\n\\xa0\\xa0\\xa0\\xa0plt.show ()\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0return\\xa0input_batch5/5/24, 1:00 PM Jorge-Gosalvez_255_HW3 (1).ipynb - Colab\\nhttps://colab.research.google.com/drive/1P3xSSReWugbWYgunQEEK6ktGeMY3734S#printMode=true 14/14', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW3_CNN.pdf', 'page': 13}),\n",
              " Document(page_content=\"Homework 04: Transf er Learning and Bounding Bo xes and Y OLOV8SJSU MSDS 255 DL, Spring 2024\\ue313\\nGit: https:/ /github.com/jr gosalv ez/data255_DL\\nNO TE: YOLOv8 r elies on P yTorch as its deep learning fr amework. NVIDI A NGC Catalog off ers additional GPU optimiz ed fr ameworks for\\nalternativ e transf er learning models, fr ameworks, and containers (e.g. NeMO).\\nCollect a sour ce video.\\nYOLO8 accepts the following video formats: .asf, .a vi, .gif, .m4v , .mkv , .mo v, .mp4, .mpeg, .mpg, .ts, .wmv , .webm.\\niphone video cr eates .mp4 videos; I cr eated 5-10 sec str eet videos and sa ved them locally .\\nYOLO8 can classify , detect, and or segment with nano, small, medium, lar ge, or huge pr e-trained models; I use small detection for this\\nexperimentStep 1:\\nConduct inf erence on video, fr ame b y frame, dr awing bounding bo xes ar ound detected objects (speci\\x00cally v ehicles) and output a video of the\\nobject detection r esults.Steps 2 & 3:\\ue313\\n#\\xa0check\\xa0mac\\xa0systems\\xa0for\\xa0GPU\\nimport\\xa0torch\\nprint(torch.backends.mps.is_available ())\\xa0\\xa0\\xa0#\\xa0check\\xa0for\\xa0Mac\\xa0M1\\xa0GPU\\nFalse\\n#\\xa0import\\xa0openCV,\\xa0numpy,\\xa0and\\xa0ultralytics\\xa0YOLO8\\nimport\\xa0cv2\\nfrom\\xa0ultralytics\\xa0 import\\xa0YOLO\\nimport\\xa0numpy\\xa0as\\xa0np\\nPart 2\\ue313\\nFine Tune Object Detection fr om 10 Cust om IM AGES with Y OLO8 Transf er Learning \\ue313\\nSour ce:\\nhttps:/ /univ erse.r obo\\x00ow .com/y olo-a6y21/squid-bat-butter\\x00y\\nhttps:/ /blog.r obo\\x00ow .com/how-t o-train-y olov8-on-a-cust om-dataset/\\nhttps:/ /www .freecodecamp.or g/news/how-t o-detect-objects-in-images-using-y olov8/\\nhttps:/ /medium.com/@pat.x.guillen/a-step-b y-step-guide-t o-running-y olov8-on-windows-122cb586b567\\nRepeat Steps 1-3 but for images on 10 images of the Robo\\x00ow squid-bat-butter\\x00y datasets.\\nPrep cust om Robo\\x00ow squid-bat-butter\\x00y dataset \\ue313\\nUse r obo\\x00ow t o prepar e dataset and annotate. NO TE: can do it online with Robo\\x00ow work\\x00ow low-code work\\x00ow t ools or 'Cust om Train and\\nUpload' t o get Y OLOv8 code snippet t o do manually .\\nfrom\\xa0IPython\\xa0 import\\xa0display\\ndisplay.clear_output ()\\nfrom\\xa0IPython.display\\xa0 import\\xa0display ,\\xa0Image5/5/24, 1:02 PM Part_2_Jorge-Gosalvez_255_HW4.ipynb - Colab\\nhttps://colab.research.google.com/drive/1Cnfp7efAdIjABAjqlTbOWh4Lu_DvdCCT#printMode=true 1/7\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW4_ObjDetect_YOLO.pdf', 'page': 0}),\n",
              " Document(page_content='Get Cust om Dataset fr om Robo\\x00ow \\ue313\\n!yolo\\xa0task=detect\\xa0mode=predict\\xa0model=yolov8s .pt\\xa0conf= 0.25\\xa0source= \\'Part_2_Jorge_Gosalvez_255_boston_dog.jpeg\\' \\xa0save=True\\nUltralytics YOLOv8.1.17 🚀  Python-3.11.5 torch-2.2.0 CPU (Intel Core(TM) i5-8210Y 1.60GHz)\\nYOLOv8s summary (fused): 168 layers, 11156544 parameters, 0 gradients, 28.6 GFLOPs\\nimage 1/1 /Users/gosalvez/Desktop/Git/python_practice/python_practice/HW/DATA255_DL_SPRING2024/HW4/Part_2_Jorge_Gosalvez_255\\nSpeed: 3.2ms preprocess, 341.9ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\\nResults saved to runs/detect/predict\\n💡 Learn more at https://docs.ultralytics.com/modes/predict\\nTip: The \\x00rst time y ou run the pr ediction tr ain and pr edict folders ar e created. Subsequent runs append a # t o the folder . Be sur e to update the \\x00le path corr ectly t o\\nfetch the corr ect r esults.\\nImage(filename= \\'runs/detect/predict/Part_2_Jorge_Gosalvez_255_bos ton_dog.jpeg\\' ,\\xa0height= 400)\\n!pip\\xa0install\\xa0roboflow\\nfrom\\xa0roboflow\\xa0 import\\xa0Roboflow\\nrf\\xa0=\\xa0Roboflow (api_key= \"ytVnstTGTkiOHxfkYXsM\" )\\nproject\\xa0=\\xa0rf.workspace (\"sjsu-dl-255-squidbatbutterfly\" ).project (\"transfer-learning-and-fine-tune\" )\\ndataset\\xa0=\\xa0project.version (2).download (\"yolov8\" )\\nRequirement already satisfied: roboflow in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (1.1.19)\\nRequirement already satisfied: certifi==2023.7.22 in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from robofl\\nRequirement already satisfied: chardet==4.0.0 in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from roboflow) \\nRequirement already satisfied: cycler==0.10.0 in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from roboflow) \\nRequirement already satisfied: idna==2.10 in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from roboflow) (2.1\\nRequirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from roboflo\\nRequirement already satisfied: matplotlib in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from roboflow) (3.7\\nRequirement already satisfied: numpy>=1.18.5 in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from roboflow) (\\nRequirement already satisfied: opencv-python-headless==4.8.0.74 in /opt/anaconda3/envs/python311/lib/python3.11/site-package\\nRequirement already satisfied: Pillow>=7.1.2 in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from roboflow) (\\nRequirement already satisfied: python-dateutil in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from roboflow)\\nRequirement already satisfied: python-dotenv in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from roboflow) (\\nRequirement already satisfied: requests in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from roboflow) (2.31.\\nRequirement already satisfied: six in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from roboflow) (1.16.0)\\nRequirement already satisfied: supervision in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from roboflow) (0.\\nRequirement already satisfied: urllib3>=1.26.6 in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from roboflow)\\nRequirement already satisfied: tqdm>=4.41.0 in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from roboflow) (4\\nRequirement already satisfied: PyYAML>=5.3.1 in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from roboflow) (\\nRequirement already satisfied: requests-toolbelt in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from roboflo\\nRequirement already satisfied: python-magic in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from roboflow) (0\\nRequirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from matplotl\\nRequirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from matplot\\nRequirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from matplotli\\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from matplotl\\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from \\nRequirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from \\nRequirement already satisfied: scipy<2.0.0,>=1.10.0 in /opt/anaconda3/envs/python311/lib/python3.11/site-packages (from supe\\nloading Roboflow workspace...\\nloading Roboflow project...\\nDependency ultralytics==8.0.196 is required but found version=8.1.17, to fix: `pip install ultralytics==8.0.196`5/5/24, 1:02 PM Part_2_Jorge-Gosalvez_255_HW4.ipynb - Colab\\nhttps://colab.research.google.com/drive/1Cnfp7efAdIjABAjqlTbOWh4Lu_DvdCCT#printMode=true 2/7', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW4_ObjDetect_YOLO.pdf', 'page': 1}),\n",
              " Document(page_content=\"Cust om Train on Y OLOv8 using Robo\\x00ow pr epar ed squid-bat-butter\\x00y dataset \\ue313\\nNO TE: Robo\\x00ow outputs a data.location path defaulted t o your pr oject name; howe ver, it repeats it twice in the data.y aml \\x00le and should be corr ected t o match the\\n../test/images  path as t o avoid duplicating the dataset folder name twice in the data.y aml \\x00le for tr ain and v al paths t o data.\\n!yolo\\xa0task=detect\\xa0mode=train\\xa0model=yolov8s .pt\\xa0data= {dataset.location }/data.yaml\\xa0epochs= 12\\xa0imgsz=800\\xa0plots=True\\nNew https://pypi.org/project/ultralytics/8.1.18  available 😃  Update with 'pip install -U ultralytics'\\nUltralytics YOLOv8.1.17 🚀  Python-3.11.5 torch-2.2.0 CPU (Intel Core(TM) i5-8210Y 1.60GHz)\\nengine/trainer: task=detect, mode=train, model=yolov8s.pt, data=/Users/gosalvez/Desktop/Git/python_practice/python_practice/\\nOverriding model.yaml nc=80 with nc=3\\n                   from  n    params  module                                       arguments                     \\n  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \\n  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \\n  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \\n  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \\n  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \\n  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \\n  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \\n  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \\n  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \\n  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \\n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \\n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \\n 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \\n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \\n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \\n 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \\n 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \\n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \\n 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \\n 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \\n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \\n 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \\n 22        [15, 18, 21]  1   2117209  ultralytics.nn.modules.head.Detect           [3, [128, 256, 512]]          \\nModel summary: 225 layers, 11136761 parameters, 11136745 gradients, 28.7 GFLOPs\\nTransferred 349/355 items from pretrained weights\\nTensorBoard: Start with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\\nFreezing layer 'model.22.dfl.conv.weight'\\ntrain: Scanning /Users/gosalvez/Desktop/Git/python_practice/python_practice/HW/D\\nval: Scanning /Users/gosalvez/Desktop/Git/python_practice/python_practice/HW/DAT\\nPlotting labels to runs/detect/train/labels.jpg... \\noptimizer:  'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'mom\\noptimizer:  AdamW(lr=0.001429, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(dec\\nTensorBoard: model graph visualization added ✅\\nImage sizes 800 train, 800 val\\nUsing 0 dataloader workers\\nLogging results to runs/detect/train\\nStarting training for 12 epochs...\\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\\n       1/12         0G      1.643      3.517      2.141          4        800: 1\\n                 Class     Images  Instances      Box(P          R      mAP50  mWARNING ⚠  NMS time limit 3.150s exceeded\\n                 Class     Images  Instances      Box(P          R      mAP50  m\\n                   all         23         24      0.727      0.169      0.188      0.124\\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\\n       2/12         0G      1.192      2.366      1.813          3        800: 1\\n                 Class     Images  Instances      Box(P          R      mAP50  mWARNING ⚠  NMS time limit 3.150s exceeded\\n                 Class     Images  Instances      Box(P          R      mAP50  m\\n                   all         23         24      0.628      0.615      0.661      0.322\\nClosing dataloader mosaic\\nEhGPU bl lldfllI Si\\n!ls\\xa0runs/detect/train\\nF1_curve.png\\nPR_curve.png\\nP_curve.png\\nR_curve.png\\nargs.yaml\\nconfusion_matrix.png\\nconfusion_matrix_normalized.png\\nevents.out.tfevents.1708901625.Ricks-MacBook-Air-2.local.51669.0\\nlabels.jpg\\nlabels_correlogram.jpg\\nresults.csv\\nresults.png5/5/24, 1:02 PM Part_2_Jorge-Gosalvez_255_HW4.ipynb - Colab\\nhttps://colab.research.google.com/drive/1Cnfp7efAdIjABAjqlTbOWh4Lu_DvdCCT#printMode=true 3/7\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW4_ObjDetect_YOLO.pdf', 'page': 2}),\n",
              " Document(page_content=\"train_batch0.jpg\\ntrain_batch1.jpg\\ntrain_batch16.jpg\\ntrain_batch17.jpg\\ntrain_batch18.jpg\\ntrain_batch2.jpg\\nval_batch0_labels.jpg\\nval_batch0_pred.jpg\\nweights\\nImage(filename= f'runs/detect/train/confusion_matrix.png' ,\\xa0width=800)\\nImage(filename= f'runs/detect/train/F1_curve.png' ,\\xa0width=800)5/5/24, 1:02 PM Part_2_Jorge-Gosalvez_255_HW4.ipynb - Colab\\nhttps://colab.research.google.com/drive/1Cnfp7efAdIjABAjqlTbOWh4Lu_DvdCCT#printMode=true 4/7\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW4_ObjDetect_YOLO.pdf', 'page': 3}),\n",
              " Document(page_content=\"Image(filename= f'runs/detect/train/results.png' ,\\xa0width=800)\\nImage(filename= f'runs/detect/train/val_batch0_pred.jpg' ,\\xa0width=800)\\n!yolo\\xa0task=detect\\xa0mode=val\\xa0model=runs/detect/train/ weights/best .pt\\xa0data= {dataset.location }/data.yaml\\nUltralytics YOLOv8.1.17 🚀  Python-3.11.5 torch-2.2.0 CPU (Intel Core(TM) i5-8210Y 1.60GHz)\\nModel summary (fused): 168 layers, 11126745 parameters, 0 gradients, 28.4 GFLOPs\\nval: Scanning /Users/gosalvez/Desktop/Git/python_practice/python_practice/HW/DAT\\n                 Class     Images  Instances      Box(P          R      mAP50  m\\n                   all         23         24      0.924      0.889      0.925      0.6015/5/24, 1:02 PM Part_2_Jorge-Gosalvez_255_HW4.ipynb - Colab\\nhttps://colab.research.google.com/drive/1Cnfp7efAdIjABAjqlTbOWh4Lu_DvdCCT#printMode=true 5/7\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW4_ObjDetect_YOLO.pdf', 'page': 4}),\n",
              " Document(page_content='                   Bat         23          6      0.903      0.667      0.831      0.443\\n             Butterfly         23          8      0.965          1      0.995      0.515\\n                 Squid         23         10      0.904          1       0.95      0.845\\nSpeed: 7.7ms preprocess, 969.1ms inference, 0.0ms loss, 2.2ms postprocess per image\\nResults saved to runs/detect/val\\n💡 Learn more at https://docs.ultralytics.com/modes/val\\n!yolo\\xa0task=detect\\xa0mode=predict\\xa0model=runs/detect/tr ain/weights/best .pt\\xa0conf= 0.25\\xa0source= {dataset.location }/test/images\\xa0save=True\\nUltralytics YOLOv8.1.17 🚀  Python-3.11.5 torch-2.2.0 CPU (Intel Core(TM) i5-8210Y 1.60GHz)\\nModel summary (fused): 168 layers, 11126745 parameters, 0 gradients, 28.4 GFLOPs\\nimage 1/10 /Users/gosalvez/Desktop/Git/python_practice/python_practice/HW/DATA255_DL_SPRING2024/HW4/Transfer-Learning-and-Fi\\nimage 2/10 /Users/gosalvez/Desktop/Git/python_practice/python_practice/HW/DATA255_DL_SPRING2024/HW4/Transfer-Learning-and-Fi\\nimage 3/10 /Users/gosalvez/Desktop/Git/python_practice/python_practice/HW/DATA255_DL_SPRING2024/HW4/Transfer-Learning-and-Fi\\nimage 4/10 /Users/gosalvez/Desktop/Git/python_practice/python_practice/HW/DATA255_DL_SPRING2024/HW4/Transfer-Learning-and-Fi\\nimage 5/10 /Users/gosalvez/Desktop/Git/python_practice/python_practice/HW/DATA255_DL_SPRING2024/HW4/Transfer-Learning-and-Fi\\nimage 6/10 /Users/gosalvez/Desktop/Git/python_practice/python_practice/HW/DATA255_DL_SPRING2024/HW4/Transfer-Learning-and-Fi\\nimage 7/10 /Users/gosalvez/Desktop/Git/python_practice/python_practice/HW/DATA255_DL_SPRING2024/HW4/Transfer-Learning-and-Fi\\nimage 8/10 /Users/gosalvez/Desktop/Git/python_practice/python_practice/HW/DATA255_DL_SPRING2024/HW4/Transfer-Learning-and-Fi\\nimage 9/10 /Users/gosalvez/Desktop/Git/python_practice/python_practice/HW/DATA255_DL_SPRING2024/HW4/Transfer-Learning-and-Fi\\nimage 10/10 /Users/gosalvez/Desktop/Git/python_practice/python_practice/HW/DATA255_DL_SPRING2024/HW4/Transfer-Learning-and-F\\nSpeed: 8.6ms preprocess, 729.6ms inference, 1.4ms postprocess per image at shape (1, 3, 800, 800)\\nResults saved to runs/detect/predict2\\n💡 Learn more at https://docs.ultralytics.com/modes/predict\\nReview r esults \\ue313\\nimport\\xa0glob\\nfrom\\xa0IPython.display\\xa0 import\\xa0display ,\\xa0HTML\\nfrom\\xa0PIL\\xa0import\\xa0Image\\n#\\xa0Get\\xa0list\\xa0of\\xa0image\\xa0paths\\nimage_paths\\xa0=\\xa0glob.glob (f\\'runs/detect/predict2/*.jpg\\' )[:10]\\n#\\xa0Create\\xa0HTML\\xa0table\\ntable_html\\xa0=\\xa0 \"<table><tr>\"\\n#\\xa0Loop\\xa0through\\xa0image\\xa0paths\\nfor\\xa0i,\\xa0image_path\\xa0 in\\xa0enumerate (image_paths ):\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Open\\xa0image\\n\\xa0\\xa0\\xa0\\xa0img\\xa0=\\xa0Image. open(image_path )\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Resize\\xa0image\\xa0to\\xa0fit\\xa0in\\xa0table\\n\\xa0\\xa0\\xa0\\xa0img.thumbnail ((400,\\xa0400))\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Add\\xa0image\\xa0to\\xa0table\\n\\xa0\\xa0\\xa0\\xa0table_html\\xa0+=\\xa0 f\"<td><img\\xa0src=\\' {image_path }\\'\\xa0width=\\'400\\'/></td>\"\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Break\\xa0to\\xa0next\\xa0row\\xa0after\\xa0every\\xa02\\xa0images\\n\\xa0\\xa0\\xa0\\xa0if\\xa0(i\\xa0+\\xa01)\\xa0%\\xa05\\xa0==\\xa00:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0table_html\\xa0+=\\xa0 \"</tr><tr>\"\\n\\xa0\\xa0\\xa0\\xa0\\ntable_html\\xa0+=\\xa0 \"</tr></table>\"\\n#\\xa0Display\\xa0table\\ndisplay(HTML(table_html ))\\nIf satis\\x00ed, BINGO! Y ou now ha ve trained weights in the runs/detect/tr ain/weights/best.pt tr ained model that can be used for inf erence in\\nweb apps b y deplo ying!\\nCan deplo y locally for local de velopment and use or on hosted infr astructur ed, such as A WS, A zure, or GCS, Robo\\x00ow , etcReady t o Deplo y Your Trained Model5/5/24, 1:02 PM Part_2_Jorge-Gosalvez_255_HW4.ipynb - Colab\\nhttps://colab.research.google.com/drive/1Cnfp7efAdIjABAjqlTbOWh4Lu_DvdCCT#printMode=true 6/7', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW4_ObjDetect_YOLO.pdf', 'page': 5}),\n",
              " Document(page_content='5/5/24, 1:02 PM Part_2_Jorge-Gosalvez_255_HW4.ipynb - Colab\\nhttps://colab.research.google.com/drive/1Cnfp7efAdIjABAjqlTbOWh4Lu_DvdCCT#printMode=true 7/7', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge-Gosalvez_HW4_ObjDetect_YOLO.pdf', 'page': 6}),\n",
              " Document(page_content='Homework 08: LSTM for Time Series Pr edictionSJSU MSDS 255 DL, Spring 2024\\ue313\\nGit: https:/ /github.com/jr gosalv ez/data255_DL\\nimport\\xa0matplotlib.pyplot\\xa0 as\\xa0plt\\nimport\\xa0numpy\\xa0as\\xa0np\\nimport\\xa0pandas\\xa0 as\\xa0pd\\nimport\\xa0torch\\nimport\\xa0torch.nn\\xa0 as\\xa0nn\\nimport\\xa0torch.optim\\xa0 as\\xa0optim\\nimport\\xa0torch.utils.data\\xa0 as\\xa0data\\nfrom\\xa0sklearn.preprocessing\\xa0 import\\xa0MinMaxScaler\\nimport\\xa0warnings\\nwarnings.filterwarnings (\"ignore\" )\\nImpor t the data fr om .csv \\x00le \\ue313\\ndf\\xa0=\\xa0pd.read_csv (\\'Google_Stock_Price_Train.csv\\' )\\nprint(\\'Number\\xa0of\\xa0rows\\xa0and\\xa0columns:\\' ,\\xa0df.shape )\\nprint()\\ndf5/5/24, 1:05 PM Jorge_Gosalvez_255_HW8_LSTM.ipynb - Colab\\nhttps://colab.research.google.com/drive/1hwLhCVmpzyBOANySoVFXy6pvuf5Rndx8#printMode=true 1/7', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW8_LSTM.pdf', 'page': 0}),\n",
              " Document(page_content='Number of rows and columns: (1258, 6)\\nDateOpenHigh LowClose Volume\\n0 1/3/2012 325.25 332.83 324.97 663.59 7,380,500\\n1 1/4/2012 331.27 333.87 329.08 666.45 5,749,400\\n2 1/5/2012 329.83 330.75 326.89 657.21 6,590,300\\n3 1/6/2012 328.34 328.77 323.68 648.24 5,405,900\\n4 1/9/2012 322.04 322.29 309.46 620.76 11,688,800\\n... ... ... ... ... ... ...\\n1253 12/23/2016 790.90 792.74 787.28 789.91 623,400\\n1254 12/27/2016 790.68 797.86 787.66 791.55 789,100\\n1255 12/28/2016 793.70 794.23 783.20 785.05 1,153,800\\n1256 12/29/2016 783.33 785.93 778.92 782.79 744,300\\n1257 12/30/2016 782.75 782.78 770.41 771.82 1,770,000\\n1258 rows × 6 columns\\nSanity Check:  The Close  featur e is incorr ect b/c mor e than 2x the High v alue of the da y... does not mak e sense.\\nCannot close higher than the highest v alue of the da y.\\nPreprocess the data \\ue313\\n#\\xa0Get\\xa0and\\xa0plot\\xa0\\'Open\\'\\xa0value;\\xa0feature\\xa0that\\xa0model\\xa0wi ll\\xa0train\\xa0and\\xa0test\\xa0on\\ntimeseries\\xa0=\\xa0df [[\"Open\"]].values.astype (\\'float32\\' )\\nplt.plot (timeseries )\\nplt.show ()5/5/24, 1:05 PM Jorge_Gosalvez_255_HW8_LSTM.ipynb - Colab\\nhttps://colab.research.google.com/drive/1hwLhCVmpzyBOANySoVFXy6pvuf5Rndx8#printMode=true 2/7', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW8_LSTM.pdf', 'page': 1}),\n",
              " Document(page_content='#\\xa0Feature\\xa0Scaling\\nsc\\xa0=\\xa0MinMaxScaler (feature_range\\xa0=\\xa0 (0,\\xa01))\\ntimeseries_scaled\\xa0=\\xa0sc.fit_transform (timeseries )\\nBecause a small dataset, cr eating a P yTorch tensor dir ectly fr om a list of nump y arr ays; howe ver,\\nthis can be slow fr or complex models with lar ge datasets. F or mor e complicated models and lar ger\\ndatasets, conv ert list of nump y arr ays int o a single nump y arr ay befor e conv erting it a tensor .\\nPerformance is not an issue with this model, so did not pr eprocess furhter .\\nSplit int o 80% tr ain 20% pr ediction \\ue313\\n#\\xa0train-test\\xa0split\\xa0for\\xa0timeseries\\xa0data\\xa0frame\\ntrain_size\\xa0\\xa0=\\xa0 int(len(timeseries_scaled )\\xa0*\\xa00.8)\\ntest_size\\xa0\\xa0\\xa0=\\xa0 len(timeseries_scaled )\\xa0-\\xa0train_size\\ntrain,\\xa0test\\xa0=\\xa0timeseries_scaled [:train_size ],\\xa0timeseries_scaled [train_size :]\\nCreate the dataset\\ue3135/5/24, 1:05 PM Jorge_Gosalvez_255_HW8_LSTM.ipynb - Colab\\nhttps://colab.research.google.com/drive/1hwLhCVmpzyBOANySoVFXy6pvuf5Rndx8#printMode=true 3/7', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW8_LSTM.pdf', 'page': 2}),\n",
              " Document(page_content='def\\xa0create_dataset (dataset,\\xa0lookback ):\\n\\xa0\\xa0\\xa0\\xa0\"\"\"Transform\\xa0a\\xa0time\\xa0series\\xa0into\\xa0a\\xa0prediction\\xa0datas et\\n\\xa0\\xa0\\xa0\\xa0Args:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0dataset:\\xa0A\\xa0numpy\\xa0array\\xa0of\\xa0time\\xa0series,\\xa0fir st\\xa0dimension\\xa0is\\xa0the\\xa0time\\xa0steps\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0lookback:\\xa0Size\\xa0of\\xa0window\\xa0for\\xa0prediction\\n\\xa0\\xa0\\xa0\\xa0\"\"\"\\n\\xa0\\xa0\\xa0\\xa0X,\\xa0y\\xa0=\\xa0[],\\xa0[]\\n\\xa0\\xa0\\xa0\\xa0for\\xa0i\\xa0in\\xa0range(len(dataset)-lookback ):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0feature\\xa0=\\xa0dataset [i:i+lookback ]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0target\\xa0=\\xa0dataset [i+1:i+lookback+ 1]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0X.append (feature)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0y.append (target)\\n\\xa0\\xa0\\xa0\\xa0return\\xa0torch.tensor (X),\\xa0torch.tensor (y)\\nlookback\\xa0=\\xa0 4\\nX_train,\\xa0y_train\\xa0=\\xa0create_dataset (train,\\xa0lookback=lookback )\\nX_test,\\xa0y_test\\xa0\\xa0\\xa0=\\xa0create_dataset (test,\\xa0lookback=lookback )\\nprint(X_train.shape ,\\xa0y_train.shape )\\nprint(X_test.shape ,\\xa0y_test.shape )\\ntorch.Size([1002, 4, 1]) torch.Size([1002, 4, 1])\\ntorch.Size([248, 4, 1]) torch.Size([248, 4, 1])\\nCreate and tr ain the model on the data \\ue313\\nCreate the model\\ue313\\nUnsuppor ted Cell Type. Double-Click t o inspect/edit the content.5/5/24, 1:05 PM Jorge_Gosalvez_255_HW8_LSTM.ipynb - Colab\\nhttps://colab.research.google.com/drive/1hwLhCVmpzyBOANySoVFXy6pvuf5Rndx8#printMode=true 4/7', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW8_LSTM.pdf', 'page': 3}),\n",
              " Document(page_content='#\\xa0LSTM\\xa0model\\xa0with\\xa02\\xa0lstm\\xa0layers,\\xa0linear\\xa0function\\xa0l ayers\\xa0and\\xa0dropout\\xa0regularization\\xa0t\\nclass\\xa0my_LSTM(nn.Module):\\n\\xa0\\xa0\\xa0\\xa0def\\xa0__init__ (self,\\xa0input_size =1,\\xa0hidden_size =50,\\xa0num_layers =2,\\xa0dropout=0.2):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0super ().__init__ ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.lstm\\xa0=\\xa0nn.LSTM (input_size=input_size ,\\xa0hidden_size=hidden_size ,\\xa0num_laye\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.dropout\\xa0=\\xa0nn.Dropout (p=dropout )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.linear\\xa0=\\xa0nn.Linear (hidden_size ,\\xa01)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0def\\xa0forward(self,\\xa0x):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Apply\\xa0dropout\\xa0to\\xa0the\\xa0input\\xa0sequence\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0 self.dropout (x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Pass\\xa0through\\xa0LSTM\\xa0layers\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x ,\\xa0_\\xa0=\\xa0self.lstm(x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Apply\\xa0dropout\\xa0to\\xa0the\\xa0LSTM\\xa0output\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0 self.dropout (x)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Pass\\xa0through\\xa0linear\\xa0layer\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0 self.linear(x)\\xa0\\xa0#\\xa0transform\\xa0the\\xa0entire\\xa0batch\\xa0of\\xa0data\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Squeeze\\xa0to\\xa0remove\\xa0the\\xa0extra\\xa0dimension\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x\\xa0=\\xa0torch.squeeze (x,\\xa0dim=1)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return\\xa0x\\nInitiate the model, optimiz er, and loader \\ue313\\nmodel\\xa0\\xa0\\xa0\\xa0\\xa0=\\xa0my_LSTM ()\\noptimizer\\xa0=\\xa0optim.Adam (model.parameters ())\\nloss_fn\\xa0\\xa0\\xa0=\\xa0nn.MSELoss ()\\xa0\\xa0\\xa0#\\xa0linear,\\xa0so\\xa0MSE\\nloader\\xa0\\xa0\\xa0\\xa0=\\xa0data.DataLoader (data.TensorDataset (X_train,\\xa0y_train ),\\xa0shuffle= True,\\xa0batc\\nTrain and v erify LSTM model / network \\ue3135/5/24, 1:05 PM Jorge_Gosalvez_255_HW8_LSTM.ipynb - Colab\\nhttps://colab.research.google.com/drive/1hwLhCVmpzyBOANySoVFXy6pvuf5Rndx8#printMode=true 5/7', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW8_LSTM.pdf', 'page': 4}),\n",
              " Document(page_content='%%time\\nn_epochs\\xa0=\\xa0 1001\\nfor\\xa0epoch\\xa0in\\xa0range(n_epochs ):\\n\\xa0\\xa0\\xa0\\xa0model.train ()\\n\\xa0\\xa0\\xa0\\xa0for\\xa0X_batch ,\\xa0y_batch\\xa0 in\\xa0loader:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0y_pred\\xa0=\\xa0model (X_batch)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0loss\\xa0\\xa0\\xa0=\\xa0loss_fn (y_pred,\\xa0y_batch )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0optimizer.zero_grad ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0loss.backward ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0optimizer.step ()\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Validation\\n\\xa0\\xa0\\xa0\\xa0if\\xa0epoch\\xa0%\\xa0 100\\xa0!=\\xa00:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 continue\\n\\xa0\\xa0\\xa0\\xa0model. eval()\\n\\xa0\\xa0\\xa0\\xa0with\\xa0torch.no_grad ():\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0y_pred\\xa0=\\xa0model (X_train)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0train_rmse\\xa0=\\xa0np.sqrt (loss_fn(y_pred,\\xa0y_train ))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0y_pred\\xa0=\\xa0model (X_test)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0test_rmse\\xa0=\\xa0np.sqrt (loss_fn(y_pred,\\xa0y_test))\\n\\xa0\\xa0\\xa0\\xa0print(\"Epoch\\xa0%d:\\xa0train\\xa0RMSE\\xa0%.4f,\\xa0test\\xa0RMSE\\xa0%.4f\" \\xa0%\\xa0(epoch,\\xa0train_rmse ,\\xa0test_rms\\nEpoch 0: train RMSE 0.1420, test RMSE 0.2750\\nEpoch 100: train RMSE 0.0906, test RMSE 0.1789\\nEpoch 200: train RMSE 0.0909, test RMSE 0.1799\\nEpoch 300: train RMSE 0.0948, test RMSE 0.1866\\nEpoch 400: train RMSE 0.0914, test RMSE 0.1755\\nEpoch 500: train RMSE 0.0908, test RMSE 0.1776\\nEpoch 600: train RMSE 0.0943, test RMSE 0.1908\\nEpoch 700: train RMSE 0.0952, test RMSE 0.1734\\nEpoch 800: train RMSE 0.0884, test RMSE 0.1828\\nEpoch 900: train RMSE 0.0919, test RMSE 0.1810\\nEpoch 1000: train RMSE 0.0900, test RMSE 0.1794\\nCPU times: user 14min 42s, sys: 17.8 s, total: 15min\\nWall time: 7min 39s\\nRepor t on the r esults of the model \\ue3135/5/24, 1:05 PM Jorge_Gosalvez_255_HW8_LSTM.ipynb - Colab\\nhttps://colab.research.google.com/drive/1hwLhCVmpzyBOANySoVFXy6pvuf5Rndx8#printMode=true 6/7', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW8_LSTM.pdf', 'page': 5}),\n",
              " Document(page_content=\"#\\xa0Predict\\xa0Google\\xa0stock\\xa0price\\xa0on\\xa0test\\xa0dataset\\xa0and\\xa0c ompare\\xa0to\\xa0actuals\\nwith\\xa0torch.no_grad ():\\n\\xa0\\xa0\\xa0\\xa0#\\xa0shift\\xa0train\\xa0predictions\\xa0for\\xa0plotting\\n\\xa0\\xa0\\xa0\\xa0train_plot\\xa0=\\xa0np.ones_like (timeseries_scaled )\\xa0*\\xa0np.nan\\n\\xa0\\xa0\\xa0\\xa0y_pred\\xa0=\\xa0model (X_train)\\n\\xa0\\xa0\\xa0\\xa0y_pred\\xa0=\\xa0y_pred [:,\\xa0-1,\\xa0:]\\n\\xa0\\xa0\\xa0\\xa0train_plot [lookback :train_size ]\\xa0=\\xa0model (X_train)[:,\\xa0-1,\\xa0:]\\n\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0#\\xa0shift\\xa0test\\xa0predictions\\xa0for\\xa0plotting\\n\\xa0\\xa0\\xa0\\xa0test_plot\\xa0=\\xa0np.ones_like (timeseries_scaled )\\xa0*\\xa0np.nan\\n\\xa0\\xa0\\xa0\\xa0test_plot [train_size+lookback :len(timeseries_scaled )]\\xa0=\\xa0model (X_test)[:,\\xa0-1,\\xa0:]\\n#\\xa0Inverse\\xa0transform\\xa0the\\xa0scaled\\xa0data\\nreal_stock_prices\\xa0=\\xa0sc.inverse_transform (timeseries_scaled )\\npredicted_train\\xa0\\xa0\\xa0=\\xa0sc.inverse_transform (train_plot.reshape (-1,\\xa01))\\npredicted_test\\xa0\\xa0\\xa0\\xa0=\\xa0sc.inverse_transform (test_plot.reshape (-1,\\xa01))\\n\\xa0\\xa0\\xa0\\xa0\\nplt.plot (real_stock_prices ,\\xa0c='b',\\xa0label='Real')\\nplt.plot (predicted_train ,\\xa0c='r',\\xa0label='Trained' )\\nplt.plot (predicted_test ,\\xa0c='g',\\xa0label='Predicted' )\\nplt.title ('Google\\xa0Stock\\xa0Price\\xa0Prediction\\xa0(Jan\\xa02012\\xa0to\\xa0Dec\\xa020 16)')\\npltxlabel('Business Days')5/5/24, 1:05 PM Jorge_Gosalvez_255_HW8_LSTM.ipynb - Colab\\nhttps://colab.research.google.com/drive/1hwLhCVmpzyBOANySoVFXy6pvuf5Rndx8#printMode=true 7/7\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW8_LSTM.pdf', 'page': 6}),\n",
              " Document(page_content='Homework 07: NLPSJSU MSDS 255 DL, Spring 2024\\ue313\\nGit: https:/ /github.com/jr gosalv ez/data255_DL\\nimport\\xa0numpy\\xa0as\\xa0np\\nimport\\xa0pandas\\xa0 as\\xa0pd\\nfrom\\xa0scipy\\xa0import\\xa0spatial\\nfrom\\xa0sklearn.metrics.pairwise\\xa0 import\\xa0cosine_similarity\\nfrom\\xa0sklearn.model_selection\\xa0 import\\xa0train_test_split\\nfrom\\xa0sklearn.naive_bayes\\xa0 import\\xa0MultinomialNB\\nfrom\\xa0sklearn.feature_extraction.text\\xa0 import\\xa0CountVectorizer\\nfrom\\xa0sklearn.metrics\\xa0 import\\xa0accuracy_score ,\\xa0confusion_matrix ,\\xa0classification_report\\nimport\\xa0scikitplot\\xa0 as\\xa0skplt\\nimport\\xa0matplotlib.pyplot\\xa0 as\\xa0plt\\nfrom\\xa0gensim.models\\xa0 import\\xa0KeyedVectors\\nimport\\xa0re\\nimport\\xa0nltk\\nfrom\\xa0nltk.corpus\\xa0 import\\xa0stopwords\\nStep 1: Load the Wikipedia GLoVE W ord2Vec (glo ve.6B.50d.txt) \\ue313\\nDownload GLoV e pretrained models: https:/ /nlp.stanfor d.edu/pr ojects/glo ve/\\nembeddings_dict\\xa0=\\xa0 {}\\nwith\\xa0open(\"glove.6B.50d.txt\" ,\\xa0\\'r\\',\\xa0encoding= \"utf-8\")\\xa0as\\xa0f:\\n\\xa0\\xa0\\xa0\\xa0for\\xa0line\\xa0in\\xa0f:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0values\\xa0=\\xa0line.split ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0word\\xa0\\xa0\\xa0=\\xa0values [0]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0vector\\xa0=\\xa0np.asarray (values[1:],\\xa0\"float32\" )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0embeddings_dict [word]\\xa0=\\xa0vector\\nCreate functions\\ue313\\ndef\\xa0find_closest_embeddings (embedding ):\\n\\xa0\\xa0\\xa0\\xa0return\\xa0sorted(embeddings_dict.keys (),\\xa0key=lambda\\xa0word:\\xa0spatial.distance.euclidean (embeddings_dict [word],\\xa0embedding ))\\ndef\\xa0analogy(wordvec1 ,\\xa0wordvec2 ,\\xa0wordvec3 ):\\n\\xa0\\xa0\\xa0\\xa0analogy\\xa0=\\xa0find_closest_embeddings (embeddings_dict [wordvec1 ]\\xa0-\\xa0embeddings_dict [wordvec3 ]\\xa0+\\xa0embeddings_dict [wordvec2 ])[1:2]\\n\\xa0\\xa0\\xa0\\xa0return\\xa0analogy [0]\\nExperimenting\\ue313\\n#\\xa0Find\\xa0closest\\xa0three\\xa0words\\xa0without\\xa0returning\\xa0the\\xa0w ord\\xa0itself\\nfind_closest_embeddings (embeddings_dict [\"king\"])[1:4]\\n[\\'prince\\', \\'queen\\', \\'uncle\\']\\nprint(find_closest_embeddings (\\n\\xa0\\xa0\\xa0\\xa0embeddings_dict [\"king\"]\\xa0-\\xa0embeddings_dict [\"man\"]\\xa0+\\xa0embeddings_dict [\"woman\"]\\n)[1:4])\\n[\\'queen\\', \\'prince\\', \\'elizabeth\\']\\nprint(find_closest_embeddings (\\n\\xa0\\xa0\\xa0\\xa0embeddings_dict [\"princess\" ]\\xa0-\\xa0embeddings_dict [\"woman\"]\\xa0+\\xa0embeddings_dict [\"man\"]\\n)[1:2])\\n[\\'prince\\']\\nanalogy(\"princess\" ,\\xa0\"man\",\\xa0\"woman\")5/5/24, 1:07 PM Jorge_Gosalvez_255_HW7_NLP.ipynb - Colab\\nhttps://colab.research.google.com/drive/1UsZg3WHJK7kuh4MNL0M1csOhlYXooiox#printMode=true 1/4', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW7_NLP.pdf', 'page': 0}),\n",
              " Document(page_content='\\'prince\\'\\nMan and W oman\\nChair and Throne\\nwater and bab yStep 2: Show how similar these wor ds ar e: \\ue313\\ndef\\xa0similar(wordvec1 ,\\xa0wordvec2 ):\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Get\\xa0the\\xa0embeddings\\xa0for\\xa0\"wordvec1\"\\n\\xa0\\xa0\\xa0\\xa0embedding\\xa0=\\xa0embeddings_dict [wordvec1 ].reshape (1,\\xa0-1)\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Get\\xa0embeddings\\xa0for\\xa0the\\xa0predicted\\xa0words\\n\\xa0\\xa0\\xa0\\xa0predicted_words\\xa0=\\xa0 [wordvec2 ]\\n\\xa0\\xa0\\xa0\\xa0predicted_embeddings\\xa0=\\xa0 [embeddings_dict [word]\\xa0for\\xa0word\\xa0in\\xa0predicted_words ]\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Calculate\\xa0cosine\\xa0similarity\\n\\xa0\\xa0\\xa0\\xa0similarities\\xa0=\\xa0cosine_similarity (embedding ,\\xa0predicted_embeddings )\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Print\\xa0the\\xa0cosine\\xa0similarity\\xa0scores\\n\\xa0\\xa0\\xa0\\xa0for\\xa0word,\\xa0similarity_score\\xa0 in\\xa0zip(predicted_words ,\\xa0similarities [0]):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 print(f\"Cosine\\xa0similarity\\xa0between\\xa0 {wordvec1 }\\xa0and\\xa0{word}:\\xa0{similarity_score :.4f}\")\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nsimilar(\"man\",\\xa0\"woman\")\\nsimilar(\"chair\",\\xa0\"throne\" )\\nsimilar(\"water\",\\xa0\"baby\")\\nCosine similarity between man and woman: 0.8860\\nCosine similarity between chair and throne: 0.2797\\nCosine similarity between water and baby: 0.4081\\n_____ is t o King as W oman is t o Man.\\n_____ is t o Princess as Man is t o Woman.\\n_____ is t o a woman as a child is t o an adult.Step 3: Using these pr ovide analogies for the following: \\ue313\\n#\\xa0Define\\xa0analogy\\xa0examples\\nanalogies\\xa0=\\xa0 [\\n\\xa0\\xa0\\xa0\\xa0(\"king\",\\xa0\"woman\",\\xa0\"man\"),\\n\\xa0\\xa0\\xa0\\xa0(\"princess\" ,\\xa0\"man\",\\xa0\"woman\"),\\n\\xa0\\xa0\\xa0\\xa0(\"woman\",\\xa0\"child\",\\xa0\"adult\")\\n]\\n#\\xa0Calculate\\xa0and\\xa0print\\xa0analogies\\nfor\\xa0analogy_pair\\xa0 in\\xa0analogies :\\n\\xa0\\xa0\\xa0\\xa0analogy_word\\xa0=\\xa0analogy (*analogy_pair )\\n\\xa0\\xa0\\xa0\\xa0print(f\"\\'{analogy_word }\\'\\xa0is\\xa0to\\xa0\\' {analogy_pair [0]}\\'\\xa0as\\xa0\\'{analogy_pair [1]}\\'\\xa0is\\xa0to\\xa0\\' {analogy_pair [2]}\\'\")\\n\\'queen\\' is to \\'king\\' as \\'woman\\' is to \\'man\\'\\n\\'prince\\' is to \\'princess\\' as \\'man\\' is to \\'woman\\'\\n\\'mother\\' is to \\'woman\\' as \\'child\\' is to \\'adult\\'\\nStep 4: Apply Naiv e-Ba yes Classi\\x00er on the Spam-Ham dataset shown in the demo \\ue313\\n#\\xa0Load\\xa0the\\xa0Spam-Ham\\xa0dataset\\ndata\\xa0=\\xa0pd.read_csv (\"spam.csv\" ,\\xa0encoding= \\'latin-1\\' )\\ndata.head ()5/5/24, 1:07 PM Jorge_Gosalvez_255_HW7_NLP.ipynb - Colab\\nhttps://colab.research.google.com/drive/1UsZg3WHJK7kuh4MNL0M1csOhlYXooiox#printMode=true 2/4', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW7_NLP.pdf', 'page': 1}),\n",
              " Document(page_content='v1 v2Unnamed: 2 Unnamed: 3 Unnamed: 4\\n0ham Go until jurong point, crazy .. Available only ... NaN NaN NaN\\n1ham Ok lar ... Joking wif u oni... NaN NaN NaN\\n2spam Free entry in 2 a wkly comp to win F A Cup ﬁna... NaN NaN NaN\\n3ham U dun say so early hor ... U c already then say ... NaN NaN NaN\\n4ham Nah I don\\'t think he goes to usf, he lives aro... NaN NaN NaN\\nv1 = tar get / labels\\nv2 = text t o pre-process and use\\nPreparing and exploring pr eprocessed spam-ham data \\ue313\\n#\\xa0Preprocess\\xa0the\\xa0text\\ndef\\xa0preprocess_text (text):\\n\\xa0\\xa0\\xa0\\xa0text\\xa0=\\xa0re.sub (r\\'\\\\W\\',\\xa0\\'\\xa0\\',\\xa0text)\\xa0\\xa0#\\xa0Remove\\xa0non-word\\xa0characters\\n\\xa0\\xa0\\xa0\\xa0text\\xa0=\\xa0text.lower ()\\xa0\\xa0#\\xa0Convert\\xa0text\\xa0to\\xa0lowercase\\n\\xa0\\xa0\\xa0\\xa0tokens\\xa0=\\xa0nltk.word_tokenize (text)\\xa0\\xa0#\\xa0Tokenize\\xa0the\\xa0text\\n\\xa0\\xa0\\xa0\\xa0tokens\\xa0=\\xa0 [word\\xa0for\\xa0word\\xa0in\\xa0tokens\\xa0 if\\xa0word\\xa0not\\xa0in\\xa0stopwords.words (\\'english\\' )]\\xa0\\xa0#\\xa0Remove\\xa0stopwords\\n\\xa0\\xa0\\xa0\\xa0return\\xa0\\'\\xa0\\'.join(tokens)\\ndata[\\'clean_text\\' ]\\xa0=\\xa0data[\\'v2\\'].apply(preprocess_text )\\nv1 v2Unnamed: 2 Unnamed: 3 Unnamed: 4 clean_text\\n0ham Go until jurong point, crazy .. Available only ... NaN NaN NaN go jurong point crazy available bugis n great ...\\n1ham Ok lar ... Joking wif u oni... NaN NaN NaN ok lar joking wif u oni\\n2spam Free entry in 2 a wkly comp to win F A Cup ﬁna... NaN NaN NaN free entry 2 wkly comp win fa cup ﬁnal tkts 2...\\n3ham U dun say so early hor ... U c already then say ... NaN NaN NaN u dun say early hor u c already say\\n4ham Nah I don\\'t think he goes to usf, he lives aro... NaN NaN NaN nah think goes usf lives around thoughdata.head ()\\nSplit the dataset int o train and test sets 80/20 \\ue313\\nX_train,\\xa0X_test,\\xa0y_train ,\\xa0y_test\\xa0=\\xa0train_test_split (data[\\'clean_text\\' ],\\xa0data[\\'v1\\'],\\xa0test_size= 0.2,\\xa0random_state= 42)\\nVectorize data for model ingestion b y conv ert texting data t o numerical f eatur es using CountV ectorizer \\ue313\\nvectorizer\\xa0=\\xa0CountVectorizer ()\\nX_train_counts\\xa0=\\xa0vectorizer.fit_transform (X_train)\\nX_test_counts\\xa0\\xa0=\\xa0vectorizer.transform (X_test)\\nTrain the Naiv e-Ba yes Classi\\x00er \\ue313\\n▾MultinomialNB\\nMultinomialNB()nb_classifier\\xa0=\\xa0MultinomialNB ()\\nnb_classifier.fit (X_train_counts ,\\xa0y_train )\\nEvaluate the classi\\x00er\\ue313\\ny_pred\\xa0=\\xa0nb_classifier.predict (X_test_counts )\\naccuracy\\xa0=\\xa0accuracy_score (y_test,\\xa0y_pred)\\nprint(f\"Accuracy:\\xa0 {accuracy :.4f}\")5/5/24, 1:07 PM Jorge_Gosalvez_255_HW7_NLP.ipynb - Colab\\nhttps://colab.research.google.com/drive/1UsZg3WHJK7kuh4MNL0M1csOhlYXooiox#printMode=true 3/4', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW7_NLP.pdf', 'page': 2}),\n",
              " Document(page_content='Accuracy: 0.9821\\nconfusion_matrix:  [[959   6]\\n [ 14 136]]\\n#confusion\\xa0matrix\\nconfusion_matrix\\xa0=\\xa0confusion_matrix (y_test,\\xa0y_pred,\\xa0labels= [\"ham\",\"spam\"])\\nskplt.metrics.plot_confusion_matrix (y_test,\\xa0y_pred,\\xa0normalize= False)\\nskplt.metrics.plot_confusion_matrix (y_test,\\xa0y_pred,\\xa0normalize= True)\\nprint(\\'confusion_matrix:\\xa0\\' ,\\xa0confusion_matrix )\\nplt.show ()\\n\\x00\\nStart coding or generate  with AI.5/5/24, 1:07 PM Jorge_Gosalvez_255_HW7_NLP.ipynb - Colab\\nhttps://colab.research.google.com/drive/1UsZg3WHJK7kuh4MNL0M1csOhlYXooiox#printMode=true 4/4', metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW7_NLP.pdf', 'page': 3})]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# split text data into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=20)\n",
        "text_chunks = text_splitter.split_documents(data)\n",
        "print(len(text_chunks))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdORRTUoi3vp",
        "outputId": "8aef10ca-aa90-446b-c5a8-393cf7584a38"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check the chunks\n",
        "text_chunks[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3t1UdQztjFp6",
        "outputId": "be03ea88-6b70-4338-e96d-99b356e34ee3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content=\"version data\\n0 v2.0 {'title': 'Beyoncé', 'paragraphs': [{'qas': [{...\\n1 v2.0 {'title': 'Frédéric_Chopin', 'paragraphs': [{'...\\n2 v2.0 {'title': 'Sino-T ibetan_relations_during_the_M...\\n3 v2.0 {'title': 'IPod', 'paragraphs': [{'qas': [{'qu...\\n4 v2.0 {'title': 'The_Legend_of_Zelda:_T wilight_Princ...\\n... ... ...\\n437 v2.0 {'title': 'Infection', 'paragraphs': [{'qas': ...\\n438 v2.0 {'title': 'Hunting', 'paragraphs': [{'qas': [{...\\n439 v2.0 {'title': 'Kathmandu', 'paragraphs': [{'qas': ...\\n440 v2.0 {'title': 'Myocardial_infarction', 'paragraphs...\\n441 v2.0 {'title': 'Matter', 'paragraphs': [{'qas': [{'...\\n442 rows × 2 columns\\nOpen and pr eprocess (add special t okens) dataset per BER T format\\nLoad pr eprocess (add special t okens) the SQU AD 2.0  dataset per BER T format. Get a minimum 20 QnA pairs. \\ue313\\n#\\xa0Function\\xa0to\\xa0load\\xa0SQuAD2\\xa0data\\xa0and\\xa0add\\xa0special\\xa0tok ens\\xa0[CLS]\\xa0and\\xa0[SEP]\\ndef\\xa0load_squad_data (file_path ,\\xa0num_samples =20):\\n\\xa0\\xa0\\xa0\\xa0with\\xa0open(file_path ,\\xa0'r',\\xa0encoding= 'utf-8')\\xa0as\\xa0f:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0squad_data\\xa0=\\xa0json.load (f)\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW10_BERT.pdf', 'page': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reformat chunks to improve vectorization; match 'jamescalam/llama-2-arxiv-papers-chunked' format sourced from Llama 2 ArXiv papers on huggingface\n",
        "dataset = []\n",
        "\n",
        "for i, chunk in enumerate(text_chunks):\n",
        "    dataset.append({\n",
        "        'doi': '',  # you can add a DOI here if available\n",
        "        'chunk-id': str(i),\n",
        "        'chunk': chunk,\n",
        "        'id': '',  # you can add an ID here if available\n",
        "        'title': '',  # you can add a title here if available\n",
        "        'summary': '',  # you can add a summary here if available\n",
        "        'source': '',  # you can add a source here if available\n",
        "        'authors': [],  # you can add authors here if available\n",
        "        'categories': [],  # you can add categories here if available\n",
        "        'comment': '',  # you can add a comment here if available\n",
        "        'journal_ref': None,  # you can add a journal reference here if available\n",
        "        'primary_category': '',  # you can add a primary category here if available\n",
        "        'published': '',  # you can add a published date here if available\n",
        "        'updated': '',  # you can add an updated date here if available\n",
        "        'references': []  # you can add references here if available\n",
        "    })\n",
        "\n",
        "print(dataset[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyeGKo8-jqAf",
        "outputId": "a6dd63b0-b88e-41b3-f5dd-43ca8f356456"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'doi': '', 'chunk-id': '3', 'chunk': Document(page_content=\"data\\xa0=\\xa0 []\\n\\xa0\\xa0\\xa0\\xa0for\\xa0i\\xa0in\\xa0range(min(len(squad_data ['data']),\\xa0num_samples )):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0paragraphs\\xa0=\\xa0squad_data ['data'][i]['paragraphs' ]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for\\xa0paragraph\\xa0 in\\xa0paragraphs :\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0context\\xa0=\\xa0paragraph ['context' ]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0qas\\xa0=\\xa0paragraph ['qas']\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for\\xa0qa\\xa0in\\xa0qas:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0question\\xa0=\\xa0qa ['question' ]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0answers\\xa0\\xa0=\\xa0qa ['answers' ]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\xa0answers :\\xa0\\xa0#\\xa0Check\\xa0if\\xa0answers\\xa0are\\xa0available\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0answer_text\\xa0=\\xa0answers [0]['text']\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 #\\xa0Tokenize\\xa0question\\xa0and\\xa0answer\\xa0text\\xa0.encode\\xa0automa tically\\xa0adds\\xa0[CLS]\\xa0and\\xa0[SEP]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0tokenized_question\\xa0=\\xa0tokenizer .encode(question ,\\xa0add_special_tokens= True)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0tokenized_answer\\xa0=\\xa0tokenizer.e ncode(answer_text ,\\xa0add_special_tokens= True)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0data.append ((tokenized_question ,\\xa0tokenized_answer ))\\n\\xa0\\xa0\\xa0\\xa0return\\xa0data\\nsquad_data\\xa0=\\xa0load_squad_data (squad_data_path ,\\xa0num_samples= 20)\\nDispla y the \\x00rst f ew question-answer pairs \\ue313\\nfor\\xa0i\\xa0in\\xa0range(8):\", metadata={'source': '/content/drive/MyDrive/MSDA/DATA255/codePDF/Jorge_Gosalvez_HW10_BERT.pdf', 'page': 1}), 'id': '', 'title': '', 'summary': '', 'source': '', 'authors': [], 'categories': [], 'comment': '', 'journal_ref': None, 'primary_category': '', 'published': '', 'updated': '', 'references': []}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lo1AYqcZw4wY"
      },
      "source": [
        "#### Dataset Overview\n",
        "\n",
        "The dataset used are PDFs samples of my (Jorge Gosalvez's) Deep Learning homeworks.\n",
        "\n",
        "Because most **L**arge **L**anguage **M**odels (LLMs) only contain knowledge of the world as it was during training, they cannot answer our questions about Jorge's code without example data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq3-dxkGw4wY"
      },
      "source": [
        "### Task 4: Building the Knowledge Base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsYU27hBw4wY"
      },
      "source": [
        "We now have a dataset that can serve as our chatbot knowledge base. Our next task is to transform that dataset into the knowledge base that our chatbot can use. To do this we must use an embedding model and vector database.\n",
        "\n",
        "We begin by initializing our connection to Pinecone, this requires a [free API key](https://app.pinecone.io)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "GxIvcXXOw4wb"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone\n",
        "\n",
        "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
        "api_key= userdata.get('PineCone')\n",
        "\n",
        "# configure client\n",
        "pc = Pinecone(api_key=api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kcal_JEgw4wb"
      },
      "source": [
        "Now we setup our index specification, this allows us to define the cloud provider and region where we want to deploy our index. You can find a list of all [available providers and regions here](https://docs.pinecone.io/docs/projects)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "uTLMWZPAw4wb"
      },
      "outputs": [],
      "source": [
        "from pinecone import ServerlessSpec\n",
        "\n",
        "spec = ServerlessSpec(\n",
        "    cloud=\"aws\", region=\"us-east-1\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSxNjSjLw4wb"
      },
      "source": [
        "Then we initialize the index. We will be using OpenAI's `text-embedding-ada-002` model for creating the embeddings, so we set the `dimension` to `1536`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ULRvhj4aw4wb",
        "outputId": "91fc9dca-c22e-4576-cf36-7e680c25089a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {},\n",
              " 'total_vector_count': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "index_name = 'llama-2-rag'\n",
        "existing_indexes = [\n",
        "    index_info[\"name\"] for index_info in pc.list_indexes()\n",
        "]\n",
        "\n",
        "# check if index already exists (it shouldn't if this is first time)\n",
        "if index_name not in existing_indexes:\n",
        "    # if does not exist, create index\n",
        "    pc.create_index(\n",
        "        index_name,\n",
        "        dimension=1536,  # dimensionality of ada 002\n",
        "        metric='dotproduct',\n",
        "        spec=spec\n",
        "    )\n",
        "    # wait for index to be initialized\n",
        "    while not pc.describe_index(index_name).status['ready']:\n",
        "        time.sleep(1)\n",
        "\n",
        "# connect to index\n",
        "index = pc.Index(index_name)\n",
        "time.sleep(1)\n",
        "# view index stats\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jae7TDz5w4wb"
      },
      "source": [
        "Our index is now ready but it's empty. It is a vector index, so it needs vectors. As mentioned, to create these vector embeddings we will OpenAI's `text-embedding-ada-002` model — we can access it via LangChain like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "uCQ-XA0Vw4wb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79ac12d7-b90f-4841-dde7-120eb2c9f224"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONTjqjWww4wb"
      },
      "source": [
        "Using this model we can create embeddings like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ugECVXqDw4wb",
        "outputId": "b5b89269-5da0-4a8c-c8df-96c4024724b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 1536)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "texts = [\n",
        "    'this is the first chunk of text',\n",
        "    'then another second chunk of text is here'\n",
        "]\n",
        "\n",
        "res = embed_model.embed_documents(texts)\n",
        "len(res), len(res[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f82zGPmIw4wb"
      },
      "source": [
        "From this we get two (aligning to our two chunks of text) 1536-dimensional embeddings.\n",
        "\n",
        "We're now ready to embed and index all our our data! We do this by looping through our dataset and embedding and inserting everything in batches.\n",
        "\n",
        "**NOTE**: *ensure that chunks are strings and ensure that they are correctly assigned to metadata (do this with the .page_content method)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "147db994a0ef40e8b156ec8efc282216",
            "e6f67823ccbe4575b67ef366559ef115",
            "e2251259cba249868868886d109ae35c",
            "1ed3b66d5f8948b79d70985d2da9bba0",
            "9918148e66a640e98ba43fdc47b8f91d",
            "47036c7a921a410e84e5ab9f538e162f",
            "c90b082e6ab64e9d9febe55ef7606f4a",
            "5facec2d0cc14519aa26f493dc202160",
            "4ad55880bd4443dd8a87981dea80b4d6",
            "b3f212423af74648bb9fb9ded43a20d9",
            "14f46db244884f08b5fde46fa48fa227"
          ],
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "AtgH_iMuw4wb",
        "outputId": "1efca4c4-a2a6-4ce0-bbc8-a1219a5b7df9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "147db994a0ef40e8b156ec8efc282216"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from tqdm.auto import tqdm  # for progress bar\n",
        "\n",
        "data = pd.DataFrame(dataset) # this makes it easier to iterate over the dataset\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "for i in tqdm(range(0, len(data), batch_size)):\n",
        "    i_end = min(len(data), i+batch_size)\n",
        "    # get batch of data\n",
        "    batch = data.iloc[i:i_end]\n",
        "    # generate unique ids for each chunk\n",
        "    ids = [f\"{x['doi']}-{x['chunk-id']}\" for i, x in batch.iterrows()]\n",
        "    # get text to embed\n",
        "    texts = [str(x['chunk']) for _, x in batch.iterrows()]\n",
        "\n",
        "    # embed text\n",
        "    embeds = embed_model.embed_documents(texts)\n",
        "    # get metadata to store in Pinecone\n",
        "    metadata = [\n",
        "        {'text': x['chunk'].page_content,\n",
        "         'source': x['source'],\n",
        "         'title': x['title']} for i, x in batch.iterrows()\n",
        "    ]\n",
        "    # add to Pinecone\n",
        "    index.upsert(vectors=zip(ids, embeds, metadata))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9p8OBkyw4wc"
      },
      "source": [
        "We can check that the vector index has been populated using `describe_index_stats` like before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "I3QdqBijw4wc",
        "outputId": "1d500ff5-e2b8-4abb-f3e5-60003a45da60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {'': {'vector_count': 166}},\n",
              " 'total_vector_count': 166}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqqVoZnkw4wc"
      },
      "source": [
        "#### Retrieval Augmented Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcm-dCPCw4wc"
      },
      "source": [
        "We've built a fully-fledged knowledge base. Now it's time to connect that knowledge base to our chatbot. To do that we'll be diving back into LangChain and reusing our template prompt from earlier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJPGGyXSw4wc"
      },
      "source": [
        "To use LangChain here we need to load the LangChain abstraction for a vector index, called a `vectorstore`. We pass in our vector `index` to initialize the object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "4MPMJmtCw4wc",
        "outputId": "9062538f-8112-4642-a4a9-b12bd07a40ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.vectorstores.pinecone.Pinecone` was deprecated in langchain-community 0.0.18 and will be removed in 0.2.0. An updated version of the class exists in the langchain-pinecone package and should be used instead. To use it run `pip install -U langchain-pinecone` and import as `from langchain_pinecone import Pinecone`.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/vectorstores/pinecone.py:68: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "text_field = \"text\"  # the metadata field that contains our text\n",
        "\n",
        "# initialize the vector store object\n",
        "vectorstore = Pinecone(\n",
        "    index, embed_model.embed_query, text_field\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zrlwGYUw4wc"
      },
      "source": [
        "Using this `vectorstore` we can already query the index and see if we have any relevant information given our question about Jorge's prior deep learning homeworks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "oyShvreew4wc",
        "outputId": "2234861a-107c-4575-d08d-18e035b14d1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Homework 07: NLPSJSU MSDS 255 DL, Spring 2024\\ue313\\nGit: https:/ /github.com/jr gosalv ez/data255_DL\\nimport\\xa0numpy\\xa0as\\xa0np\\nimport\\xa0pandas\\xa0 as\\xa0pd\\nfrom\\xa0scipy\\xa0import\\xa0spatial\\nfrom\\xa0sklearn.metrics.pairwise\\xa0 import\\xa0cosine_similarity\\nfrom\\xa0sklearn.model_selection\\xa0 import\\xa0train_test_split\\nfrom\\xa0sklearn.naive_bayes\\xa0 import\\xa0MultinomialNB\\nfrom\\xa0sklearn.feature_extraction.text\\xa0 import\\xa0CountVectorizer\\nfrom\\xa0sklearn.metrics\\xa0 import\\xa0accuracy_score ,\\xa0confusion_matrix ,\\xa0classification_report\\nimport\\xa0scikitplot\\xa0 as\\xa0skplt\\nimport\\xa0matplotlib.pyplot\\xa0 as\\xa0plt\\nfrom\\xa0gensim.models\\xa0 import\\xa0KeyedVectors\\nimport\\xa0re\\nimport\\xa0nltk\\nfrom\\xa0nltk.corpus\\xa0 import\\xa0stopwords\\nStep 1: Load the Wikipedia GLoVE W ord2Vec (glo ve.6B.50d.txt) \\ue313\\nDownload GLoV e pretrained models: https:/ /nlp.stanfor d.edu/pr ojects/glo ve/\\nembeddings_dict\\xa0=\\xa0 {}\\nwith\\xa0open(\"glove.6B.50d.txt\" ,\\xa0\\'r\\',\\xa0encoding= \"utf-8\")\\xa0as\\xa0f:\\n\\xa0\\xa0\\xa0\\xa0for\\xa0line\\xa0in\\xa0f:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0values\\xa0=\\xa0line.split ()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0word\\xa0\\xa0\\xa0=\\xa0values [0]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0vector\\xa0=\\xa0np.asarray (values[1:],\\xa0\"float32\" )\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0embeddings_dict [word]\\xa0=\\xa0vector', metadata={'source': '', 'title': ''}),\n",
              " Document(page_content='Evaluate P erformance of GAN model (\\x00rst 300 epochs) \\ue3135/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 12/25', metadata={'source': '', 'title': ''}),\n",
              " Document(page_content='G.eval()\\nSequential(\\n  (0): Linear(in_features=64, out_features=256, bias=True)\\n  (1): ReLU()\\n  (2): Linear(in_features=256, out_features=256, bias=True)\\n  (3): ReLU()\\n  (4): Linear(in_features=256, out_features=784, bias=True)\\n  (5): Tanh()\\n)\\nD.eval()5/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\\nhttps://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 15/25', metadata={'source': '', 'title': ''})]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "query = \"Did Jorge Gosalvez code in python?\"\n",
        "\n",
        "vectorstore.similarity_search(query, k=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huzLImnKw4wc"
      },
      "source": [
        "We return a lot of text here and it's not that clear what we need or what is relevant. Fortunately, our LLM will be able to parse this information much faster than us. All we need is to connect the output from our `vectorstore` to our `chat` chatbot. To do that we can use the same logic as we used earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Rp5NBaqfw4wc"
      },
      "outputs": [],
      "source": [
        "def augment_prompt(query: str):\n",
        "    # get top 3 results from knowledge base\n",
        "    results = vectorstore.similarity_search(query, k=3)\n",
        "    # get the text from the results\n",
        "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
        "    # feed into an augmented prompt\n",
        "    augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
        "\n",
        "    Contexts:\n",
        "    {source_knowledge}\n",
        "\n",
        "    Query: {query}\"\"\"\n",
        "    return augmented_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2-7QCTew4wc"
      },
      "source": [
        "Using this we produce an augmented prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Mftcb16Cw4wc",
        "outputId": "fe5a660e-68f9-4d82-ba42-4f59c440a04a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the contexts below, answer the query.\n",
            "\n",
            "    Contexts:\n",
            "    Homework 07: NLPSJSU MSDS 255 DL, Spring 2024\n",
            "Git: https:/ /github.com/jr gosalv ez/data255_DL\n",
            "import numpy as np\n",
            "import pandas  as pd\n",
            "from scipy import spatial\n",
            "from sklearn.metrics.pairwise  import cosine_similarity\n",
            "from sklearn.model_selection  import train_test_split\n",
            "from sklearn.naive_bayes  import MultinomialNB\n",
            "from sklearn.feature_extraction.text  import CountVectorizer\n",
            "from sklearn.metrics  import accuracy_score , confusion_matrix , classification_report\n",
            "import scikitplot  as skplt\n",
            "import matplotlib.pyplot  as plt\n",
            "from gensim.models  import KeyedVectors\n",
            "import re\n",
            "import nltk\n",
            "from nltk.corpus  import stopwords\n",
            "Step 1: Load the Wikipedia GLoVE W ord2Vec (glo ve.6B.50d.txt) \n",
            "Download GLoV e pretrained models: https:/ /nlp.stanfor d.edu/pr ojects/glo ve/\n",
            "embeddings_dict =  {}\n",
            "with open(\"glove.6B.50d.txt\" , 'r', encoding= \"utf-8\") as f:\n",
            "    for line in f:\n",
            "        values = line.split ()\n",
            "        word   = values [0]\n",
            "        vector = np.asarray (values[1:], \"float32\" )\n",
            "        embeddings_dict [word] = vector\n",
            "Evaluate P erformance of GAN model (\u0000rst 300 epochs) 5/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\n",
            "https://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 12/25\n",
            "G.eval()\n",
            "Sequential(\n",
            "  (0): Linear(in_features=64, out_features=256, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (3): ReLU()\n",
            "  (4): Linear(in_features=256, out_features=784, bias=True)\n",
            "  (5): Tanh()\n",
            ")\n",
            "D.eval()5/5/24, 12:56 PM Jorge_Gosalvez_255_HW5.ipynb - Colab\n",
            "https://colab.research.google.com/drive/1j3aCjVv002PQOv7jr5oOrvRKXPjXZsRI#printMode=true 15/25\n",
            "\n",
            "    Query: Did Jorge Gosalvez code in python?\n"
          ]
        }
      ],
      "source": [
        "print(augment_prompt(query))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZI3-CdZw4wc"
      },
      "source": [
        "There is still a lot of text here, so let's pass it onto our chat model to see how it performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "3j-9JMRWw4wc",
        "outputId": "9f25f18f-218f-4fd6-9328-e6c1af934003",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided contexts, the code snippets and references indicate that Jorge Gosalvez worked on coding tasks in Python. The code snippets include Python libraries such as numpy, pandas, scikit-learn, gensim, and nltk, which are commonly used in Python programming for data analysis, machine learning, and natural language processing tasks. Additionally, the references to evaluating the performance of a GAN model in a Jupyter notebook on Google Colab further suggest that Python was used for coding purposes.\n",
            "\n",
            "Therefore, based on the information available, it can be inferred that Jorge Gosalvez coded in Python for the tasks related to the NLPSJSU MSDS 255 DL course and the evaluation of the GAN model.\n"
          ]
        }
      ],
      "source": [
        "# create a new user prompt\n",
        "prompt = HumanMessage(\n",
        "    content=augment_prompt(query)\n",
        ")\n",
        "# add to messages\n",
        "messages.append(prompt)\n",
        "\n",
        "res = chat(messages)\n",
        "\n",
        "print(res.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwWgr78fw4wc"
      },
      "source": [
        "We can continue with more questions about Jorge's prior deep learning homeworks. Let's try _without_ RAG first:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "pBuJCW1ew4wc",
        "outputId": "79dc2ba5-ccca-4f57-ddbb-0d8a03f6097d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jorge Gosalvez coded a GAN (Generative Adversarial Network) model in Python. The GAN model consists of a Generator (G.eval()) and a Discriminator (D.eval()), both of which are defined using a Sequential neural network architecture in PyTorch. The Generator comprises linear layers with ReLU activation functions and a final Tanh activation function, while the Discriminator also includes linear layers with ReLU activation functions.\n"
          ]
        }
      ],
      "source": [
        "prompt = HumanMessage(\n",
        "    content=\"what model did Jorge Gosalvez code?\"\n",
        ")\n",
        "\n",
        "res = chat(messages + [prompt])\n",
        "print(res.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ts3kGD0w4wc"
      },
      "source": [
        "The chatbot is able to respond about Jorge's prior deep learning homeworks thanks to it's conversational history stored in `messages`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "cuWbaJAIw4wc",
        "outputId": "719e1ae6-f17c-4bab-9219-2e5a91837797",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided contexts, Jorge Gosalvez appears to have coded custom models using PyTorch for deep learning tasks such as transfer learning and object detection. Specifically, he implemented a custom model class called \"CustomModel\" which includes convolutional and pooling layers, as well as fully connected layers for classification tasks. Additionally, he worked on projects involving YOLOv8 for object detection on videos.\n",
            "\n",
            "Therefore, based on the information provided, it seems that Jorge Gosalvez has coded custom models using PyTorch for various deep learning applications, including transfer learning and object detection tasks.\n"
          ]
        }
      ],
      "source": [
        "prompt = HumanMessage(\n",
        "    content=augment_prompt(\n",
        "        \"did he code other models?\"\n",
        "    )\n",
        ")\n",
        "\n",
        "res = chat(messages + [prompt])\n",
        "print(res.content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = HumanMessage(\n",
        "    content=augment_prompt(\n",
        "        \"Show the code that Jorge Gosalvez used to optimze his GAN.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "res = chat(messages + [prompt])\n",
        "print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZq8hZQemR15",
        "outputId": "b011c6cd-ac77-48f1-9a20-09677d065d89"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided context, the code snippet used by Jorge Gosalvez to optimize his GAN (Generative Adversarial Network) model is as follows:\n",
            "\n",
            "```python\n",
            "# Device configuration\n",
            "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
            "\n",
            "# Define optimizer for the GAN model\n",
            "G_optimizer = optim.Adam(G.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
            "D_optimizer = optim.Adam(D.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
            "\n",
            "# Training loop\n",
            "for epoch in range(num_epochs):\n",
            "    for i, (real_images, _) in enumerate(data_loader):\n",
            "        real_images = real_images.to(device)\n",
            "        batch_size = real_images.size(0)\n",
            "        \n",
            "        # Train Discriminator\n",
            "        D.zero_grad()\n",
            "        real_outputs = D(real_images)\n",
            "        real_loss = criterion(real_outputs, torch.ones(batch_size, 1).to(device))\n",
            "        \n",
            "        fake_images = G(generate_noise(batch_size)).detach()\n",
            "        fake_outputs = D(fake_images)\n",
            "        fake_loss = criterion(fake_outputs, torch.zeros(batch_size, 1).to(device))\n",
            "        \n",
            "        D_loss = real_loss + fake_loss\n",
            "        D_loss.backward()\n",
            "        D_optimizer.step()\n",
            "        \n",
            "        # Train Generator\n",
            "        G.zero_grad()\n",
            "        fake_images = G(generate_noise(batch_size))\n",
            "        outputs = D(fake_images)\n",
            "        G_loss = criterion(outputs, torch.ones(batch_size, 1).to(device))\n",
            "        \n",
            "        G_loss.backward()\n",
            "        G_optimizer.step()\n",
            "```\n",
            "\n",
            "In this code snippet, the GAN model is optimized using the Adam optimizer with specified learning rates and betas for both the generator (G) and discriminator (D) networks. The training loop contains the steps for training the discriminator and generator networks iteratively to improve the overall performance of the GAN model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = HumanMessage(\n",
        "    content=augment_prompt(\n",
        "        \"Did Jorge code a transformer model?\"\n",
        "    )\n",
        ")\n",
        "\n",
        "res = chat(messages + [prompt])\n",
        "print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8moQ46ZmrTU",
        "outputId": "8cf24734-5165-47d9-af7b-89652ea26222"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, based on the provided contexts, Jorge Gosalvez did code a Transformer model. The context mentions that Jorge created a Seq2Seq network using Transformer for German to English translation in Pytorch. Additionally, the context includes references to Jorge's GitHub repository where the code for the Transformer model implementation can be found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = HumanMessage(\n",
        "    content=augment_prompt(\n",
        "        \"How many pre-trained models did Jorge use for tranfer learning?\"\n",
        "    )\n",
        ")\n",
        "\n",
        "res = chat(messages + [prompt])\n",
        "print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tmf9qV7GnBW-",
        "outputId": "97501195-bcd4-46f8-d92a-a84417f255d5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jorge used two pre-trained models for transfer learning. The first pre-trained model is \"models.inception_v3\" which was loaded and adjusted to match the original model's output units. The second pre-trained model is \"models.resnet18\" which was also loaded and adjusted to match the desired output units. By using these pre-trained models and adjusting them accordingly, Jorge performed transfer learning for his task.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = HumanMessage(\n",
        "    content=augment_prompt(\n",
        "        \"Show the roboflow code snippet Jorge wrote to predict what objects existed in the boston_dog.jpeg image.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "res = chat(messages + [prompt])\n",
        "print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhQNQS4EnZ9T",
        "outputId": "6129276f-2ce5-4f68-ca5f-266054ba8629"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided context, the code snippet that Jorge wrote using Roboflow to predict what objects existed in the \"boston_dog.jpeg\" image is as follows:\n",
            "\n",
            "```python\n",
            "!pip install roboflow\n",
            "from roboflow import Roboflow\n",
            "\n",
            "This image most likely belongs to frog with a 57.76% confidence.\n",
            "This image most likely belongs to airplane with a 57.76% confidence.\n",
            "This image most likely belongs to bird with a 57.76% confidence.\n",
            "This image most likely belongs to airplane with a 57.76% confidence.\n",
            "```\n",
            "\n",
            "In this code snippet, Jorge is using Roboflow to predict the objects present in the \"boston_dog.jpeg\" image, with corresponding confidence levels for each predicted object.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = HumanMessage(\n",
        "    content=augment_prompt(\n",
        "        \"How did Jorge split the CNN dataset into training and validation sets?\"\n",
        "    )\n",
        ")\n",
        "\n",
        "res = chat(messages + [prompt])\n",
        "print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zyI_DCfn5le",
        "outputId": "86c8f494-cb5b-4a19-d60a-a8bf9a9f12b7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided context, Jorge split the CNN dataset into training and validation sets using the following approach:\n",
            "\n",
            "Jorge used the `Subset` class from PyTorch to split the dataset into training and validation sets. By creating a custom dataset with at least 100 images for each of the 3 categories from CIFAR-10, Jorge then utilized the `Subset` class to separate the dataset into training and validation subsets. This method allows for the creation of custom training and validation sets with specific criteria, such as the number of images per category.\n",
            "\n",
            "If you need more detailed information or code snippets related to Jorge's specific implementation for splitting the dataset, please let me know.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9A8xHh5w4wc"
      },
      "source": [
        "We get a much more informed response that includes several items missing in the previous non-RAG response, such as \"red-teaming\", \"iterative evaluations\", and the intention of the researchers to share this research to help \"improve their safety, promoting responsible development in the field\"."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations and Limitations:**\n",
        "* although the RAG provided more informed based on text content, the LLM could not return code examples based on embeddings\n",
        "* notebooks PDFed include special characters, removing these will likely improve the quality of responses\n",
        "* chunking format ensures data loading and ingestion occurs properly\n",
        "* appending prompts and responses to messages expand content to enable the chatbot to 'converse'\n",
        "* configuring system (assistant) parameters affects LLM results"
      ],
      "metadata": {
        "id": "kbqygLLrqrpQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPrsKLj7w4wc"
      },
      "source": [
        "Delete the index to save resources and not be charged for non-use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "E8LSIHSGw4wc"
      },
      "outputs": [],
      "source": [
        "pc.delete_index(index_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtiAQmWdw4wd"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "147db994a0ef40e8b156ec8efc282216": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e6f67823ccbe4575b67ef366559ef115",
              "IPY_MODEL_e2251259cba249868868886d109ae35c",
              "IPY_MODEL_1ed3b66d5f8948b79d70985d2da9bba0"
            ],
            "layout": "IPY_MODEL_9918148e66a640e98ba43fdc47b8f91d"
          }
        },
        "e6f67823ccbe4575b67ef366559ef115": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47036c7a921a410e84e5ab9f538e162f",
            "placeholder": "​",
            "style": "IPY_MODEL_c90b082e6ab64e9d9febe55ef7606f4a",
            "value": "100%"
          }
        },
        "e2251259cba249868868886d109ae35c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5facec2d0cc14519aa26f493dc202160",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4ad55880bd4443dd8a87981dea80b4d6",
            "value": 2
          }
        },
        "1ed3b66d5f8948b79d70985d2da9bba0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3f212423af74648bb9fb9ded43a20d9",
            "placeholder": "​",
            "style": "IPY_MODEL_14f46db244884f08b5fde46fa48fa227",
            "value": " 2/2 [00:10&lt;00:00,  4.97s/it]"
          }
        },
        "9918148e66a640e98ba43fdc47b8f91d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47036c7a921a410e84e5ab9f538e162f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c90b082e6ab64e9d9febe55ef7606f4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5facec2d0cc14519aa26f493dc202160": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ad55880bd4443dd8a87981dea80b4d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b3f212423af74648bb9fb9ded43a20d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14f46db244884f08b5fde46fa48fa227": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}