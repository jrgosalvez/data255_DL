{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7bd5b46-90ca-4cfa-a0ad-4f7b05bb6c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.9/site-packages (2.31.0)\n",
      "Requirement already satisfied: bs4 in /opt/anaconda3/lib/python3.9/site-packages (0.0.2)\n",
      "Requirement already satisfied: PyPDF2 in /opt/anaconda3/lib/python3.9/site-packages (3.0.1)\n",
      "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.9/site-packages (0.28.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.9/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.9/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.9/site-packages (from requests) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.9/site-packages (from requests) (2023.11.17)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.9/site-packages (from bs4) (4.12.2)\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from PyPDF2) (4.9.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.9/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.9/site-packages (from openai) (3.9.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.9/site-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.9/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.9/site-packages (from aiohttp->openai) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.9/site-packages (from aiohttp->openai) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.9/site-packages (from aiohttp->openai) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/anaconda3/lib/python3.9/site-packages (from aiohttp->openai) (4.0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4->bs4) (2.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests bs4 PyPDF2 openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40cb734b-8091-4751-9603-fbfe75649242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import PyPDF2\n",
    "import openai\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5dd3b86-2740-4287-a5d7-1b81ee13b5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up OpenAI API\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdeab8d5-49e5-4ada-b09d-7e41379eda1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "def generate_text_from_website(url):\n",
    "    # Retrieve webpage content\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract text from webpage\n",
    "    webpage_text = ' '.join([p.text for p in soup.find_all('p')])\n",
    "\n",
    "    # Limit text to the first 4000 words\n",
    "    words = webpage_text.split()\n",
    "    first_4000_words = ' '.join(words[:4000])\n",
    "\n",
    "    # Example prompt to generate text from the website\n",
    "    prompt = \"Generate text from the website: \" + first_4000_words\n",
    "    \n",
    "    # Call OpenAI API to generate text\n",
    "    generated_text = openai.Completion.create(\n",
    "        engine=\"gpt-3.5-turbo-instruct\",  # Choose appropriate engine\n",
    "        prompt=prompt,\n",
    "        max_tokens=1000  # Adjust as needed 100 = best accurate; 500 = ok 1000 = worst\n",
    "    )\n",
    "    \n",
    "    return generated_text['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e1494a0-5463-48a7-b72f-9d6d4327e663",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72338212-163b-4b63-88d9-e91fc2ef2641",
   "metadata": {},
   "source": [
    "## Part a - Ask the bot to solve one complex math problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2c6da4-4a8a-45a0-bf77-3716e118aeb0",
   "metadata": {},
   "source": [
    "Sample word problem and solution, see [problem 1](https://www.onlinemath4all.com/differential-calculus-word-problems-with-solutions.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08fcb1a6-b0e5-483d-b4bd-c6ee6e837b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \n",
      " (1) To find the initial velocity of the missile, we need to find the velocity at time t=0. The initial velocity is the derivative of the height function x(t) with respect to time t. \n",
      "\n",
      "x(t) = 100t - (25/2)t^2\n",
      "v(t) = dx/dt = 100 - 25t\n",
      "\n",
      "At t=0, v(0) = 100 m/s\n",
      "\n",
      "Therefore, the initial velocity of the missile is 100 m/s.\n",
      "\n",
      "(2) To find the time when the height of the missile is a maximum, we need to find the time when the velocity is 0. This occurs at the maximum height.\n",
      "\n",
      "v(t) = 100 - 25t\n",
      "0 = 100 - 25t\n",
      "25t = 100\n",
      "t = 4 seconds\n",
      "\n",
      "Therefore, the time when the height of the missile is a maximum is 4 seconds.\n",
      "\n",
      "(3) To find the maximum height reached by the missile, we substitute t=4 into the height function x(t).\n",
      "\n",
      "x(4) = 100(4) - (25/2)(4)^2\n",
      "x(4) = 400 - 200\n",
      "x(4) = 200 meters\n",
      "\n",
      "Therefore, the maximum height reached by the missile is 200 meters.\n",
      "\n",
      "(4) To find the velocity with which the missile strikes the ground, we need to find the velocity at the time when the missile hits the ground. This occurs when x(t) = 0.\n",
      "\n",
      "x(t) = 100t - (25/2)t^2\n",
      "0 = 100t - (25/2)t^2\n",
      "0 = 2(100t - 12.5t^2)\n",
      "0 = 100t - 12.5t^2\n",
      "0 = t(100 - 12.5t)\n",
      "t = 0 or t = 8 seconds\n",
      "\n",
      "Since the missile is fired from the ground, we consider t=0 to be the initial time. Therefore, the missile strikes the ground at t=8 seconds.\n",
      "\n",
      "To find the velocity at t=8 seconds, we substitute t=8 into the velocity function v(t).\n",
      "\n",
      "v(8) = 100 - 25(8)\n",
      "v(8) = 100 - 200\n",
      "v(8) = -100 m/s\n",
      "\n",
      "Therefore, the velocity with which the missile strikes the ground is -100 m/s, indicating that it is moving downward.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "\n",
    "prompt = f\"\"\"\n",
    "    A missile fired ground level rises x meters vertically upwards in t seconds and x = 100t - (25/2)t2. Find answers for the following four parts\n",
    "    (1) the initial velocity of the missile,\n",
    "    (2) the time when the height of the missile is a maximum\n",
    "    (3) the maximum height reached and\n",
    "    (4) the velocity with which the missile strikes the ground.\n",
    "    \"\"\"\n",
    "response = get_completion(prompt)\n",
    "chat += response\n",
    "print(f'Answer: \\n {response}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb32d45-8278-405a-9a3f-3aae41505e8d",
   "metadata": {},
   "source": [
    "### Actual answers per website"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c52e7d64-e5e3-4f1c-8373-3013dc80804c",
   "metadata": {},
   "source": [
    "x = 100t - (25/2)t²\n",
    "(i)  dx/dt  =  100 meter/seconds\n",
    "(ii) t  =  4 seconds  So, the object is taking 4 seconds to reach the maximum height.\n",
    "(iii)  The missile is taking 4 seconds to reach its maximum height =  200 meter\n",
    "(iv) When the missile reaches the ground, the height of the missile  =  0; dx/dt =  -100 meter/seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae6cd97-a666-4f24-b319-f312097262e1",
   "metadata": {},
   "source": [
    "OpenAI predicted 3 of 4 answers correctly. The fourth answer followed good logic; however, was incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf2816b-c2c2-4747-9292-67d81bb6c0c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part b - Rewrite Answers Based on Fine Tuning Data from a Website Based on Questions from a PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c892701-142f-469a-ae6d-ce27ad192211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Fetch website content\n",
    "url = 'https://ai.meta.com/blog/5-steps-to-getting-started-with-llama-2/'\n",
    "response = requests.get(url)\n",
    "html_content = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7eac20a7-0b85-4f47-9b27-75f5d171c5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Parse HTML content to extract information\n",
    "soup = BeautifulSoup(html_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3786ae02-eb95-413a-bf17-c4ea52e3333d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions:\n",
      "1.Is Llama2 freely available for research?\n",
      "2.What the top 5 steps to getting started with llama2 in 10 words or less per step are?\n",
      "3.What date was the website made?\n",
      "4.Is Llama2 pithy?\n",
      "5.Is Llama2 concise?\n",
      "6.Is Llama2 easy or hard to learn?\n",
      "7.Do users have to code everything from scratch with Llama2?\n",
      "8.Does Llama2 have built-in tools?\n",
      "9.Is Llama2 unique?\n",
      "10.How many parameters was Llama2 trained on?\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Extract text from PDF containing questions\n",
    "pdf_path = 'questions.pdf'\n",
    "pdf_text = ''\n",
    "with open(pdf_path, 'rb') as pdf_file:\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "    for page_num in range(len(pdf_reader.pages)):\n",
    "        page = pdf_reader.pages[page_num]\n",
    "        pdf_text += page.extract_text()\n",
    "\n",
    "print(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32801516-cec0-4c05-8cbe-b07c4fde8485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Is Llama2 freely available for research', 'What the top 5 steps to getting started with llama2 in 10 words or less per step are', 'What date was the website made', 'Is Llama2 pithy', 'Is Llama2 concise', 'Is Llama2 easy or hard to learn', 'Do users have to code everything from scratch with Llama2', 'Does Llama2 have built-in tools', 'Is Llama2 unique', 'How many parameters was Llama2 trained on']\n"
     ]
    }
   ],
   "source": [
    "# Define the regular expression pattern to match the questions\n",
    "pattern = r'\\d+\\.(.*?)\\?'\n",
    "\n",
    "# Find all matches of the pattern in the text input\n",
    "questions = re.findall(pattern, pdf_text, re.DOTALL)\n",
    "\n",
    "# Strip whitespace from the questions and append to a list\n",
    "question_list = [question.strip() for question in questions]\n",
    "\n",
    "print(question_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf60b065-d152-4732-a2a3-9a4d5cc0918d",
   "metadata": {},
   "source": [
    "### Ask OpenAI Questions from the PDF Generated List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8741ab83-3f3d-4960-9b99-f38d48c2168c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is Llama2 freely available for research\n",
      "Answer: \n",
      " Yes, Llama2 is freely available for research purposes. It is an open-source software tool that can be downloaded and used by researchers for their studies.\n",
      "\n",
      "Question: What the top 5 steps to getting started with llama2 in 10 words or less per step are\n",
      "Answer: \n",
      " 1. Install llama2 software on your computer.\n",
      "2. Create a new project in llama2.\n",
      "3. Import your data into the project.\n",
      "4. Analyze your data using llama2 tools.\n",
      "5. Visualize and interpret your results for insights.\n",
      "\n",
      "Question: What date was the website made\n",
      "Answer: \n",
      " I'm sorry, but I do not have access to that information.\n",
      "\n",
      "Question: Is Llama2 pithy\n",
      "Answer: \n",
      " No, llamas are not typically known for being pithy.\n",
      "\n",
      "Question: Is Llama2 concise\n",
      "Answer: \n",
      " Yes, the statement \"Is Llama2 concise\" is concise.\n",
      "\n",
      "Question: Is Llama2 easy or hard to learn\n",
      "Answer: \n",
      " It depends on the individual's learning style and experience with similar technologies. Some people may find Llama2 easy to learn due to its user-friendly interface and comprehensive documentation, while others may find it challenging if they are new to programming or data analysis. Overall, with dedication and practice, most people should be able to learn Llama2 effectively.\n",
      "\n",
      "Question: Do users have to code everything from scratch with Llama2\n",
      "Answer: \n",
      " No, users do not have to code everything from scratch with Llama2. Llama2 is a low-code platform that allows users to build applications and workflows using a visual interface and pre-built components. Users can drag and drop elements to create their desired functionality without needing to write code from scratch. However, users can also customize and extend their applications with code if they choose to do so.\n",
      "\n",
      "Question: Does Llama2 have built-in tools\n",
      "Answer: \n",
      " No, Llama2 does not have built-in tools. It is a virtual assistant that can provide information and assistance through text-based communication.\n",
      "\n",
      "Question: Is Llama2 unique\n",
      "Answer: \n",
      " Yes, Llama2 is unique as it is a specific identifier for a particular llama.\n",
      "\n",
      "Question: How many parameters was Llama2 trained on\n",
      "Answer: \n",
      " It is not specified how many parameters Llama2 was trained on. The number of parameters can vary depending on the specific architecture and configuration of the model.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "for i in question_list:\n",
    "    prompt = f\"\"\"\n",
    "    {i}\n",
    "    \"\"\"\n",
    "    response = get_completion(prompt)\n",
    "    print(f'Question: {i}')\n",
    "    print(f'Answer: \\n {response}')\n",
    "    chat += ' ' + response\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8d8437-1725-4a7a-9ee0-048dfc1dd23b",
   "metadata": {},
   "source": [
    "### Website text to fine-tune for prompt questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fb3931f-3257-41b5-908d-e913f87e4cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Artificial intelligence (AI) has been making remarkable progress in recent years, attracting the attention of the public and highlighting the enormous potential these technologies have in empowering us to achieve the extraordinary. This potential goes beyond just algorithms; it promises to usher in a new era of economic growth, social progress, and innovative means of expression and connection. At Meta, we strongly believe in an open approach to AI development, particularly in the ever-evolving landscape of generative AI. By openly sharing AI models, we can extend their benefits to all levels of society. As part of this belief, we have recently released Llama 2, a large language model that is now open source and accessible to businesses, startups, aspiring entrepreneurs, and researchers. In this blog, we will discuss five steps for you to get started with Llama 2 and take advantage of its benefits in your own projects. We will cover the key concepts, how to set it up, available resources, and provide you with a step-by-step process to implement and run Llama 2.\n",
      "\n",
      "Introduction:\n",
      "Llama 2 includes model weights and starting code for pre-trained and fine-tuned large language models, with parameter ranges from 7B to 70B. Llama 2 has been trained on 40% more data than Llama 1 and has double the context length. The model was pre-trained on publicly available online data sources and fine-tuned using publicly available instruction datasets and over 1 million human annotations, using reinforcement learning from human feedback (RLHF) to ensure safety and helpfulness. Llama 2 has outperformed other open language models on various external benchmarks, including reasoning, coding, proficiency, and knowledge tests. For more detailed information on these benchmarks, visit our website. Llama 2 is free for research and commercial use. In the following section, we will discuss five steps you can take to start using Llama 2. There are various ways to set up Llama 2 locally; we will discuss one of these ways, which will make it easier and faster to set up and start using Llama.\n",
      "\n",
      "Getting started with Llama 2:\n",
      "Step 1: Prerequisites and dependencies\n",
      "Python will be used to write the script to set up and run the pipeline. To install Python, visit the Python website, choose your OS, and download the version that you prefer. For this example, we will be using the transformers and accelerate libraries from Hugging Face.\n",
      "pip install accelerate\n",
      "\n",
      "Step 2: Download the model weights\n",
      "The models are available on our Llama 2 Github repository. To download the model through our Github repository:\n",
      "\n",
      "- Launch the download.sh script (sh download.sh).\n",
      "- When prompted, enter the presigned URL that you receive in your email.\n",
      "- Run ln -h ./tokenizer.model ./llama-2-7b-chat/tokenizer.model to create a link to the tokenizer. This is necessary for conversion (next step).\n",
      "- Convert the model weights to run with Hugging Face: pip install protobuf && python $TRANSFORM --input_dir ./llama-2-7b-chat --model_size 7B --output_dir ./llama-2-7b-chat-hf\n",
      "- We also provide converted Llama 2 weights on Hugging Face. To use the downloads on Hugging Face, you must first request a download, as shown in the steps above. Make sure to use the same email address as your Hugging Face account.\n",
      "\n",
      "Step 3: Write your Python script\n",
      "Next, create a new Python script that will be used to run the example. This script will contain all the necessary code to load the model and run inference with transformers.\n",
      "Import necessary modules:\n",
      "First, import the following necessary modules into your script: LlamaForCausalLM (the Llama 2 model class), LlamaTokenizer (prepares the prompt for the model to process), pipeline (an abstraction to generate model outputs), and torch (allows us to use PyTorch and specify the datatype we’d like to use).\n",
      "import transformers from transformers import LlamaForCausalLM, LlamaTokenizer \n",
      "Load your model:\n",
      "Next, load the Llama model with the weights downloaded and converted (stored at ./llama-2-7b-chat-hf in this example).\n",
      "model = LlamaForCausalLM.from_pretrained(model_dir) \n",
      "Define and instantiate the tokenizer and pipeline:\n",
      "Next, we need to ensure that our inputs are prepared for the model. This is done by loading the tokenizer associated with the model. In your script, add the following to initialize the tokenizer from the same model directory:\n",
      "tokenizer = LlamaTokenizer.from_pretrained(model_dir) \n",
      "Then, we need a way to use our model for inference. Pipeline allows us to specify which type of task the pipeline needs to run (“text-generation”), specify the model that the pipeline should use to make predictions (model), define the precision for using\n"
     ]
    }
   ],
   "source": [
    "# Webpage dataset based on number of tokens specified\n",
    "generated_text = generate_text_from_website(url)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de5d7a3-6002-4957-a22f-1afa2c9cab82",
   "metadata": {},
   "source": [
    "### Tuned prompts for questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e414ef7-5329-49ba-b50a-dc2cd4ea2506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is Llama2 freely available for research\n",
      "Answer: \n",
      "  the model (device=\"cuda\" for GPU or \"cpu\" for CPU), and specify any additional arguments needed for the pipeline. Add the following to your script:\n",
      "generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=0) \n",
      "Run inference:\n",
      "Now that everything is set up, you can run inference with your model. Add the following code to your script to generate text based on a prompt:\n",
      "prompt = \"Once upon a time\"\n",
      "output = generator(prompt, max_length=50, num_return_sequences=3) \n",
      "This will generate three different text sequences based on the prompt \"Once upon a time\" with a maximum length of 50 tokens each.\n",
      "\n",
      "Step 4: Run your Python script\n",
      "Save your Python script and run it using the Python interpreter. You should see the model generating text based on the prompt you provided.\n",
      "\n",
      "Step 5: Explore and experiment\n",
      "Now that you have successfully set up and run Llama 2, you can explore and experiment with different prompts, parameters, and tasks to see the capabilities of the model. You can also fine-tune the model on your own datasets to further enhance its performance for specific tasks.\n",
      "\n",
      "Conclusion:\n",
      "In this blog, we have discussed the release of Llama 2 as an open-source language model and provided a step-by-step guide for getting started with the model. By following these steps, you can begin using Llama 2 for research, commercial projects, or any other applications you have in mind. We encourage you to explore the possibilities of Llama 2 and contribute to the advancement of AI technology. Thank you for reading, and we look forward to seeing the innovative ways you will use Llama 2 in your projects.\n",
      "\n",
      "Question: What the top 5 steps to getting started with llama2 in 10 words or less per step are\n",
      "Answer: \n",
      " the model (torch.float32), and specify the device to run the model on (torch.device(\"cuda\")).\n",
      "pipeline = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, device=torch.device(\"cuda\"), dtype=torch.float32) \n",
      "\n",
      "Step 4: Run the script\n",
      "Now that you have written your Python script, you can run it to generate text using Llama 2. Make sure to follow the instructions in your script to input prompts and generate text outputs.\n",
      "\n",
      "Step 5: Explore and experiment\n",
      "Once you have successfully set up and run Llama 2, take the time to explore its capabilities and experiment with different prompts and tasks. You can fine-tune the model on your own datasets, customize the prompts, and integrate Llama 2 into your own projects to leverage its powerful language generation capabilities.\n",
      "\n",
      "By following these five steps, you can quickly get started with Llama 2 and begin harnessing the power of this advanced language model in your own projects. Whether you are a researcher, developer, entrepreneur, or enthusiast, Llama 2 offers a wide range of possibilities for enhancing your work and exploring new avenues of creativity and innovation. We encourage you to dive in, experiment, and discover the exciting potential that Llama 2 has to offer.\n",
      "\n",
      "Question: What date was the website made\n",
      "Answer: \n",
      " The website was made on an unspecified date.\n",
      "\n",
      "Question: Is Llama2 pithy\n",
      "Answer: \n",
      " No, the technical specifications provided do not indicate that Llama 2 is pithy.\n",
      "\n",
      "Question: Is Llama2 concise\n",
      "Answer: \n",
      " Llama 2 is not concise in its technical specifications as it provides detailed information on the model, its training data, benchmarks, and steps to get started with using it. The technical specifications are thorough and informative, but not necessarily concise.\n",
      "\n",
      "Question: Is Llama2 easy or hard to learn\n",
      "Answer: \n",
      " Llama 2 may be considered hard to learn for individuals who are not familiar with Python scripting and working with AI models. The technical specifications provided above outline the steps required to set up and run Llama 2, which involve downloading model weights, writing Python scripts, and using specific libraries and tools. However, for those with experience in these areas, learning Llama 2 may not be as challenging. It ultimately depends on the individual's background and familiarity with the necessary technologies and concepts.\n",
      "\n",
      "Question: Do users have to code everything from scratch with Llama2\n",
      "Answer: \n",
      " the model (torch.float32), and specify the tokenizer that the pipeline should use to process inputs (tokenizer). Add the following to your script to instantiate the pipeline:\n",
      "generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=0, framework='pt', dtype=torch.float32) \n",
      "\n",
      "Step 4: Run the script\n",
      "Now that you have set up your Python script with the necessary modules, loaded the model, tokenizer, and pipeline, you can run the script to generate text using Llama 2. Make sure to provide the necessary input prompts for the model to generate text based on your desired task.\n",
      "\n",
      "Step 5: Customize and fine-tune\n",
      "Once you have successfully set up and run Llama 2, you can further customize and fine-tune the model for your specific use case. You can experiment with different prompts, adjust parameters, and fine-tune the model on your own datasets to improve performance and achieve better results.\n",
      "\n",
      "In conclusion, while users do need to write some code to set up and run Llama 2, they do not have to code everything from scratch. Llama 2 provides pre-trained and fine-tuned large language models, along with starting code and model weights, to make it easier for users to get started and take advantage of its benefits in their projects. By following the five steps outlined above, users can quickly set up Llama 2 and start using it for various tasks, such as text generation, coding, proficiency tests, and more.\n",
      "\n",
      "Question: Does Llama2 have built-in tools\n",
      "Answer: \n",
      "  the model (torch.float32), and specify the tokenizer that the pipeline should use to process inputs (tokenizer). Add the following to your script:\n",
      "pipe = pipeline('text-generation', model=model, tokenizer=tokenizer, device=0, dtype=torch.float32) \n",
      "\n",
      "Step 4: Run the script\n",
      "Now that you have written your Python script, you can run it to start using Llama 2 for text generation. Make sure to follow the instructions provided in the script to input prompts and generate text outputs.\n",
      "\n",
      "Step 5: Explore and experiment\n",
      "Once you have successfully set up and run Llama 2, you can explore its capabilities further and experiment with different prompts, tasks, and datasets. You can also fine-tune the model on your own data to customize its performance for specific applications.\n",
      "\n",
      "In conclusion, Llama 2 is a powerful open-source language model that offers a wide range of possibilities for research, development, and innovation. By following the steps outlined in this blog, you can quickly get started with Llama 2 and leverage its capabilities to enhance your projects and achieve your goals. We encourage you to explore the resources available on our website and join the growing community of developers and researchers using Llama 2 to push the boundaries of AI and create new opportunities for growth and progress.\n",
      "\n",
      "Question: Is Llama2 unique\n",
      "Answer: \n",
      "  the model (torch.float32), and specify the device to run the model on (CPU or GPU).\n",
      "pipeline = pipeline('text-generation', model=model, tokenizer=tokenizer, device=0, dtype=torch.float32) \n",
      "\n",
      "Step 4: Run the script\n",
      "Now that you have written your Python script, you can run it to start using Llama 2 for text generation. Make sure to follow the instructions provided in the script to input prompts and generate text outputs.\n",
      "\n",
      "Step 5: Explore and experiment\n",
      "Once you have successfully set up and run Llama 2, take the time to explore its capabilities and experiment with different prompts and tasks. Llama 2 has been trained on a wide range of data sources and fine-tuned for various tasks, so there is a lot to discover and leverage in your projects.\n",
      "\n",
      "In conclusion, Llama 2 is a powerful and versatile language model that is now open source and accessible to all. By following the steps outlined in this blog, you can quickly get started with Llama 2 and begin harnessing its benefits for your own projects. Whether you are a researcher, entrepreneur, or developer, Llama 2 offers a wealth of opportunities for innovation and creativity. Start exploring today and see what Llama 2 can do for you!\n",
      "\n",
      "Question: How many parameters was Llama2 trained on\n",
      "Answer: \n",
      " Llama 2 was trained on parameter ranges from 7B to 70B.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "for i in question_list:\n",
    "    prompt = f\"\"\"\n",
    "    {i}\n",
    "\n",
    "    Technical specifications: ```{generated_text}```\n",
    "    \"\"\"\n",
    "    response = get_completion(prompt)\n",
    "    print(f'Question: {i}')\n",
    "    print(f'Answer: \\n {response}')\n",
    "    chat += ' ' + response\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d51ff43b-563f-4ea8-8cbe-1a3623cf205b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is Llama2 freely available for research\n",
      "Answer: \n",
      " Yes, Llama2 is freely available for research purposes.\n",
      "\n",
      "Question: What the top 5 steps to getting started with llama2 in 10 words or less per step are\n",
      "Answer: \n",
      " the model (torch.float32), and specify the tokenizer to use (tokenizer).\n",
      "pipeline = pipeline('text-generation', model=model, tokenizer=tokenizer, device=0, framework='pt', dtype=torch.float32) \n",
      "\n",
      "Step 4: Run the script\n",
      "Now that you have written your Python script, you can run it to generate text using Llama 2. Make sure to follow the instructions in your script to input prompts and generate text outputs.\n",
      "\n",
      "Step 5: Explore and experiment\n",
      "Once you have successfully set up and run Llama 2, take the time to explore its capabilities and experiment with different prompts and tasks. You can fine-tune the model further on your own datasets or use it for various natural language processing tasks.\n",
      "\n",
      "By following these five steps, you can quickly get started with Llama 2 and leverage its powerful capabilities in your projects. Remember to refer to the official documentation and resources for more in-depth information and support. Happy coding with Llama 2!\n",
      "\n",
      "Question: What date was the website made\n",
      "Answer: \n",
      " Date of website creation: Not specified in provided text.\n",
      "\n",
      "Question: Is Llama2 pithy\n",
      "Answer: \n",
      " Yes, Llama2 is pithy.\n",
      "\n",
      "Question: Is Llama2 concise\n",
      "Answer: \n",
      " Yes, Llama2 is concise.\n",
      "\n",
      "Question: Is Llama2 easy or hard to learn\n",
      "Answer: \n",
      " Llama2 is hard to learn due to technical specifications involved.\n",
      "\n",
      "Question: Do users have to code everything from scratch with Llama2\n",
      "Answer: \n",
      " No, users can use pre-trained models and fine-tune them.\n",
      "\n",
      "Question: Does Llama2 have built-in tools\n",
      "Answer: \n",
      " Yes, Llama2 has built-in tools for language processing tasks.\n",
      "\n",
      "Question: Is Llama2 unique\n",
      "Answer: \n",
      " Yes, Llama2 is unique in its capabilities and performance.\n",
      "\n",
      "Question: How many parameters was Llama2 trained on\n",
      "Answer: \n",
      " Trained on parameter ranges from 7B to 70B.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "for i in question_list:\n",
    "    prompt = f\"\"\"\n",
    "    In 10 words or less answer {i}\n",
    "\n",
    "    Technical specifications: ```{generated_text}```\n",
    "    \"\"\"\n",
    "    response = get_completion(prompt)\n",
    "    print(f'Question: {i}')\n",
    "    print(f'Answer: \\n {response}')\n",
    "    chat += ' ' + response\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a8b5b6-82b9-4325-b01f-7702a461cea5",
   "metadata": {},
   "source": [
    "# Part c - Ask chatbot to summarize the our chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "764a143c-8fcf-4388-8e2c-927fe353661e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \n",
      " The chat above discusses the technical specifications of finding the initial velocity, maximum height, and velocity at impact of a missile. It also provides a step-by-step guide on setting up and running Llama 2, an open-source language model, for text generation tasks. Llama 2 is a powerful tool for researchers and developers to explore and experiment with natural language processing tasks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "\n",
    "prompt = f\"\"\"\n",
    "    Summarize our chat above in 50 words.\n",
    "    \n",
    "    Technical specifications: ```{chat}```\n",
    "    \"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(f'Answer: \\n {response}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52816bee-43b7-44df-a70b-b9ae625a6352",
   "metadata": {},
   "outputs": [],
   "source": [
    "done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
